{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Extraction and Summarization with Sequence to Sequence Learning.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "czSR_WOeHqea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# KDD 2018 Hands-On Tutorial  https://kddseq2seq.com/\n",
        "\n",
        "Feature Extraction and Summarization With Sequence-to-Sequence Learning\n",
        "\n",
        "\n",
        "### Pre-requisites\n",
        "\n",
        "The target audience of this tutorial are moderately skilled users who have some familiarity with neural networks and are comfortable writing code.  These blog posts are good background for this tutorial:\n",
        "\n",
        "- [How To Create Data Products That Are Magical Using Sequence-to-Sequence Models](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8)\n",
        "\n",
        "- [How To Create Natural Language Semantic Search For Arbitrary Objects With Deep Learning](https://towardsdatascience.com/semantic-code-search-3cd6d244a39c)\n",
        "\n",
        "### Google Colab Notebooks\n",
        "\n",
        "This tutorial can be run in Google Colab notebooks, which provides a free gpu-enabled Jupyter Notebook on the cloud.  **You can open this notebook in Colab  by following [this link](https://colab.research.google.com/github/hohsiangwu/kdd-2018-hands-on-tutorials/blob/master/Feature%20Extraction%20and%20Summarization%20with%20Sequence%20to%20Sequence%20Learning.ipynb).**"
      ]
    },
    {
      "metadata": {
        "id": "Q-gKT37Didb7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup Notebook\n",
        "\n",
        "Install [ktext](https://github.com/hamelsmu/ktext) and [annoy](https://github.com/spotify/annoy)."
      ]
    },
    {
      "metadata": {
        "id": "E7l80u-0fyHK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q ktext\n",
        "!pip install -q annoy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W-KjInFk0v8l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "from urllib.request import urlopen\n",
        "\n",
        "from annoy import AnnoyIndex\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, LSTM, GRU, Embedding, Lambda, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from ktext.preprocess import processor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iZzcs-PDPZqn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data sets"
      ]
    },
    {
      "metadata": {
        "id": "McfeOLlh0v8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [CoNaLa](https://conala-corpus.github.io/)\n",
        "\n",
        "Challenge designed to test systems for generating program snippets from natural language.\n",
        "\n",
        "\n",
        "### Preview of the CoNaLa Dataset\n",
        "\n",
        "```\n",
        "{\n",
        "  \"question_id\": 36875258,\n",
        "  \"intent\": \"copying one file's contents to another in python\", \n",
        "  \"rewritten_intent\": \"copy the content of file 'file.txt' to file 'file2.txt'\", \n",
        "  \"snippet\": \"shutil.copy('file.txt', 'file2.txt')\", \n",
        "}\n",
        "\n",
        "{\n",
        "  \"intent\": \"How do I check if all elements in a list are the same?\", \n",
        "  \"rewritten_intent\": \"check if all elements in list `mylist` are the same\", \n",
        "  \"snippet\": \"len(set(mylist)) == 1\", \n",
        "  \"question_id\": 22240602\n",
        "}\n",
        "\n",
        "{\n",
        "  \"intent\": \"Iterate through words of a file in Python\", \n",
        "  \"rewritten_intent\": \"get a list of words `words` of a file 'myfile'\", \n",
        "  \"snippet\": \"words = open('myfile').read().split()\", \n",
        "  \"question_id\": 7745260\n",
        "}\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "JAAOUd-k0v8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8491cf27-16f6-4d78-d9c3-2b99d9f13f93"
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.phontron.com/download/conala-corpus-v1.1.zip\n",
        "!unzip -o conala-corpus-v1.1.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-08-20 15:57:30--  http://www.phontron.com/download/conala-corpus-v1.1.zip\n",
            "Resolving www.phontron.com (www.phontron.com)... 208.113.196.149\n",
            "Connecting to www.phontron.com (www.phontron.com)|208.113.196.149|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52105440 (50M) [application/zip]\n",
            "Saving to: ‘conala-corpus-v1.1.zip’\n",
            "\n",
            "conala-corpus-v1.1. 100%[===================>]  49.69M  26.0MB/s    in 1.9s    \n",
            "\n",
            "2018-08-20 15:57:32 (26.0 MB/s) - ‘conala-corpus-v1.1.zip’ saved [52105440/52105440]\n",
            "\n",
            "Archive:  conala-corpus-v1.1.zip\n",
            "   creating: conala-corpus/\n",
            "  inflating: conala-corpus/conala-mined.jsonl  \n",
            "  inflating: conala-corpus/conala-train.json  \n",
            "  inflating: conala-corpus/conala-test.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hFzeLKxg0v81",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('conala-corpus/conala-mined.jsonl', 'r') as f:\n",
        "    lines = [json.loads(line) for line in f.readlines()]\n",
        "source_docs = [line['snippet'] for line in lines]\n",
        "target_docs = [line['intent'] for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QsXzuRH50v83",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('conala-corpus/conala-train.json', 'r') as f:\n",
        "    lines = json.load(f)\n",
        "train_source_docs = [line['snippet'] for line in lines]\n",
        "train_target_docs = [line['intent'] for line in lines]\n",
        "test_docs = [line['rewritten_intent'] for line in lines if line['rewritten_intent']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-t0x8A5w0v87",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('conala-corpus/conala-test.json', 'r') as f:\n",
        "    lines = json.load(f)\n",
        "test_source_docs = [line['snippet'] for line in lines]\n",
        "test_target_docs = [line['intent'] for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EhmB0qpr3RD5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Other Data Sources (For Later Use)\n",
        "\n",
        "The below datasets are alternate sources of data for this same exercise.  We will not be reviewing these data as part of this tutorial.  However, we encourage you to inspect these data for additional practice and to get more intuition regarding these techniques.  Practicing with these other datasets will  give you confidence regarding the general application of the techniques we are teaching in this tutorial."
      ]
    },
    {
      "metadata": {
        "id": "oQLV9P8h0v8q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### [English to French](http://www.manythings.org/anki/)"
      ]
    },
    {
      "metadata": {
        "id": "1uK2D4090v8r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !wget http://www.manythings.org/anki/fra-eng.zip\n",
        "# !unzip -o fra-eng.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8UMp9B4M0v8u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with open('fra.txt', 'r') as f:\n",
        "#     lines = f.readlines()\n",
        "# target_docs, source_docs = zip(*[line.strip().split('\\t') for line in lines])\n",
        "# target_docs = list(set(target_docs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bw33N_AcPZqp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### GitHub issues data"
      ]
    },
    {
      "metadata": {
        "id": "GSR2uek3PZqq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# issues = pd.read_csv('https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip')\n",
        "# source_docs = list(issues.body)\n",
        "# target_docs = list(issues.issue_title)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AP58__OgPZqt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Python (function, docstring) pairs\n",
        "\n",
        "Purpose of this dataset is to see if you can generate the docstring of a python function or method by looking at the code."
      ]
    },
    {
      "metadata": {
        "id": "izpU8EXGPZqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# f = urlopen('https://storage.googleapis.com/kubeflow-examples/code_search/data/train.function')\n",
        "# source_docs = [line.decode('utf-8') for line in f.readlines()]\n",
        "# f = urlopen('https://storage.googleapis.com/kubeflow-examples/code_search/data/train.docstring')\n",
        "# target_docs = [line.decode('utf-8') for line in f.readlines()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8JykNMln0v9I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use subset of the data\n",
        "\n",
        "We will use only of the training set in the interest of brevity.  However, we can use the full dataset in a subsequent pass if desired."
      ]
    },
    {
      "metadata": {
        "id": "Hj1tDqDFPZqx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "source_docs = source_docs[:100000]\n",
        "target_docs = target_docs[:100000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lxYjchOjPZqz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Language Model\n",
        "\n",
        "What is a language model?\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1440/1*XGfyUGtWq0yZ4RfufYfbRw.jpeg)\n",
        "\n",
        "Source: https://medium.com/paper-club/language-modeling-survey-333077e43dd9"
      ]
    },
    {
      "metadata": {
        "id": "COkelzEXPZq0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "Tokenize, generate vocabulary, apply padding and vectorize."
      ]
    },
    {
      "metadata": {
        "id": "PYXCJuN69MOo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Keras Text Pre-Processing Primer\n",
        "\n",
        "Now that we have gathered the data, we need to prepare the data for the modeling. Before jumping into the code, let’s warm up with a toy example of two documents:\n",
        "\n",
        "```\n",
        "[“The quick brown fox jumped over the lazy dog 42 times.”, “The dog is lazy”]\n",
        "```\n",
        "\n",
        "Below is a rough outline of the steps I will take in order to pre-processes this raw text:\n",
        "\n",
        "**1. Clean text:** in this step, we want to remove or replace specific characters and lower case all the text. This step is discretionary and depends on the size of the data and the specifics of your domain. In this toy example, I lower-case all characters and replace numbers with *number* in the text. In the real data, I handle more scenarios.\n",
        "\n",
        "[“the quick brown fox jumped over the lazy dog *number* times”, “the dog is lazy”]\n",
        "\n",
        "\n",
        "**3. Tokenize:** split each document into a list of words\n",
        "\n",
        "```\n",
        "[[‘the’, ‘quick’, ‘brown’, ‘fox’, ‘jumped’, ‘over’, ‘the’, ‘lazy’, ‘dog’, ‘*number*’, ‘times’], [‘the’, ‘dog’, ‘is’, ‘lazy’]]\n",
        "```\n",
        "\n",
        "**4. Build vocabulary:** You will need to represent each distinct word in your corpus as an integer, which means you will need to build a map of token -> integers. Furthermore, I find it useful to reserve an integer for rare words that occur below a certain threshold as well as 0 for padding (see next step). After you apply a token -> integer mapping, your data might look like this:\n",
        "\n",
        "```\n",
        "[[2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 11], [2, 9, 12, 8]]\n",
        "```\n",
        "\n",
        "**5. Padding:** 5. Padding: You will have documents that have different lengths. There are many strategies on how to deal with this for deep learning, however for this tutorial I will pad and truncate documents such that they are all transformed to the same length for simplicity. You can decide to pad (with zeros) and truncate your document at the beginning or end, which I will refer to as “pre” and “post” respectively. After pre-padding our toy example, the data might look like this:\n",
        "\n",
        "```\n",
        "[[2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 11], [0, 0, 0, 0, 0, 0, 0, 2, 9, 12, 8]]\n",
        "```\n",
        "\n",
        "A reasonable way to decide your target document length is to build a histogram of document lengths and choose a sensible number. (Note that the above example has padded the data in front but we could also pad at the end. We will discuss this more in the next section)."
      ]
    },
    {
      "metadata": {
        "id": "6b7dZOxkDutv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Inspect the raw text of source and target documents:\n",
        "\n",
        "Source docs:\n"
      ]
    },
    {
      "metadata": {
        "id": "AdyYWgT_Eu13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "729fc1ef-d230-4c0e-841c-f58308f6aeaa"
      },
      "cell_type": "code",
      "source": [
        "for x in source_docs[:10]:\n",
        "  print(x)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sorted(l, key=lambda x: (-int(x[1]), x[0]))\n",
            "[int(x) for x in str(num)]\n",
            "c.decode('unicode_escape')\n",
            "parser.add_argument('-t', dest='table', help='', nargs='+')\n",
            "datetime.datetime.strptime(s, '%Y-%m-%dT%H:%M:%SZ')\n",
            "np.array(x._data).reshape(x.size[::-1]).T\n",
            "soup.get_text().replace('\\n', '\\n\\n')\n",
            "re.sub('(?<!\\\\S)((\\\\S+)(?:\\\\s+\\\\2))(?:\\\\s+\\\\2)+(?!\\\\S)', '\\\\1', s)\n",
            "mylist.sort(key=lambda d: (d['weight'], d['factor']))\n",
            "itertools.combinations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hT8NcmEU4C4A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Target docs:"
      ]
    },
    {
      "metadata": {
        "id": "Ww4AHHUX4CPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "8d992f36-eae2-4dab-d7fd-4e4d725dee28"
      },
      "cell_type": "code",
      "source": [
        "target_docs[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sort a nested list by two elements',\n",
              " 'converting integer to list in python',\n",
              " 'Converting byte string in unicode string',\n",
              " 'List of arguments with argparse',\n",
              " 'How to convert a Date string to a DateTime object?',\n",
              " 'How to efficiently convert Matlab engine arrays to numpy ndarray?',\n",
              " 'Converting html to text with Python',\n",
              " 'regex for repeating words in a string in Python',\n",
              " 'Ordering a list of dictionaries in python',\n",
              " 'Two Combination Lists from One List']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "LTxQMjaa4WsL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to pre-process this data, we will use the [`ktext` package](https://github.com/hamelsmu/ktext).   `ktext` helps accomplish the pre-processing steps outlined in the previous section. This library is a thin wrapper around keras and spacy text processing utilities, and leverages python process-based-threading to speed things up. It also chains all of the pre-processing steps together and provides a bunch of convenience functions. Warning: this package is under development so use with caution outside this tutorial (pull requests are welcome!). To learn more about how this library works, look at this [tutorial](https://github.com/hamelsmu/ktext/blob/master/notebooks/Tutorial.ipynb) (but for now I suggest reading ahead)."
      ]
    },
    {
      "metadata": {
        "id": "uHOMxrONPZq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "0677e9e5-8b6e-4898-c447-9f4f72c137a1"
      },
      "cell_type": "code",
      "source": [
        "proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
        "vecs = proc.fit_transform(target_docs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:Setting maximum document length to 10 based upon hueristic of 0.7 percentile.\n",
            " See full histogram by insepecting the `document_length_stats` attribute.\n",
            "WARNING:root:(1/2) done. 22 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 100,000 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 1 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0jld4HT0Dp_r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above code cleans, tokenizes, and applies pre-padding and post-truncating such that each document length is equal to the 70th percentile of document lengths, which is an arbitrary choice. I made decisions about padding length by studying histograms of document length provided by ktext. Furthermore, only the top 20,000 words in the vocabulary are retained and remaining words are set to the index 1 which correspond to rare words (this was another arbitrary choice). \n",
        "\n",
        "Below is an example where tokens are mapped to integers."
      ]
    },
    {
      "metadata": {
        "id": "-ogmGKqUC-Jd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5841c855-4546-495c-c1f7-ed9e0ba0625a"
      },
      "cell_type": "code",
      "source": [
        "print('original list: ', target_docs[0])\n",
        "print('tokenized list: ', vecs[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original list:  Sort a nested list by two elements\n",
            "tokenized list:  [  0   0   0 118   2 151  10  43  38  56]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ldna19QcEXYp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see the most common words here, by calling the `token_count_pandas()` method."
      ]
    },
    {
      "metadata": {
        "id": "dNOluhQpD8Fq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "b0863ab5-1168-4354-ff70-d29ca911db37"
      },
      "cell_type": "code",
      "source": [
        "proc.token_count_pandas().head(20)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>52824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>python</th>\n",
              "      <td>48171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>47702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>47281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>how</th>\n",
              "      <td>36116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>22831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>with</th>\n",
              "      <td>15954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>13505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>list</th>\n",
              "      <td>12651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>from</th>\n",
              "      <td>11626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>i</th>\n",
              "      <td>9902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>using</th>\n",
              "      <td>8372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>string</th>\n",
              "      <td>7917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>7140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number</th>\n",
              "      <td>6636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>file</th>\n",
              "      <td>6383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>do</th>\n",
              "      <td>6142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pandas</th>\n",
              "      <td>6108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>for</th>\n",
              "      <td>5724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>matplotlib</th>\n",
              "      <td>5640</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            count\n",
              "a           52824\n",
              "python      48171\n",
              "in          47702\n",
              "to          47281\n",
              "how         36116\n",
              "of          22831\n",
              "with        15954\n",
              "the         13505\n",
              "list        12651\n",
              "from        11626\n",
              "i            9902\n",
              "using        8372\n",
              "string       7917\n",
              "and          7140\n",
              "number       6636\n",
              "file         6383\n",
              "do           6142\n",
              "pandas       6108\n",
              "for          5724\n",
              "matplotlib   5640"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "Ab3JU2L5Ey9_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Furthermore, the documents in our corpus have different lengths. By setting `hueristic_pct_padding=.7`, `ktext` will truncate and pad all sequences to the 70th percentile length. However, it can be useful to sanity check a histogram of lengths. We inspect the `document_length_stats` property below which displays a histogram of document lengths."
      ]
    },
    {
      "metadata": {
        "id": "ZIH4R1HREiTO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "60952097-4e40-473e-e91b-f83eac00428e"
      },
      "cell_type": "code",
      "source": [
        "proc.document_length_stats"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bin</th>\n",
              "      <th>doc_count</th>\n",
              "      <th>cumsum_pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>0.00031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>34978</td>\n",
              "      <td>0.35009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>50700</td>\n",
              "      <td>0.85709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15</td>\n",
              "      <td>12664</td>\n",
              "      <td>0.98373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20</td>\n",
              "      <td>1486</td>\n",
              "      <td>0.99859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>25</td>\n",
              "      <td>124</td>\n",
              "      <td>0.99983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30</td>\n",
              "      <td>17</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bin  doc_count  cumsum_pct\n",
              "6    0         31     0.00031\n",
              "0    5      34978     0.35009\n",
              "1   10      50700     0.85709\n",
              "2   15      12664     0.98373\n",
              "3   20       1486     0.99859\n",
              "5   25        124     0.99983\n",
              "4   30         17     1.00000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "SCNn24hRDxeF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is useful to keep track of the maximum length and the unique number of tokens in the corpus for later purposes."
      ]
    },
    {
      "metadata": {
        "id": "wnjhEa-NPZq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e3ba8e84-339c-4704-d9d0-c15e9f277dba"
      },
      "cell_type": "code",
      "source": [
        "vocab_size = max(proc.id2token.keys()) + 1\n",
        "max_length = proc.padding_maxlen\n",
        "\n",
        "print('vocab size: ', vocab_size)\n",
        "print('max length allowed for documents: ', max_length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size:  10225\n",
            "max length allowed for documents:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "srQ2EApW0v9X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Language model architecture"
      ]
    },
    {
      "metadata": {
        "id": "V31lnrq70v9T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Prepare training data for language model."
      ]
    },
    {
      "metadata": {
        "id": "Ioq7Sd9EPZq8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "for arr in tqdm(vecs):\n",
        "    non_zero = (arr != 0).argmax()\n",
        "    for i in range(non_zero, len(arr)):\n",
        "        sequences.append(arr[:i+1])\n",
        "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "# y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WKW7TCl6PZq_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = Input(shape=(max_length-1,))\n",
        "x = Embedding(vocab_size, 256, input_length=max_length-1)(i)\n",
        "x = LSTM(256, return_sequences=True)(x)\n",
        "last_timestep = Lambda(lambda x: x[:, -1, :])(x)\n",
        "last_timestep = Dense(vocab_size, activation='softmax')(last_timestep)\n",
        "model = Model(i, last_timestep)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jnVeYIZV0v9a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Now that we have created our architecture, we can train our model.  \n",
        "\n",
        "**This step takes approximately 20 minutes.  This is a good time to take a bathroom break!**"
      ]
    },
    {
      "metadata": {
        "id": "Psve_dWVPZrB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(X, y, epochs=10, batch_size=50, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFH1DmGHPZrE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate sequence\n",
        "\n",
        "The goal of a language model is to predict the next word in a sequence. To sanity check the language model, we will see what kind of sentence is generated when we start with a a seed word of 'is'. We are looking to see if the sentence generated appears to be sampled from the distribution of the data.\n",
        "\n",
        "In other words does the sentence generated look like it was written by the same author(s) pertaining to the same domain as the training corpus?"
      ]
    },
    {
      "metadata": {
        "id": "2R-2I_HRPZrE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_seq(model, proc, n_words, seed_text):\n",
        "    in_text = seed_text\n",
        "    for _ in range(n_words):\n",
        "        vec = proc.transform([in_text])[:,1:]\n",
        "        index = np.argmax(model.predict(vec, verbose=0), axis=1)[0]\n",
        "        out_word = ''\n",
        "        if index == 1:\n",
        "            out_word = '_unk_'\n",
        "        else:\n",
        "            out_word = proc.id2token[index]\n",
        "        in_text += ' ' + out_word\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MCDhy30KqF2j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "See what sentence is generated from language model, seeded witht he word `is`."
      ]
    },
    {
      "metadata": {
        "id": "8wpHSxIOPZrH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_seq(model, proc, max_length, 'is')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zzNZqTqVPZrJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate sentence embeddings\n",
        "\n",
        "One of the goals of training the language model is learning reprsentations of sentences in our corpus. For example, we can extract values from intermediate layers of this language model, and use those as sentence embeddings."
      ]
    },
    {
      "metadata": {
        "id": "Y-Rig9cePZrM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_model = Model(inputs=model.inputs, outputs=model.layers[-3].output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fW1UeLKWqkMp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The below code extracts the hidden states from the encoder when given an input. There is one hidden state for each word in the sentence."
      ]
    },
    {
      "metadata": {
        "id": "X-FFznLQPZrO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_sequence = test_docs[random.randint(0, len(test_docs))]\n",
        "print('input sequence: ', input_sequence, '\\n\\nhidden states:\\n')\n",
        "vec = proc.transform([input_sequence])[:,1:]\n",
        "embedding_model.predict(vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PBksOOsJrTkk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's extract the hidden states for all the sentences in our training data."
      ]
    },
    {
      "metadata": {
        "id": "haQjC19y0v9l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vecs = proc.transform(test_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rLeLELte0v9n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_states = embedding_model.predict(vecs[:, 1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aF90GHpSraaK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To create a sentence embedding, we need to summarize the hidden states (there is one for each term). A simple approach is to use aggregate stastics like the mean, max, or the sum of all the hidden states. There are other approaches that are outside the scope of this notebook, but that we will discuss."
      ]
    },
    {
      "metadata": {
        "id": "SCf0kjpr0v9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mean_vecs = np.mean(hidden_states, axis=1)\n",
        "max_vecs = np.max(hidden_states, axis=1)\n",
        "sum_vecs = np.sum(hidden_states, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "giC48Kt00v9s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Application - Nearest Neighbor Search\n",
        "\n",
        "Now that we have a way to represent each sentence as a vector, we can use this representation on many kinds of downstream tasks. One such task is finding a similar sentence to any given sentence.\n",
        "\n",
        "### Build vector indices\n",
        "\n",
        "We will first place all the vectorized sentences in a special data structure that allows for fast nearest neighbor lookups. We will use [annoy](https://github.com/spotify/annoy) for this purpose."
      ]
    },
    {
      "metadata": {
        "id": "afBInH4C0v9s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dimension = hidden_states.shape[-1]\n",
        "index = AnnoyIndex(dimension)\n",
        "for i, v in enumerate(sum_vecs):\n",
        "    index.add_item(i, v)\n",
        "index.build(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pp2KAWNj0v9u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Search nearest neighbors"
      ]
    },
    {
      "metadata": {
        "id": "tr_tKEDx0v9w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_sequence = test_docs[random.randint(0, len(test_docs))]\n",
        "print('Query: ', input_sequence)\n",
        "\n",
        "vec = proc.transform([input_sequence])[:,1:]\n",
        "vec = np.sum(embedding_model.predict(vec), axis=1)\n",
        "ids, _ = index.get_nns_by_vector(vec.T, 10, include_distances=True)\n",
        "\n",
        "print('Search Results:')\n",
        "[test_docs[i] for i in ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pnPW20o9PZrR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sequence to Sequence Model"
      ]
    },
    {
      "metadata": {
        "id": "VpxbvU6OCDje",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A [sequence to sequence model](https://towardsdatascience.com/how-to-create-data-products-that-are-magical-using-sequence-to-sequence-models-703f86a231f8) allows you to take an input sequence (source), and predict an output sequence (target).  These sequences can be anything, however we will focus on natural language for this tutorial. Sequence-to-sequence models have been used with great success in summarizing texts as well as generating translations from one language to another. For this tutorial, we will demonstrate a very creative task: given a snippet of code, we will train a model that generates a description of that code!\n",
        "\n",
        "### Sequence to Sequence Primer\n",
        "\n",
        "There are many variants of seq2seq models, however we will walk through one of the most simplest forms: an encoder-decoder network using RNNs.\n",
        "\n",
        "#### Training\n",
        "\n",
        "The decoder receives the ground truth, shifted by one time-step (is allowed to see the ground-truth of the previous time step).  This is called teacher forcing.   \n",
        "\n",
        "![alt text](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)\n",
        "\n",
        "#### Inference\n",
        "\n",
        "At inference time, we will not be able to see the ground truth from the last time step.   Therefore we can use the last predicted output in place of the previous time step's ground truth.  We will generate our sequence this way using a greedy approach, stopping only when we either reach a maximum length or predict a special <stop> token.  There are more sophisticated ways of generating sequences such as using [beam search](https://en.wikipedia.org/wiki/Beam_search) that we will not cover in this tutorial.\n",
        "\n",
        "![alt text](https://blog.keras.io/img/seq2seq/seq2seq-inference.png)\n",
        "\n",
        "Credit: https://blog.keras.io/category/tutorials.html\n",
        "\n",
        "\n",
        "**Building a neural network architecture is like stacking lego bricks.** For beginners, it can be useful to think of each layer as an API: you send the API some data and then the API returns some data. Thinking of things this way frees you from becoming overwhelmed, and you can build your understanding of things slowly. It is important to understand two concepts:\n",
        "\n",
        "the shape of data that each layer expects, and the shape of data the layer will return. (When you stack many layers on top of each other, the input and output shapes must be compatible, like legos).\n",
        "conceptually, what will the output(s) of a layer represent? What does the output of a subset of stacked layers represent?\n",
        "\n",
        "Let's take a look at the data we want to use. The `source` is the snippet of code and the `target` is the description of that code."
      ]
    },
    {
      "metadata": {
        "id": "ESQyR0l2CH01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "28dbd16c-acb3-41a9-fb39-d059f31ecc83"
      },
      "cell_type": "code",
      "source": [
        "print('source (code input): ', source_docs[2])\n",
        "print('target (description output): ', target_docs[2])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source (code input):  c.decode('unicode_escape')\n",
            "target (description output):  Converting byte string in unicode string\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BQsnI6720v9z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "\n",
        "Similar to previous excercises, we must pre-process the raw strings into a format that can be utilized by our model. One such format is to map each word in our corpus to a unique integer value, which we will refer to as a vocabulary. If the source and target are from the same distribution, (which they are not in this example) the vocabulary can be shared.\n",
        "\n",
        "\n",
        "Concretely, we will tokenize, generate vocabulary, apply padding and vectorize. These steps are as follows:\n",
        "\n",
        "**1. Tokenize:** Process of parsing strings into discrete words or tokens.\n",
        "\n",
        "**2. Generate Vocabulary:** Assign each token to a unique integer, rare-occuring tokens may be assigned to the same integer.\n",
        "\n",
        "**3. Padding:** We standardize the sequence length of each example to be the same by truncating and padding each example to the same lentgh.\n",
        "\n",
        "The `ktext` package helps us accomplish these steps."
      ]
    },
    {
      "metadata": {
        "id": "s_nE-SeKPZrS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "source_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
        "source_vecs = source_proc.fit_transform(source_docs)\n",
        "\n",
        "target_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
        "target_vecs = target_proc.fit_transform(target_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qkfDO83HE8Xc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will use teacher forcing for the decoder of the sequence to sequence model, so we will offset the target sequence by one."
      ]
    },
    {
      "metadata": {
        "id": "3b0wFryNPZrU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_input_data = source_vecs\n",
        "encoder_seq_len = encoder_input_data.shape[1]\n",
        "\n",
        "decoder_input_data = target_vecs[:, :-1]\n",
        "decoder_target_data = target_vecs[:, 1:]\n",
        "\n",
        "num_encoder_tokens = max(source_proc.id2token.keys()) + 1\n",
        "num_decoder_tokens = max(target_proc.id2token.keys()) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FCkBg1_1PZrZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder model"
      ]
    },
    {
      "metadata": {
        "id": "wvDkJOqoFHhf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The role of the encoder is to extract features and generate a representation of the input sequence, which in this case is a snippet of code. "
      ]
    },
    {
      "metadata": {
        "id": "nThkgdknPZra",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_emb_dim=512\n",
        "hidden_state_dim=1024\n",
        "encoder_seq_len=encoder_seq_len\n",
        "num_encoder_tokens=num_encoder_tokens\n",
        "num_decoder_tokens=num_decoder_tokens\n",
        "\n",
        "encoder_inputs = Input(shape=(encoder_seq_len,), name='Encoder-Input')\n",
        "x = Embedding(num_encoder_tokens, word_emb_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
        "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "_, state_h = GRU(hidden_state_dim, return_state=True, name='Encoder-Last-GRU', dropout=.5)(x)\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8PnsgReVPZre",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decoder model"
      ]
    },
    {
      "metadata": {
        "id": "t9IbLqTCFQr3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The role of the decoder is to generate a description of the code conditioned on the features extracted by the encoder."
      ]
    },
    {
      "metadata": {
        "id": "bu-KgmfjPZre",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None,), name='Decoder-Input')\n",
        "dec_emb = Embedding(num_decoder_tokens, word_emb_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
        "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "decoder_gru = GRU(hidden_state_dim, return_state=True, return_sequences=True, name='Decoder-GRU', dropout=.5)\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
        "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WfH-WvGwPZrg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sequence to sequence model"
      ]
    },
    {
      "metadata": {
        "id": "kW3bg3XnFYqx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can connect the encoder and decoder together to create the sequence to sequence model."
      ]
    },
    {
      "metadata": {
        "id": "xsNgKYlKPZri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq2seq_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WT8maZlD0v-A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "JxHP00h6FgL0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The below hyperparameters were found through some trial and error.\n",
        "\n",
        "**This should take approximately ~ 35 minutes to train.**"
      ]
    },
    {
      "metadata": {
        "id": "HgUemXCbPZrk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "epochs = 16\n",
        "\n",
        "seq2seq_model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = seq2seq_model.fit([encoder_input_data, decoder_input_data],\n",
        "                            np.expand_dims(decoder_target_data, -1),\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1lbdGPHE0v-B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extract encoder and decoder models"
      ]
    },
    {
      "metadata": {
        "id": "xcl66yGRF11e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For inference, we will not have teacher forcing for the decoder. Therefore, we must re-assemble our model such we can feed one prediction at a time."
      ]
    },
    {
      "metadata": {
        "id": "UAqk3fIwPZrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def extract_decoder_model(model):\n",
        "    latent_dim = model.get_layer('Encoder-Model').output_shape[-1]\n",
        "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
        "    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
        "    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
        "    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
        "    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
        "    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
        "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
        "    decoder_model = Model([decoder_inputs, gru_inference_state_input], [dense_out, gru_state_out])\n",
        "    return decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PU8JuOvfGKta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One side effect of training a sequence-to-sequence model in this way is that the encoder can be re-used as a general purpose feature extractor. We extract the encoder below for this purpose in a later exercise."
      ]
    },
    {
      "metadata": {
        "id": "Xe0_KakGPZrp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_model = seq2seq_model.get_layer('Encoder-Model')\n",
        "for layer in encoder_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "decoder_model = extract_decoder_model(seq2seq_model)\n",
        "decoder_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZFdwR6UR0v-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Predict code descriptions using the trained sequence-to-sequence model\n",
        "\n",
        "You will see that the predicted descriptions are not perfect, but seem to be picking up on correlations between common code token sequences and natural language descriptions of that code.\n",
        "\n",
        "Feel free to run the below block of code as many times as you want. A new random sample from the test set will be drawn each time."
      ]
    },
    {
      "metadata": {
        "id": "JvK1eN9fPZrr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = random.randint(0, len(test_source_docs))\n",
        "\n",
        "max_len = target_proc.padding_maxlen\n",
        "raw_input_text = test_source_docs[i]\n",
        "\n",
        "raw_tokenized = source_proc.transform([raw_input_text])\n",
        "encoding = encoder_model.predict(raw_tokenized)\n",
        "original_encoding = encoding\n",
        "state_value = np.array(target_proc.token2id['_start_']).reshape(1, 1)\n",
        "\n",
        "decoded_sentence = []\n",
        "stop_condition = False\n",
        "while not stop_condition:\n",
        "    preds, st = decoder_model.predict([state_value, encoding])\n",
        "    pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
        "    pred_word_str = target_proc.id2token[pred_idx]\n",
        "\n",
        "    if pred_word_str == '_end_' or len(decoded_sentence) >= max_len:\n",
        "        stop_condition = True\n",
        "        break\n",
        "    decoded_sentence.append(pred_word_str)\n",
        "\n",
        "    # update the decoder for the next word\n",
        "    encoding = st\n",
        "    state_value = np.array(pred_idx).reshape(1, 1)\n",
        "\n",
        "print('sample code from test set:\\n------------------------\\n', raw_input_text)\n",
        "print('\\nground truth:\\n------------------------\\n', test_target_docs[i])\n",
        "print('\\npredicted description:\\n------------------------')\n",
        "print(' '.join(decoded_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tf2bITb3Ny7H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "qYPM3Tqf0v-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_source_emb = encoder_model.predict(source_proc.transform(train_source_docs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "geb3SOyk0v-Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vecs = proc.transform(train_target_docs)\n",
        "hidden_states = embedding_model.predict(vecs[:, 1:])\n",
        "mean_vecs = np.mean(hidden_states, axis=1)\n",
        "max_vecs = np.max(hidden_states, axis=1)\n",
        "sum_vecs = np.sum(hidden_states, axis=1)\n",
        "train_target_emb = sum_vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_f5ptIL0v-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('source embedding shape on training set: ', train_source_emb.shape)\n",
        "print('target embedding shape on training set: ', train_target_emb.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8xOKEWFj0v-S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Construct a Joint Vector Space\n",
        "\n",
        "Right now we have a way of representing:\n",
        "- a blob of code as a vector using the encoder of the sequence-to-sequence model, and \n",
        "- the code descriptions as a vector using the language model.\n",
        "\n",
        "However, these two vector spaces are not related to eachother. It can be useful to project the vectors for code and descriptions into the same space so that we can search code with natural language. There are many ways of accomplishing this task, however we will demonstrate a technique inspired from [this paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf), where we use regression to \"pull\" these vectors into the same space."
      ]
    },
    {
      "metadata": {
        "id": "-noqpPJY0v-T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inp = Input(shape=(train_source_emb.shape[1],))\n",
        "x = Dense(train_target_emb.shape[1], use_bias=False)(inp)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Dense(512)(x)\n",
        "modal_model = Model([inp], x)\n",
        "modal_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7zALzqWO0v-U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "modal_model.compile(optimizer=optimizers.Nadam(lr=0.002), loss='cosine_proximity', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 1024\n",
        "epochs = 10\n",
        "history = modal_model.fit([train_source_emb], train_target_emb,\n",
        "                          batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Re8nxdPs0v-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Application - Semantic Search\n",
        "\n",
        "### Use test data"
      ]
    },
    {
      "metadata": {
        "id": "ZX0AIo_B0v-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_source_emb = encoder_model.predict(source_proc.transform(test_source_docs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2BaHfbsl0v-a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vecs = proc.transform(test_target_docs)\n",
        "hidden_states = embedding_model.predict(vecs[:, 1:])\n",
        "mean_vecs = np.mean(hidden_states, axis=1)\n",
        "max_vecs = np.max(hidden_states, axis=1)\n",
        "sum_vecs = np.sum(hidden_states, axis=1)\n",
        "test_target_emb = sum_vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QgKykGVh0v-b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(test_source_emb.shape)\n",
        "print(test_target_emb.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eg7JX1Vu0v-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build vector indices"
      ]
    },
    {
      "metadata": {
        "id": "D7HKpc3j0v-d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dimension = hidden_states.shape[-1]\n",
        "index = AnnoyIndex(dimension)\n",
        "for i, v in enumerate(test_target_emb):\n",
        "    index.add_item(i, v)\n",
        "index.build(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C-fpIqvQ0v-f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Search nearest neighbors"
      ]
    },
    {
      "metadata": {
        "id": "zPlMXs500v-g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = random.randint(0, len(test_source_docs))\n",
        "input_sequence = test_source_docs[i]\n",
        "print(input_sequence)\n",
        "\n",
        "vec = np.expand_dims(test_source_emb[i], 0)\n",
        "out_vec = modal_model.predict(vec)\n",
        "ids, _ = index.get_nns_by_vector(out_vec.T, 10, include_distances=True)\n",
        "[test_target_docs[i] for i in ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U-u1WX4EQa83",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
