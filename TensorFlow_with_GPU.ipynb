{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/hamelsmu/kdd-2018-hands-on-tutorials/blob/master/TensorFlow_with_GPU.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "BlmQIFSLZDdc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Confirm TensorFlow can see the GPU\n",
        "\n",
        "Simply select \"GPU\" in the Accelerator drop-down in Notebook Settings (either through the Edit menu or the command palette at cmd/ctrl-shift-P)."
      ]
    },
    {
      "metadata": {
        "id": "3IEVK-KFxi5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd8bbc57-ba51-4367-9fa5-a0b576d1cdf1"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k7T4nl7dupDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "facecfa8-db32-4c82-f5df-80dd98343f89"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "issues = pd.read_csv('https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QXRh0DPiZRyG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a typical convolutional neural network layer over a\n",
        "random image and manually places the resulting ops on either the CPU or the GPU\n",
        "to compare execution speed."
      ]
    },
    {
      "metadata": {
        "id": "HLyZlSlbudX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9094ec8f-46f3-415a-afff-fbf00406ff10"
      },
      "cell_type": "code",
      "source": [
        "import urllib2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-C3DuMzzvrQf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6c332394-1c26-47ee-96ab-1dfb503ebcab"
      },
      "cell_type": "code",
      "source": [
        "f = urllib2.urlopen('https://storage.googleapis.com/kubeflow-examples/code_search/data/train.function')\n",
        "source_docs = f.readlines()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PFLTVNvEv0l1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17054
        },
        "outputId": "a0cbca89-77e6-4a66-91e8-ddd9d4a5ccec"
      },
      "cell_type": "code",
      "source": [
        "source_docs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['def batch_generator batch_size data labels None n_batches int np ceil len data float batch_size idx np random permutation len data data_shuffled data idx if labels is not None labels_shuffled labels idx for i in range n_batches start i batch_size end start batch_size if labels is not None yield data_shuffled start end labels_shuffled start end else yield data_shuffled start end\\n',\n",
              " 'def to_categorical labels num_classes new_labels np zeros len labels num_classes label_to_idx_map idx_to_label_map dict dict idx 0 for i label in enumerate labels if label not in label_to_idx_map label_to_idx_map label idx idx_to_label_map idx label idx 1 new_labels i label_to_idx_map label 1 return new_labels label_to_idx_map idx_to_label_map\\n',\n",
              " 'classmethod def function cls x return 1 1 0 np exp x\\n',\n",
              " 'classmethod def prime cls x return x 1 x\\n',\n",
              " 'classmethod def function cls x return np maximum np zeros x shape x\\n',\n",
              " 'classmethod def prime cls x return x 0 astype int\\n',\n",
              " 'classmethod def function cls x return np tanh x\\n',\n",
              " 'classmethod def prime cls x return 1 x x\\n',\n",
              " 'click group def main args None\\n',\n",
              " 'main command click argument name required True click argument directory required False default def newapp name directory top_dir os path abspath directory if not os path isdir top_dir raise ValueError Destination directory s does not exists please create it first top_dir app_dir os path join top_dir name if os path isdir app_dir raise ValueError s already exist app_dir bf_dir blueflask __path__ 0 app_template_dir os path join bf_dir templates app_template shutil copytree app_template_dir app_dir click echo Created a s app in s name app_dir\\n',\n",
              " 'def test_constants assert mu interface themes NIGHT_STYLE assert mu interface themes DAY_STYLE\\n',\n",
              " 'def test_Font f mu interface themes Font assert f color black assert f paper white assert f bold is False assert f italic is False f mu interface themes Font color pink paper black bold True italic True assert f color pink assert f paper black assert f bold assert f italic\\n',\n",
              " 'def test_theme_apply_to lexer mu interface editor PythonLexer theme mu interface themes DayTheme lexer setFont mock MagicMock return_value None lexer setColor mock MagicMock return_value None lexer setEolFill mock MagicMock return_value None lexer setPaper mock MagicMock return_value None theme apply_to lexer assert lexer setFont call_count 17 assert lexer setColor call_count 16 assert lexer setEolFill call_count 16 assert lexer setPaper call_count 16\\n',\n",
              " 'def actions self return name play display_name _ Play description _ Play your Pygame Zero game handler self play_toggle shortcut F5 name images display_name _ Images description _ Show the images used by Pygame Zero handler self show_images shortcut Ctrl Shift I name fonts display_name _ Fonts description _ Show the fonts used by Pygame Zero handler self show_fonts shortcut Ctrl Shift F name sounds display_name _ Sounds description _ Show the sounds used by Pygame Zero handler self show_sounds shortcut Ctrl Shift S name music display_name _ Music description _ Show the music used by Pygame Zero handler self show_music shortcut Ctrl Shift M\\n',\n",
              " 'def api self return SHARED_APIS PYTHON3_APIS PI_APIS PYGAMEZERO_APIS\\n',\n",
              " 'def play_toggle self event if self runner self stop_game play_slot self view button_bar slots play play_slot setIcon load_icon play play_slot setText _ Play play_slot setToolTip _ Play your Pygame Zero game else self run_game if self runner play_slot self view button_bar slots play play_slot setIcon load_icon stop play_slot setText _ Stop play_slot setToolTip _ Stop your Pygame Zero game\\n',\n",
              " 'def run_game self tab self view current_tab if tab is None logger debug There is no active text editor self stop_game return if tab path is None self editor save if tab path if tab isModified with open tab path w newline as f logger info Saving script to format tab path logger debug tab text write_and_flush f tab text tab setModified False logger debug tab text envars self editor envars self runner self view add_python3_runner tab path self workspace_dir interactive False envars envars runner pgzrun self runner process waitForStarted\\n',\n",
              " 'def stop_game self logger debug Stopping script if self runner self runner process kill self runner process waitForFinished self runner None self view remove_python_runner\\n',\n",
              " 'def show_images self event image_dir os path join self workspace_dir images self view open_directory_from_os image_dir\\n',\n",
              " 'def show_fonts self event image_dir os path join self workspace_dir fonts self view open_directory_from_os image_dir\\n',\n",
              " 'def show_sounds self event sound_dir os path join self workspace_dir sounds self view open_directory_from_os sound_dir\\n',\n",
              " 'def show_music self event sound_dir os path join self workspace_dir music self view open_directory_from_os sound_dir\\n',\n",
              " 'def test_gettext_translation old_lc_all os environ get LC_ALL None os environ LC_ALL es_ES UTF 8 with mock patch gettext translation as translation importlib reload mu if old_lc_all os environ LC_ALL old_lc_all else del os environ LC_ALL assert translation call_count 1 assert translation call_args 1 languages es\\n',\n",
              " 'def test_defaultlocale_type_error old_lc_all os environ get LC_ALL None os environ LC_ALL es_ES UTF 8 mock_locale mock MagicMock side_effect TypeError Ups with mock patch locale getdefaultlocale mock_locale mock patch gettext translation as translation mock patch traceback print_exc as print_exc importlib reload mu if old_lc_all os environ LC_ALL old_lc_all else del os environ LC_ALL assert print_exc call_count 1 assert translation call_count 1 assert translation call_args 1 languages en\\n',\n",
              " 'def test_defaultlocale_value_error old_lc_all os environ get LC_ALL None os environ LC_ALL es_ES UTF 8 mock_locale mock MagicMock side_effect ValueError Ups with mock patch locale getdefaultlocale mock_locale mock patch gettext translation as translation mock patch traceback print_exc as print_exc importlib reload mu if old_lc_all os environ LC_ALL old_lc_all else del os environ LC_ALL assert print_exc call_count 1 assert translation call_count 1 assert translation call_args 1 languages en\\n',\n",
              " 'def path name resource_dir images return resource_filename __name__ resource_dir name\\n',\n",
              " 'def load_icon name return QIcon path name\\n',\n",
              " 'def load_pixmap name return QPixmap path name\\n',\n",
              " 'def load_stylesheet name return resource_string __name__ css name decode utf8\\n',\n",
              " 'def load_font_data name return resource_string __name__ fonts name\\n',\n",
              " 'def _process_code executable use_python args if use_python execution python executable else execution executable returncodes set for filepath in _walk INCLUDE_PATTERNS EXCLUDE_PATTERNS False p subprocess run execution filepath list args returncodes add p returncode for filepath in _walk mu INCLUDE_PATTERNS EXCLUDE_PATTERNS p subprocess run execution filepath list args returncodes add p returncode for filepath in _walk tests INCLUDE_PATTERNS EXCLUDE_PATTERNS p subprocess run execution filepath list args returncodes add p returncode return max returncodes\\n',\n",
              " 'def _rmfiles start_from pattern for filepath in _walk start_from pattern os remove filepath\\n',\n",
              " 'def export function _exported function __name__ function return function\\n',\n",
              " 'export def test pytest_args print ntest return subprocess run PYTEST list pytest_args returncode\\n',\n",
              " 'export def coverage print ncoverage return subprocess run PYTEST cov config coveragerc cov report term missing cov mu tests returncode\\n',\n",
              " 'export def pyflakes pyflakes_args print npyflakes os environ PYFLAKES_BUILTINS _ return _process_code PYFLAKES False pyflakes_args\\n',\n",
              " 'export def pycodestyle pycodestyle_args print nPEP8 args ignore E731 E402 W504 pycodestyle_args return _process_code PYCODESTYLE False args\\n',\n",
              " 'export def pep8 pep8_args return pycodestyle pep8_args\\n',\n",
              " 'export def check print nCheck clean pyflakes pycodestyle coverage\\n',\n",
              " 'export def clean print nClean _rmtree build _rmtree dist _rmtree mu egg info _rmtree coverage _rmtree docs build _rmtree lib _rmtree pynsist_pkgs _rmfiles pyc\\n',\n",
              " 'export def translateall if not os path exists PYGETTEXT raise RuntimeError pygettext py could not be found at s PYGETTEXT result subprocess run python PYGETTEXT mu mu debugger mu modes mu resources returncode print nNew messages pot file created print Remember to update the translation stringsfound in the locale directory return result\\n',\n",
              " 'export def run clean if not os environ get VIRTUAL_ENV raise RuntimeError Cannot run Mu your Python virtualenv is not activated subprocess run python m mu returncode\\n',\n",
              " 'export def dist check print Checks pass good to package subprocess run python setup py sdist bdist_wheel returncode\\n',\n",
              " 'export def publish_test dist print Packaging complete upload to PyPI return subprocess run twine upload r test sign dist returncode\\n',\n",
              " 'export def publish_live dist print Packaging complete upload to PyPI return subprocess run twine upload sign dist returncode\\n',\n",
              " 'export def win32 check print Building 32 bit Windows installer return subprocess run python win_installer py 32 returncode\\n',\n",
              " 'export def win64 check print Building 64 bit Windows installer return subprocess run python win_installer py 64 returncode\\n',\n",
              " 'export def docs cwd os getcwd os chdir docs try return subprocess run cmd c make bat html returncode finally os chdir cwd\\n',\n",
              " 'export def help module_doc sys modules __main__ __doc__ or check print module_doc n len module_doc n for command function in sorted _exported items doc function __doc__ if doc first_line doc splitlines 0 else first_line print make format command first_line\\n',\n",
              " 'def main command help args try function _exported command except KeyError raise RuntimeError No such command s command else return function args\\n',\n",
              " 'def test_ModeItem_init name item_name description item_description icon icon_name mock_text mock MagicMock mock_icon mock MagicMock mock_load mock MagicMock return_value icon with mock patch mu interface dialogs QListWidgetItem setText mock_text mock patch mu interface dialogs QListWidgetItem setIcon mock_icon mock patch mu interface dialogs load_icon mock_load mi mu interface dialogs ModeItem name description icon assert mi name name assert mi description description assert mi icon icon mock_text assert_called_once_with n format name description mock_load assert_called_once_with icon mock_icon assert_called_once_with icon\\n',\n",
              " 'def test_ModeSelector_setup editor mock MagicMock view mock MagicMock modes python PythonMode editor view adafruit AdafruitMode editor view microbit MicrobitMode editor view debugger DebugMode editor view current_mode python mock_item mock MagicMock with mock patch mu interface dialogs ModeItem mock_item ms mu interface dialogs ModeSelector ms setup modes current_mode day assert mock_item call_count 3\\n',\n",
              " 'def test_ModeSelector_setup_night_theme editor mock MagicMock view mock MagicMock modes python PythonMode editor view adafruit AdafruitMode editor view microbit MicrobitMode editor view current_mode python mock_item mock MagicMock mock_css mock MagicMock with mock patch mu interface dialogs ModeItem mock_item ms mu interface dialogs ModeSelector ms setStyleSheet mock_css ms setup modes current_mode night assert mock_item call_count 3 mock_css assert_called_once_with mu interface themes NIGHT_STYLE\\n',\n",
              " 'def test_ModeSelector_setup_contrast_theme editor mock MagicMock view mock MagicMock modes python PythonMode editor view adafruit AdafruitMode editor view microbit MicrobitMode editor view current_mode python mock_item mock MagicMock mock_css mock MagicMock with mock patch mu interface dialogs ModeItem mock_item ms mu interface dialogs ModeSelector ms setStyleSheet mock_css ms setup modes current_mode contrast assert mock_item call_count 3 mock_css assert_called_once_with mu interface themes CONTRAST_STYLE\\n',\n",
              " 'def test_ModeSelector_select_and_accept ms mu interface dialogs ModeSelector ms accept mock MagicMock ms select_and_accept ms accept assert_called_once_with\\n',\n",
              " 'def test_ModeSelector_get_mode ms mu interface dialogs ModeSelector ms result mock MagicMock return_value QDialog Accepted item mock MagicMock item icon name ms mode_list mock MagicMock ms mode_list currentItem return_value item result ms get_mode assert result name ms result return_value None with pytest raises RuntimeError ms get_mode\\n',\n",
              " 'def test_LogWidget_setup log this is the contents of a log file lw mu interface dialogs LogWidget lw setup log assert lw log_text_area toPlainText log assert lw log_text_area isReadOnly\\n',\n",
              " 'def test_EnvironmentVariablesWidget_setup envars name value evw mu interface dialogs EnvironmentVariablesWidget evw setup envars assert evw text_area toPlainText envars assert not evw text_area isReadOnly\\n',\n",
              " 'def test_MicrobitSettingsWidget_setup minify True custom_runtime_path foo bar mbsw mu interface dialogs MicrobitSettingsWidget mbsw setup minify custom_runtime_path assert mbsw minify isChecked assert mbsw runtime_path text foo bar\\n',\n",
              " 'def test_AdminDialog_setup log this is the contents of a log file settings envars name value minify True microbit_runtime foo bar ad mu interface dialogs AdminDialog ad setStyleSheet mock MagicMock ad setup log settings day assert ad log_widget log_text_area toPlainText log assert ad settings settings ad setStyleSheet assert_called_once_with mu interface themes DAY_STYLE\\n',\n",
              " 'def test_AdminDialog_setup_night log this is the contents of a log file settings envars name value minify True microbit_runtime foo bar ad mu interface dialogs AdminDialog ad setStyleSheet mock MagicMock ad setup log settings night ad setStyleSheet assert_called_once_with mu interface themes NIGHT_STYLE\\n',\n",
              " 'def test_LogDisplay_setup_contrast log this is the contents of a log file settings envars name value minify True microbit_runtime foo bar ad mu interface dialogs AdminDialog ad setStyleSheet mock MagicMock ad setup log settings contrast ad setStyleSheet assert_called_once_with mu interface themes CONTRAST_STYLE\\n',\n",
              " 'def test_adafruit_mode editor mock MagicMock view mock MagicMock am AdafruitMode editor view assert am name Adafruit CircuitPython assert am description is not None assert am icon adafruit assert am editor editor assert am view view actions am actions assert len actions 2 assert actions 0 name serial assert actions 0 handler am toggle_repl assert actions 1 name plotter assert actions 1 handler am toggle_plotter\\n',\n",
              " 'def test_adafruit_mode_no_charts editor mock MagicMock view mock MagicMock am AdafruitMode editor view with mock patch mu modes adafruit CHARTS False actions am actions assert len actions 1 assert actions 0 name serial assert actions 0 handler am toggle_repl\\n',\n",
              " 'def test_workspace_dir_posix_exists editor mock MagicMock view mock MagicMock am AdafruitMode editor view with open tests modes mount_exists txt rb as fixture_file fixture fixture_file read with mock patch os name posix with mock patch mu modes adafruit check_output return_value fixture assert am workspace_dir media ntoll CIRCUITPY\\n',\n",
              " 'def test_workspace_dir_posix_no_mount_command editor mock MagicMock view mock MagicMock am AdafruitMode editor view with open tests modes mount_exists txt rb as fixture_file fixture fixture_file read mock_check mock MagicMock side_effect FileNotFoundError fixture with mock patch os name posix mock patch mu modes adafruit check_output mock_check assert am workspace_dir media ntoll CIRCUITPY assert mock_check call_count 2 assert mock_check call_args_list 0 0 0 mount assert mock_check call_args_list 1 0 0 sbin mount\\n',\n",
              " 'def test_workspace_dir_posix_missing editor mock MagicMock view mock MagicMock am AdafruitMode editor view with open tests modes mount_missing txt rb as fixture_file fixture fixture_file read with mock patch os name posix with mock patch mu modes adafruit check_output return_value fixture mock patch mu modes adafruit MicroPythonMode workspace_dir as mpm mpm return_value foo assert am workspace_dir foo\\n',\n",
              " 'def test_workspace_dir_nt_exists mock_windll mock MagicMock mock_windll kernel32 mock MagicMock mock_windll kernel32 GetVolumeInformationW mock MagicMock mock_windll kernel32 GetVolumeInformationW return_value None editor mock MagicMock view mock MagicMock am AdafruitMode editor view with mock patch os name nt with mock patch os path exists return_value True return_value ctypes create_unicode_buffer CIRCUITPY with mock patch ctypes create_unicode_buffer return_value return_value ctypes windll mock_windll assert am workspace_dir A\\n',\n",
              " 'def test_workspace_dir_nt_missing mock_windll mock MagicMock mock_windll kernel32 mock MagicMock mock_windll kernel32 GetVolumeInformationW mock MagicMock mock_windll kernel32 GetVolumeInformationW return_value None editor mock MagicMock view mock MagicMock am AdafruitMode editor view with mock patch os name nt with mock patch os path exists return_value True return_value ctypes create_unicode_buffer 1024 with mock patch ctypes create_unicode_buffer return_value return_value mock patch mu modes adafruit MicroPythonMode workspace_dir as mpm mpm return_value foo ctypes windll mock_windll assert am workspace_dir foo\\n',\n",
              " 'def test_workspace_dir_unknown_os editor mock MagicMock view mock MagicMock am AdafruitMode editor view with mock patch os name foo with pytest raises NotImplementedError as ex am workspace_dir assert ex value args 0 OS foo not supported\\n',\n",
              " 'def test_api editor mock MagicMock view mock MagicMock am AdafruitMode editor view assert am api SHARED_APIS ADAFRUIT_APIS\\n',\n",
              " 'def __init__ self debugger super __init__ self debugger debugger self stopped False\\n',\n",
              " 'def worker self connected False tries 0 connection_attempts 50 pause_between_attempts 0 2 while not connected try self debugger socket socket socket socket AF_INET socket SOCK_STREAM self debugger socket connect self debugger host self debugger port connected True except ConnectionRefusedError as e tries 1 if tries connection_attempts self on_fail emit _ Connection timed out Is your machine slow or busy Free up some of the machine s resources and try again return time sleep pause_between_attempts except OSError as e self on_fail emit _ Could not find localhost Ensure you have 127 0 0 1 localhost in your etc hosts file return remainder b while not self stopped new_buffer None try new_buffer self debugger socket recv 1024 except Exception self stopped True if new_buffer if new_buffer endswith self debugger ETX terminator self debugger ETX pos new_buffer rfind self debugger ETX full_buffer remainder new_buffer pos else terminator None full_buffer remainder new_buffer commands full_buffer split self debugger ETX if terminator is None remainder commands pop else remainder b for command in commands command command decode utf 8 logger debug command self on_command emit command else logger debug Debug client closed break\\n',\n",
              " 'def __init__ self host port proc None self host host self port port self proc proc self view None super __init__\\n',\n",
              " 'def start self self listener_thread QThread self view view self command_handler CommandBufferHandler self self command_handler moveToThread self listener_thread self command_handler on_command connect self on_command self command_handler on_fail connect self on_fail self listener_thread started connect self command_handler worker self listener_thread start\\n',\n",
              " 'def on_command self command event data json loads command if hasattr self on_ format event getattr self on_ format event data\\n',\n",
              " 'def on_fail self message logger error message self view debug_on_fail message\\n',\n",
              " 'def stop self self command_handler stopped True self listener_thread quit self listener_thread wait if self proc is not None self output quit self socket shutdown socket SHUT_WR if self proc is not None self proc wait\\n',\n",
              " 'def output self event data try dumped json dumps event data encode utf 8 self socket sendall dumped Debugger ETX except OSError as e logger debug Debugger client error logger debug e except AttributeError as e logger debug Debugger client not connected to runner logger debug e\\n',\n",
              " 'def breakpoint self breakpoint try if isinstance breakpoint tuple filename line breakpoint return self bp_index filename line else return self bp_list breakpoint except KeyError raise UnknownBreakpoint\\n',\n",
              " 'def breakpoints self filename return self bp_index get filename\\n',\n",
              " 'def create_breakpoint self filename line temporary False self output break filename filename line line temporary temporary\\n',\n",
              " 'def enable_breakpoint self breakpoint self output enable bpnum breakpoint bpnum\\n',\n",
              " 'def disable_breakpoint self breakpoint self output disable bpnum breakpoint bpnum\\n',\n",
              " 'def ignore_breakpoint self breakpoint count self output ignore bpnum breakpoint bpnum count count\\n',\n",
              " 'def clear_breakpoint self breakpoint self output clear bpnum breakpoint bpnum\\n',\n",
              " 'def do_run self self output continue\\n',\n",
              " 'def do_step self self output step\\n',\n",
              " 'def do_next self self output next\\n',\n",
              " 'def do_return self self output return\\n',\n",
              " 'def on_bootstrap self breakpoints self bp_index self bp_list list True for bp_data in breakpoints self on_breakpoint_create bp_data self view debug_on_bootstrap\\n',\n",
              " 'def on_breakpoint_create self bp_data bp Breakpoint bp_data self bp_index setdefault bp filename setdefault bp line bp self bp_list append bp if bp enabled self view debug_on_breakpoint_enable bp else self view debug_on_breakpoint_disable bp\\n',\n",
              " 'def on_breakpoint_enable self bpnum bp self bp_list bpnum bp enabled True self view debug_on_breakpoint_enable bp\\n',\n",
              " 'def on_breakpoint_disable self bpnum bp self bp_list bpnum bp enabled False self view debug_on_breakpoint_disable bp\\n',\n",
              " 'def on_breakpoint_ignore self bpnum count bp self bp_list bpnum bp ignore count self view debug_on_breakpoint_ignore bp count\\n',\n",
              " 'def on_breakpoint_clear self bpnum bp self bp_list bpnum self view debug_on_breakpoint_clear bp\\n',\n",
              " 'def on_stack self stack self stack stack self view debug_on_stack stack\\n',\n",
              " 'def on_restart self self view debug_on_restart\\n',\n",
              " 'def on_finished self self view debug_on_finished\\n',\n",
              " 'def on_call self args self view debug_on_call args\\n',\n",
              " 'def on_return self retval self view debug_on_return retval\\n',\n",
              " 'def on_line self filename line self view debug_on_line filename line\\n',\n",
              " 'def on_exception self name value self view debug_on_exception name value\\n',\n",
              " 'def on_postmortem self args kwargs self view debug_on_postmortem args kwargs\\n',\n",
              " 'def on_info self message logger info Debug runner says format message self view debug_on_info message\\n',\n",
              " 'def on_warning self message logger warning Debug runner says format message self view debug_on_warning message\\n',\n",
              " 'def on_error self message logger error Debug runner says format message self view debug_on_error message\\n',\n",
              " 'def test_pgzero_mode editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view assert pm name Pygame Zero assert pm description is not None assert pm icon pygamezero assert pm is_debugger is False assert pm editor editor assert pm view view assert pm builtins actions pm actions assert len actions 5 assert actions 0 name play assert actions 0 handler pm play_toggle assert actions 1 name images assert actions 1 handler pm show_images assert actions 2 name fonts assert actions 2 handler pm show_fonts assert actions 3 name sounds assert actions 3 handler pm show_sounds assert actions 4 name music assert actions 4 handler pm show_music\\n',\n",
              " 'def test_pgzero_api editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view result pm api assert result SHARED_APIS PYTHON3_APIS PI_APIS PYGAMEZERO_APIS\\n',\n",
              " 'def test_pgzero_play_toggle_on editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view pm runner None def runner pm pm pm runner True pm run_game mock MagicMock side_effect runner pm play_toggle None pm run_game assert_called_once_with slot pm view button_bar slots play assert slot setIcon call_count 1 slot setText assert_called_once_with Stop slot setToolTip assert_called_once_with Stop your Pygame Zero game\\n',\n",
              " 'def test_pgzero_play_toggle_on_cancelled editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view pm runner None pm run_game mock MagicMock pm play_toggle None pm run_game assert_called_once_with slot pm view button_bar slots play assert slot setIcon call_count 0\\n',\n",
              " 'def test_pgzero_play_toggle_off editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view pm runner True pm stop_game mock MagicMock pm play_toggle None pm stop_game assert_called_once_with slot pm view button_bar slots play assert slot setIcon call_count 1 slot setText assert_called_once_with Play slot setToolTip assert_called_once_with Play your Pygame Zero game\\n',\n",
              " 'def test_pgzero_run_game editor mock MagicMock editor envars name value view mock MagicMock view current_tab path foo view current_tab isModified return_value True mock_runner mock MagicMock view add_python3_runner return_value mock_runner pm PyGameZeroMode editor view pm workspace_dir mock MagicMock return_value bar with mock patch builtins open as oa mock patch mu modes pygamezero write_and_flush pm run_game oa assert_called_once_with foo w newline view add_python3_runner assert_called_once_with foo bar interactive False envars editor envars runner pgzrun mock_runner process waitForStarted assert_called_once_with\\n',\n",
              " 'def test_pgzero_run_game_no_editor editor mock MagicMock view mock MagicMock view current_tab None pm PyGameZeroMode editor view pm stop_game mock MagicMock pm run_game assert pm runner is None pm stop_game assert_called_once_with\\n',\n",
              " 'def test_pgzero_run_game_needs_saving editor mock MagicMock view mock MagicMock view current_tab path None pm PyGameZeroMode editor view pm stop_game mock MagicMock pm run_game editor save assert_called_once_with\\n',\n",
              " 'def test_pgzero_stop_game editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view mock_runner mock MagicMock pm runner mock_runner pm stop_game mock_runner process kill assert_called_once_with mock_runner process waitForFinished assert_called_once_with assert pm runner is None view remove_python_runner assert_called_once_with\\n',\n",
              " 'def test_pgzero_stop_game_no_runner editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view pm runner None pm stop_game view remove_python_runner assert_called_once_with\\n',\n",
              " 'def test_pgzero_show_images editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view pm show_images None image_dir os path join pm workspace_dir images view open_directory_from_os assert_called_once_with image_dir\\n',\n",
              " 'def test_pgzero_show_fonts editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view pm show_fonts None fonts_dir os path join pm workspace_dir fonts view open_directory_from_os assert_called_once_with fonts_dir\\n',\n",
              " 'def test_pgzero_show_sounds editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view pm show_sounds None sounds_dir os path join pm workspace_dir sounds view open_directory_from_os assert_called_once_with sounds_dir\\n',\n",
              " 'def test_pgzero_show_music editor mock MagicMock view mock MagicMock pm PyGameZeroMode editor view pm show_music None music_dir os path join pm workspace_dir music view open_directory_from_os assert_called_once_with music_dir\\n',\n",
              " 'def test_setup_logging with mock patch mu app TimedRotatingFileHandler as log_conf mock patch mu app os path exists return_value False mock patch mu app logging as logging mock patch mu app os makedirs return_value None as mkdir setup_logging mkdir assert_called_once_with LOG_DIR log_conf assert_called_once_with LOG_FILE when midnight backupCount 5 delay 0 encoding ENCODING logging getLogger assert_called_once_with assert sys excepthook excepthook\\n',\n",
              " 'def test_setup_modes_with_pgzero with mock patch mu app pkgutil iter_modules return_value pgzero mock_editor mock MagicMock mock_view mock MagicMock modes setup_modes mock_editor mock_view assert pygamezero in modes\\n',\n",
              " 'def test_setup_modes_without_pgzero with mock patch mu app pkgutil iter_modules return_value foo mock_editor mock MagicMock mock_view mock MagicMock modes setup_modes mock_editor mock_view assert pygamezero not in modes\\n',\n",
              " 'def test_run with mock patch mu app setup_logging as set_log mock patch mu app QApplication as qa mock patch mu app QSplashScreen as qsp mock patch mu app Editor as ed mock patch mu app load_pixmap mock patch mu app Window as win mock patch mu app QTimer as timer mock patch sys argv mu mock patch sys exit as ex run assert set_log call_count 1 assert qa call_count 1 assert len qa mock_calls 7 assert qsp call_count 1 assert len qsp mock_calls 2 assert timer call_count 1 assert len timer mock_calls 4 assert ed call_count 1 assert len ed mock_calls 3 assert win call_count 1 assert len win mock_calls 9 assert ex call_count 1\\n',\n",
              " 'def test_excepthook ex Exception BANG exc_args type ex ex ex __traceback__ with mock patch mu app logging error as error mock patch mu app sys exit as exit excepthook exc_args error assert_called_once_with Unrecoverable error exc_info exc_args exit assert_called_once_with 1\\n',\n",
              " 'def test_debug mock_sys mock MagicMock mock_sys argv None foo py foo bar baz mock_runner mock MagicMock with mock patch mu app sys mock_sys mock patch mu app run_debugger mock_runner debug expected_filename os path normcase os path abspath foo py mock_runner assert_called_once_with localhost DEBUGGER_PORT expected_filename foo bar baz\\n',\n",
              " 'def test_debug_no_args mock_sys mock MagicMock mock_sys argv None mock_print mock MagicMock with mock patch mu app sys mock_sys mock patch builtins print mock_print debug msg Debugger requires a Python script filename to run mock_print assert_called_once_with msg\\n',\n",
              " 'def parse self response for next_page in response css div toctree wrapper li a yield response follow next_page self parse_api\\n',\n",
              " 'def to_dict self name args description if name endswith __ return None return name name args args description description\\n',\n",
              " 'def parse_api self response for func in response css dl function func_spec func css dt 0 func_doc func css dd 0 fn1 BeautifulSoup func_spec css code descclassname extract 0 html parser text fn2 BeautifulSoup func_spec css code descname extract 0 html parser text func_name fn1 fn2 args for ems in func_spec css em args append ems extract replace em replace em soup BeautifulSoup func_doc extract html parser d self to_dict func_name args soup text if d yield d for classes in response css dl class class_spec classes css dt 0 class_doc classes css dd 0 cn1 BeautifulSoup class_spec css code descclassname extract 0 html parser text cn2 BeautifulSoup class_spec css code descname extract 0 html parser text class_name cn1 cn2 init_args for ems in class_spec css em props property in ems css attr class extract if not props init_args append ems extract replace em replace em soup BeautifulSoup class_doc extract html parser contents soup contents 0 contents description for child in contents if child name p description child text n n if child name table raw child text rows r strip for r in raw split n if r strip description n description n join rows break if child name dl break d self to_dict class_name init_args description if d yield d for methods in classes css dl method method_name BeautifulSoup methods css code descname extract 0 html parser text if method_name startswith __ break method_name class_name method_name method_args for ems in methods css em method_args append ems extract replace em replace em description BeautifulSoup methods css dd 0 extract html parser text d self to_dict method_name method_args description if d yield d for data in classes css dl attribute name BeautifulSoup data css code descname extract 0 html parser text name class_name name description BeautifulSoup data css dd 0 extract html parser text d self to_dict name None description if d yield d for data in classes css dl data name BeautifulSoup data css code descname extract 0 html parser text name class_name name description BeautifulSoup data css dd 0 extract html parser text d self to_dict name None description if d yield d\\n',\n",
              " 'def __init__ self cwd envars super __init__ self cwd cwd self envars dict envars\\n',\n",
              " 'def start_kernel self logger info sys path os chdir self cwd logger info Starting iPython kernel with user defined envars format self envars for k v in self envars items os environ k v self repl_kernel_manager QtKernelManager self repl_kernel_manager start_kernel self repl_kernel_client self repl_kernel_manager client self kernel_started emit self repl_kernel_manager self repl_kernel_client\\n',\n",
              " 'def stop_kernel self os environ clear for k v in self default_envars items os environ k v self repl_kernel_client stop_channels self repl_kernel_manager shutdown_kernel now True self kernel_finished emit\\n',\n",
              " 'def actions self buttons name run display_name _ Run description _ Run your Python script handler self run_toggle shortcut F5 name debug display_name _ Debug description _ Debug your Python script handler self debug shortcut F6 name repl display_name _ REPL description _ Use the REPL for live coding handler self toggle_repl shortcut Ctrl Shift I if CHARTS buttons append name plotter display_name _ Plotter description _ Plot data from your script or the REPL handler self toggle_plotter shortcut CTRL Shift P return buttons\\n',\n",
              " 'def api self return SHARED_APIS PYTHON3_APIS PI_APIS\\n',\n",
              " 'def run_toggle self event run_slot self view button_bar slots run if self runner self stop_script run_slot setIcon load_icon run run_slot setText _ Run run_slot setToolTip _ Run your Python script self set_buttons debug True modes True else self run_script if self runner run_slot setIcon load_icon stop run_slot setText _ Stop run_slot setToolTip _ Stop your Python script self set_buttons debug False modes False\\n',\n",
              " 'def run_script self tab self view current_tab if tab is None logger debug There is no active text editor self stop_script return if tab path is None self editor save if tab path if tab isModified with open tab path w newline as f logger info Saving script to format tab path logger debug tab text write_and_flush f tab text tab setModified False logger debug tab text envars self editor envars self runner self view add_python3_runner tab path self workspace_dir interactive True envars envars self runner process waitForStarted if self kernel_runner self set_buttons plotter False elif self plotter self set_buttons repl False\\n',\n",
              " 'def stop_script self logger debug Stopping script if self runner self runner process kill self runner process waitForFinished self runner None self view remove_python_runner self set_buttons plotter True repl True\\n',\n",
              " 'def debug self event logger info Starting debug mode self editor change_mode debugger self editor mode debugger self editor modes debugger start\\n',\n",
              " 'def toggle_repl self event if self kernel_runner is None logger info Toggle REPL on self editor show_status_message _ Starting iPython REPL self add_repl else logger info Toggle REPL off self editor show_status_message _ Stopping iPython REPL this may take a short amount of time self remove_repl\\n',\n",
              " 'def add_repl self self set_buttons repl False self kernel_thread QThread self kernel_runner KernelRunner cwd self workspace_dir envars self editor envars self kernel_runner moveToThread self kernel_thread self kernel_runner kernel_started connect self on_kernel_start self kernel_runner kernel_finished connect self kernel_thread quit self stop_kernel connect self kernel_runner stop_kernel self kernel_thread started connect self kernel_runner start_kernel self kernel_thread finished connect self on_kernel_stop self kernel_thread start\\n',\n",
              " 'def remove_repl self self view remove_repl self set_buttons repl False self stop_kernel emit\\n',\n",
              " 'def toggle_plotter self if self plotter is None logger info Toggle plotter on self add_plotter else logger info Toggle plotter off self remove_plotter\\n',\n",
              " 'def add_plotter self self view add_python3_plotter self logger info Started plotter self plotter True self set_buttons debug False if self repl self set_buttons run False elif self runner self set_buttons repl False\\n',\n",
              " 'def remove_plotter self self set_buttons run True repl True debug True super remove_plotter\\n',\n",
              " 'def on_data_flood self self set_buttons run True repl True debug True if self kernel_runner self remove_repl elif self runner self run_toggle None super on_data_flood\\n',\n",
              " 'def on_kernel_start self kernel_manager kernel_client self view add_jupyter_repl kernel_manager kernel_client self set_buttons repl True if self runner self set_buttons plotter False elif self plotter self set_buttons run False debug False self editor show_status_message _ REPL started\\n',\n",
              " 'def on_kernel_stop self self repl_kernel_manager None self set_buttons repl True plotter True run True self editor show_status_message _ REPL stopped self kernel_runner None\\n',\n",
              " 'def test_path with mock patch mu resources resource_filename return_value bar as r assert mu resources path foo bar r assert_called_once_with mu resources __name__ images foo\\n',\n",
              " 'def test_load_icon result mu resources load_icon icon assert isinstance result QIcon\\n',\n",
              " 'def test_load_pixmap result mu resources load_pixmap icon assert isinstance result QPixmap\\n',\n",
              " 'def test_stylesheet with mock patch mu resources resource_string return_value b foo as rs assert foo mu resources load_stylesheet foo rs assert_called_once_with mu resources __name__ css foo\\n',\n",
              " 'def test_load_font_data with mock patch mu resources resource_string return_value b foo as rs assert b foo mu resources load_font_data foo rs assert_called_once_with mu resources __name__ fonts foo\\n',\n",
              " 'def download_file url local_filename url split 1 r requests get url stream True with open local_filename wb as f for chunk in r iter_content chunk_size 1024 if chunk f write chunk return local_filename\\n',\n",
              " 'def unzip_file filename with zipfile ZipFile filename as z z extractall\\n',\n",
              " 'def run bitness print Downloading tkinter for bit platform format bitness if bitness 32 filename download_file ASSETS_32 else filename download_file ASSETS_64 print Unzipping format filename unzip_file filename print Running pynsist subprocess call pynsist win_installer cfg format bitness\\n',\n",
              " 'def setup_logging if not os path exists LOG_DIR os makedirs LOG_DIR log_fmt asctime s name s lineno d funcName s levelname s message s formatter logging Formatter log_fmt handler TimedRotatingFileHandler LOG_FILE when midnight backupCount 5 delay 0 encoding ENCODING handler setFormatter formatter handler setLevel logging DEBUG log logging getLogger log setLevel logging DEBUG log addHandler handler sys excepthook excepthook print _ Logging to format LOG_FILE\\n',\n",
              " 'def setup_modes editor view modes python PythonMode editor view adafruit AdafruitMode editor view microbit MicrobitMode editor view debugger DebugMode editor view if any m for m in pkgutil iter_modules if pgzero in m modes pygamezero PyGameZeroMode editor view return modes\\n',\n",
              " 'def excepthook exc_args logging error Unrecoverable error exc_info exc_args sys __excepthook__ exc_args sys exit 1\\n',\n",
              " 'def run setup_logging logging info n n n nStarting Mu format __version__ logging info platform uname logging info Python path format sys path app QApplication sys argv app setApplicationName mu app setDesktopFileName mu codewith editor app setApplicationVersion __version__ app setAttribute Qt AA_DontShowIconsInMenus editor_window Window app setWindowIcon load_icon editor_window icon editor Editor view editor_window editor setup setup_modes editor editor_window editor_window closeEvent editor quit editor_window setup editor debug_toggle_breakpoint editor theme editor restore_session sys argv 1 editor_window connect_tab_rename editor rename_tab Ctrl Shift S status_bar editor_window status_bar status_bar connect_logs editor show_admin Ctrl Shift D splash QSplashScreen load_pixmap splash screen splash show splash_be_gone QTimer splash_be_gone timeout connect lambda splash finish editor_window splash_be_gone setSingleShot True splash_be_gone start 2000 sys exit app exec_\\n',\n",
              " 'def debug if len sys argv 1 filename os path normcase os path abspath sys argv 1 args sys argv 2 run_debugger localhost DEBUGGER_PORT filename args else print _ Debugger requires a Python script filename to run\\n',\n",
              " 'def select_and_accept self self accept\\n',\n",
              " 'def get_mode self if self result QDialog Accepted return self mode_list currentItem icon else raise RuntimeError Mode change cancelled\\n',\n",
              " 'def settings self return envars self envar_widget text_area toPlainText minify self microbit_widget minify isChecked microbit_runtime self microbit_widget runtime_path text\\n',\n",
              " 'def actions self return name stop display_name _ Stop description _ Stop the running code handler self button_stop shortcut Shift F5 name run display_name _ Continue description _ Continue to run your Python script handler self button_continue shortcut F5 name step over display_name _ Step Over description _ Step over a line of code handler self button_step_over shortcut F10 name step in display_name _ Step In description _ Step into a function handler self button_step_in shortcut F11 name step out display_name _ Step Out description _ Step out of a function handler self button_step_out shortcut Shift F11\\n',\n",
              " 'def api self return\\n',\n",
              " 'def start self tab self view current_tab if tab is None logger debug There is no active text editor self stop return if tab path is None self editor save if tab path if tab isModified with open tab path w newline as f logger info Saving script to format tab path logger debug tab text write_and_flush f tab text tab setModified False logger debug tab text self set_buttons modes False envars self editor envars self runner self view add_python3_runner tab path self workspace_dir debugger True envars envars self runner process waitForStarted self runner process finished connect self finished self view add_debug_inspector self view set_read_only True self debugger Debugger localhost DEBUGGER_PORT proc self runner process self debugger view self self debugger start else logger debug Current script has not been saved Aborting debug self stop\\n',\n",
              " 'def stop self logger debug Stopping debugger if self runner self runner process kill self runner process waitForFinished self runner None self debugger None self view remove_python_runner self view remove_debug_inspector self set_buttons modes True self editor change_mode python self editor mode python self view set_read_only False\\n',\n",
              " 'def finished self buttons action name False for action in self actions if action name stop self set_buttons buttons self editor show_status_message _ Your script has finished running for tab in self view widgets tab markerDeleteAll tab breakpoint_lines set tab setSelection 0 0 0 0 if hasattr self debugger bp_index for line breakpoint in self debugger breakpoints tab path items if breakpoint enabled tab markerAdd line 1 tab BREAKPOINT_MARKER tab breakpoint_lines add line 1\\n',\n",
              " 'def button_stop self event self stop\\n',\n",
              " 'def button_continue self event self debugger do_run\\n',\n",
              " 'def button_step_over self event self debugger do_next\\n',\n",
              " 'def button_step_in self event self debugger do_step\\n',\n",
              " 'def button_step_out self event self debugger do_return\\n',\n",
              " 'def toggle_breakpoint self line tab bps self debugger breakpoints tab path if tab markersAtLine line self debugger disable_breakpoint bps line 1 tab markerDelete line tab BREAKPOINT_MARKER else breakpoint bps get line 1 None tab markerAdd line tab BREAKPOINT_MARKER if breakpoint self debugger enable_breakpoint breakpoint else self debugger create_breakpoint tab path line 1\\n',\n",
              " 'def debug_on_fail self message process_runner self view process_runner if process_runner msg _ Unable to connect to the Python debugger n n message process_runner append msg encode utf 8 self finished process_runner finished 1 1\\n',\n",
              " 'def debug_on_bootstrap self for tab in self view widgets for line in tab breakpoint_lines self debugger create_breakpoint tab path line 1 self debugger do_run\\n',\n",
              " 'def debug_on_breakpoint_enable self breakpoint tab self view current_tab tab markerAdd breakpoint line 1 tab BREAKPOINT_MARKER\\n',\n",
              " 'def debug_on_breakpoint_disable self breakpoint tab self view current_tab tab markerDelete breakpoint line 1 tab BREAKPOINT_MARKER\\n',\n",
              " 'def debug_on_line self filename line ignored bdb py if os path basename filename in ignored self debugger do_return return self view current_tab setSelection 0 0 0 0 tab self editor get_tab filename tab setSelection line 1 0 line 0\\n',\n",
              " 'def debug_on_stack self stack if stack locals_dict for frame in stack for k v in frame 1 locals items locals_dict k v self view update_debug_inspector locals_dict\\n',\n",
              " 'def debug_on_postmortem self args kwargs process_runner self view process_runner for item in args process_runner append item encode utf 8 for k v in kwargs items msg format k v process_runner append msg encode utf 8\\n',\n",
              " 'def debug_on_info self message self editor show_status_message _ Debugger info format message\\n',\n",
              " 'def debug_on_warning self message self editor show_status_message _ Debugger warning format message\\n',\n",
              " 'def debug_on_error self message self editor show_status_message _ Debugger error format message\\n',\n",
              " 'def debug_on_call self args self debugger do_step\\n',\n",
              " 'def debug_on_return self return_value self debugger do_step\\n',\n",
              " 'def debug_on_finished self self finished\\n',\n",
              " 'def debug_on_breakpoint_ignore self breakpoint count pass\\n',\n",
              " 'def debug_on_breakpoint_clear self breakpoint pass\\n',\n",
              " 'def debug_on_restart self pass\\n',\n",
              " 'def debug_on_exception self name value pass\\n',\n",
              " 'def actions self buttons name serial display_name _ Serial description _ Open a serial connection to your device handler self toggle_repl shortcut CTRL Shift S if CHARTS buttons append name plotter display_name _ Plotter description _ Plot incoming REPL data handler self toggle_plotter shortcut CTRL Shift P return buttons\\n',\n",
              " 'def workspace_dir self device_dir None if os name posix for mount_command in mount sbin mount try mount_output check_output mount_command splitlines mounted_volumes x split 2 for x in mount_output for volume in mounted_volumes if volume endswith b CIRCUITPY device_dir volume decode utf 8 except FileNotFoundError next elif os name nt def get_volume_name disk_name Each disk or external device connected to windows has an attribute called volume name This function returns the volume name for the given disk device Code from http stackoverflow com a 12056414 vol_name_buf ctypes create_unicode_buffer 1024 ctypes windll kernel32 GetVolumeInformationW ctypes c_wchar_p disk_name vol_name_buf ctypes sizeof vol_name_buf None None None None 0 return vol_name_buf value old_mode ctypes windll kernel32 SetErrorMode 1 try for disk in ABCDEFGHIJKLMNOPQRSTUVWXYZ path format disk if os path exists path and get_volume_name path CIRCUITPY return path finally ctypes windll kernel32 SetErrorMode old_mode else raise NotImplementedError OS not supported format os name if device_dir self connected True return device_dir else wd super workspace_dir if self connected m _ Could not find an attached Adafruit CircuitPython device info _ Python files for Adafruit CircuitPython devices are stored on the device Therefore to edit these files you need to have the device plugged in Until you plug in a device Mu will use the directory found here to store your code self view show_message m info format wd self connected False return wd\\n',\n",
              " 'def api self return SHARED_APIS ADAFRUIT_APIS\\n',\n",
              " 'classmethod def get_database cls if cls _DATABASE is None cls _DATABASE QFontDatabase for variant in FONT_VARIANTS filename FONT_FILENAME_PATTERN format variant variant font_data load_font_data filename cls _DATABASE addApplicationFontFromData font_data return cls _DATABASE\\n',\n",
              " 'def load self size DEFAULT_FONT_SIZE return Font get_database font FONT_NAME self stylename size\\n',\n",
              " 'property def stylename self if self bold if self italic return Semibold Italic return Semibold if self italic return Italic return Regular\\n',\n",
              " 'def get_default_workspace sp get_settings_path workspace_dir os path join HOME_DIRECTORY WORKSPACE_NAME settings try with open sp as f settings json load f except FileNotFoundError logger error Settings file does not exist format sp except ValueError logger error Settings file could not be parsed format sp else if workspace in settings if os path isdir settings workspace workspace_dir settings workspace else logger error Workspace value in the settings file is not a validdirectory format settings workspace return workspace_dir\\n',\n",
              " 'def actions self return NotImplemented\\n',\n",
              " 'def workspace_dir self return get_default_workspace\\n',\n",
              " 'def api self return NotImplemented\\n',\n",
              " 'def set_buttons self kwargs for k v in kwargs items if k in self view button_bar slots self view button_bar slots k setEnabled bool v\\n',\n",
              " 'def add_plotter self return NotImplemented\\n',\n",
              " 'def remove_plotter self data_dir os path join get_default_workspace data_capture if not os path exists data_dir logger debug Creating directory format data_dir os makedirs data_dir filename csv format time strftime Y m d H M S f os path join data_dir filename with open f w as csvfile csv_writer csv writer csvfile csv_writer writerows self view plotter_pane raw_data self view remove_plotter self plotter None logger info Removing plotter\\n',\n",
              " 'def on_data_flood self logger error Plotting data flood detected self view remove_plotter self plotter None msg _ Data Flood Detected info _ The plotter is flooded with data which will make Mu unresponsive and freeze As a safeguard the plotter has been stopped Flooding is when chunks of data of more than 1024 bytes are repeatedly sent to the plotter To fix this make sure your code prints small tuples of data between calls to sleep for a very short period of time self view show_message msg info\\n',\n",
              " 'def find_device self with_logging True available_ports QSerialPortInfo availablePorts for port in available_ports pid port productIdentifier vid port vendorIdentifier if vid pid in self valid_boards port_name port portName if with_logging logger info Found device on port format port_name return self port_path port_name if with_logging logger warning Could not find device logger debug Available ports logger debug PID VID PORT format p productIdentifier p vendorIdentifier p portName for p in available_ports return None\\n',\n",
              " 'def toggle_repl self event if self repl self remove_repl logger info Toggle REPL off else self add_repl logger info Toggle REPL on\\n',\n",
              " 'def remove_repl self self view remove_repl self repl False\\n',\n",
              " 'def add_repl self device_port self find_device if device_port try self view add_micropython_repl device_port self name self force_interrupt logger info Started REPL on port format device_port self repl True except IOError as ex logger error ex self repl False info _ Click on the device s reset button wait a few seconds and then try again self view show_message str ex info except Exception as ex logger error ex else message _ Could not find an attached device information _ Please make sure the device is plugged into this computer It must have a version of MicroPython or CircuitPython flashed onto it before the REPL will work Finally press the device s reset button and wait a few seconds before trying again self view show_message message information\\n',\n",
              " 'def toggle_plotter self event if self plotter self remove_plotter logger info Toggle plotter off else self add_plotter logger info Toggle plotter on\\n',\n",
              " 'def add_plotter self device_port self find_device if device_port try self view add_micropython_plotter device_port self name self logger info Started plotter self plotter True except IOError as ex logger error ex self plotter False info _ Click on the device s reset button wait a few seconds and then try again self view show_message str ex info except Exception as ex logger error ex else message _ Could not find an attached device information _ Please make sure the device is plugged into this computer It must have a version of MicroPython or CircuitPython flashed onto it before the Plotter will work Finally press the device s reset button and wait a few seconds before trying again self view show_message message information\\n',\n",
              " 'def on_data_flood self self remove_repl super on_data_flood\\n',\n",
              " 'def test_object_comparison_with_default_vs_none class MyObject BaseObject field1 StringField default field2 StringField default something field3 StringField default obj1 MyObject field1 val1 field2 None obj2 MyObject field1 val1 field3 assert obj1 is_equivalent obj2\\n',\n",
              " 'def test_data_loss_on_update request ckan_client_ll stage_1pre generate_dataset stage_1 ckan_client_ll post_dataset stage_1pre dataset_id stage_1 id check_dataset stage_1pre stage_1 retrieved ckan_client_ll get_dataset dataset_id assert retrieved stage_1 check_dataset stage_1pre stage_1 stage_2pre copy deepcopy retrieved stage_2pre title My new dataset title stage_2 ckan_client_ll put_dataset stage_2pre assert stage_2 title My new dataset title diffs diff_mappings map clean_dataset stage_1 stage_2 assert diffs right diffs left set assert diffs differing set title check_dataset stage_2pre stage_2\\n',\n",
              " 'def diff_mappings left right left_keys set left iterkeys right_keys set right iterkeys common_keys left_keys right_keys left_only_keys left_keys right_keys right_only_keys right_keys left_keys differing set k for k in common_keys if left k right k return common common_keys left left_only_keys right right_only_keys differing differing\\n',\n",
              " 'def diff_sequences left right return length_match len left len right differing set i for i l r in enumerate zip left right if l r\\n',\n",
              " 'def check_response_ok response status_code 200 warnings warn check_response_ok is deprecated use check_api_v3_response DeprecationWarning return check_api_v3_response response status_code status_code\\n',\n",
              " 'def check_response_error response status_code warnings warn check_response_ok is deprecated use check_api_v3_error DeprecationWarning return check_api_v3_error response status_code status_code\\n',\n",
              " 'def check_api_v3_response response status_code 200 assert response ok assert response status_code status_code content_type cgi parse_header response headers content type assert content_type 0 application json assert content_type 1 charset utf 8 data response json assert data success is True assert result in data assert error not in data assert help in data return data\\n',\n",
              " 'def check_api_v3_error response status_code assert not response ok assert response status_code status_code content_type cgi parse_header response headers content type assert content_type 0 application json assert content_type 1 charset utf 8 data response json assert data success is False assert error in data assert result not in data return data\\n',\n",
              " 'def generate_password length 20 return binascii hexlify os urandom length\\n',\n",
              " 'def generate_random_alphanum length 10 charset string ascii_letters string digits return join random choice charset for _ in xrange length\\n',\n",
              " 'def gen_random_id length 10 charset string ascii_lowercase string digits return join random choice charset for _ in xrange length\\n',\n",
              " 'def gen_dataset_name return dataset 0 format gen_random_id\\n',\n",
              " 'def gen_picture s size 200 return gen_robohash s size\\n',\n",
              " 'def gen_gravatar s size 200 h hashlib md5 s hexdigest return http www gravatar com avatar 0 jpg d identicon f y s 1 format h size\\n',\n",
              " 'def gen_robohash s size 200 h hashlib sha1 s hexdigest return http robohash org 0 png size 1 x 1 bgset bg2 set set1 format h size\\n',\n",
              " 'def test_site_read ckan_url api_url ckan_url api 3 action site_read response requests get api_url data check_response_ok response assert data result is True response requests get ckan_url api 3 action site_read something assert not response ok assert response status_code 404\\n',\n",
              " 'def test_invalid_method_name ckan_url api_url ckan_url api 3 action invalid_method_name response requests get api_url assert not response ok assert response status_code 400\\n',\n",
              " 'def test_package_list ckan_url api_url ckan_url api 3 action package_list response requests get api_url data check_response_ok response assert data result api_url ckan_url api 3 action package_list limit 10 response requests get api_url data check_response_ok response assert data result api_url ckan_url api 3 action package_list offset 10 limit 20 response requests get api_url data check_response_ok response assert data result api_url ckan_url api 3 action package_list invalid hello response requests get api_url data check_response_ok response\\n',\n",
              " 'def test_current_package_list_with_resources ckan_url api_url ckan_url api 3 action current_package_list_with_resources response requests get api_url data check_response_ok response assert data result api_url ckan_url api 3 action current_package_list_with_resources response requests get api_url limit 10 data check_response_ok response assert data result response requests get api_url limit 10 offset 20 data check_response_ok response assert data result response requests get api_url offset 20 data check_response_ok response assert data result\\n',\n",
              " 'pytest mark xfail def test_package_revision_list_without_id ckan_url api_url ckan_url api 3 action package_revision_list response requests get api_url check_response_error response 400\\n',\n",
              " 'def test_package_revision_list_id_non_existing ckan_url api_url ckan_url api 3 action package_revision_list id 00000000 0000 0000 0000 000000000000 response requests get api_url check_response_error response 404 api_url ckan_url api 3 action package_revision_list id missing_object response requests get api_url check_response_error response 404\\n',\n",
              " 'def flush_solr solr_url site_id s solr SolrConnection solr_url query site_id 0 format site_id s delete_query query s commit assert s query query numFound 0\\n',\n",
              " 'def __init__ self base_url api_key None self base_url base_url self api_key api_key\\n',\n",
              " 'property def anonymous self return CkanLowlevelClient self base_url\\n',\n",
              " 'def request self method path kwargs headers kwargs get headers or kwargs headers headers if self api_key is not None headers Authorization self api_key if data in kwargs if not isinstance kwargs data basestring kwargs data json dumps kwargs data headers content type application json if isinstance path list tuple path join path url urlparse urljoin self base_url path response requests request method url kwargs if not response ok raise HTTPError status_code response status_code message Error while performing request original self _figure_out_error_message response return response\\n',\n",
              " 'def _figure_out_error_message self response with SuppressExceptionIf True return self _figure_out_error_from_json response json\\n',\n",
              " 'def list_datasets self path api 2 rest dataset response self request GET path data response json self _validate_response_idlist data return data\\n',\n",
              " 'def iter_datasets self for ds_id in self list_datasets yield self get_dataset ds_id\\n',\n",
              " 'def get_dataset self dataset_id path api 2 rest dataset 0 format dataset_id response self request GET path data response json self _validate_response_dict data return data\\n',\n",
              " 'def post_dataset self dataset path api 2 rest dataset response self request POST path data dataset data response json self _validate_response_dict data return data\\n',\n",
              " 'def put_dataset self dataset path api 2 rest dataset 0 format dataset id response self request PUT path data dataset data response json self _validate_response_dict data return data\\n',\n",
              " 'def delete_dataset self dataset_id ignore_404 True ign404 SuppressExceptionIf lambda e ignore_404 and isinstance e HTTPError and e status_code 404 path api 2 rest dataset 0 format dataset_id with ign404 self request DELETE path data id dataset_id\\n',\n",
              " 'def put_organization self organization path api 3 action organization_update response self request POST path data organization data response json result self _validate_response_dict data return data\\n',\n",
              " 'def check_response response code 200 success True if type success bool raise TypeError success must be a boolean assert response ok is success assert response status_code code content_type cgi parse_header response headers content type 0 assert content_type application json data response json assert data success is success if success assert result in data else assert error in data return data\\n',\n",
              " 'def check_dataset expected actual assert id in actual assert isinstance expected dict assert isinstance actual dict if id in expected assert actual id expected id for key value in expected iteritems if key tags check_dataset_tags expected key actual key elif key extras assert sorted actual key sorted value elif key resources check_dataset_resources expected key actual key else assert actual key expected key\\n',\n",
              " 'def test_package_creation ckan_instance dummy_package API_KEY ckan_instance get_sysadmin_api_key with ckan_instance serve client CkanClient ckan_instance server_url api_key API_KEY response client post api 3 action package_create data dummy_package assert response ok assert response status_code 200 data response json assert data success is True dataset_id data result id response client get api 3 action package_show id 0 format dataset_id assert response ok assert response status_code 200 assert data success is True assert result in data assert data result id dataset_id for key in dummy_package if key in tags continue assert data result key dummy_package key if tags in dummy_package expected_tags sorted x name for x in dummy_package tags actual_tags sorted x name for x in data result tags assert actual_tags expected_tags\\n',\n",
              " 'def test_real_case_scenario ckan_instance state state 0 http www example com dataset 1 name dataset 1 title Dataset 1 info key_1 value_1 key_2 value_2 resources url http www example com dataset 1 resource1 json mimetype application json url http www example com dataset 1 resource2 json mimetype application json API_KEY ckan_instance get_sysadmin_api_key with ckan_instance serve client CkanClient ckan_instance server_url api_key API_KEY response client post api 3 action organization_create data name my organization title My organization assert response ok assert response status_code 200 data response json assert data success is True organization_id data result id response client get api 3 action organization_show id 0 format organization_id assert response ok assert response status_code 200 data response json assert data success is True assert result in data assert data result id organization_id def create_dataset data Create a dataset and check that everything is ok param data Data for the dataset to be created return The created dataset object response client post api 3 action package_create data data data check_response response dataset_id data result id response client get api 3 action package_show id 0 format dataset_id data check_response response assert data result id dataset_id return data result dataset_obj name dataset 1 title Dataset 1 url http example com dataset 1 state active license_id cc zero notes Some notes for the first dataset owner_org organization_id extras key extra0 value Extra value 0 key extra1 value Extra value 1 dataset create_dataset dataset_obj dataset_id dataset id dataset_obj title First dataset dataset_obj notes Updated notes here response client post api 3 action dataset_update id 0 format dataset_id data dataset_obj data check_response response updated_dataset data result check_dataset dataset_obj updated_dataset response client get api 3 action dataset_show id 0 format dataset_id data check_response response updated_dataset data result check_dataset dataset_obj updated_dataset assert updated_dataset title First dataset assert updated_dataset notes Updated notes here assert updated_dataset url http example com dataset 1 assert updated_dataset license_id cc zero\\n',\n",
              " 'def __init__ self default NOTSET is_key NOTSET required False if default is not NOTSET self default default if is_key is not NOTSET self is_key is_key self required required self _conf default self default is_key self is_key required self required\\n',\n",
              " 'def get self instance name if name in instance _updates return instance _updates name if name in instance _values return instance _values name return self get_default\\n',\n",
              " 'def validate self instance name value return value\\n',\n",
              " 'def set_initial self instance name value value self validate instance name value instance _values name value\\n',\n",
              " 'def set self instance name value value self validate instance name value instance _updates name value\\n',\n",
              " 'def delete self instance name instance _updates pop name None\\n',\n",
              " 'def serialize self instance name return self get instance name\\n',\n",
              " 'def is_modified self instance name return name in instance _updates\\n',\n",
              " 'def set_initial self values for name field in self iter_fields if name in values field set_initial self name values name\\n',\n",
              " 'def __getattribute__ self key attr object __getattribute__ self key if isinstance attr BaseField return attr get self key return attr\\n',\n",
              " 'def __setattr__ self key value v object __getattribute__ self key if isinstance v BaseField return v set self key value return object __setattr__ self key value\\n',\n",
              " 'def __delattr__ self key v object __getattribute__ self key if isinstance v BaseField return v delete self key return object __delattr__ self key\\n',\n",
              " 'def serialize self serialized for name field in self iter_fields serialized name field serialize self name return serialized\\n',\n",
              " 'def iter_fields self for name in dir self attr object __getattribute__ self name if isinstance attr BaseField yield name attr\\n',\n",
              " 'def is_equivalent self other ignore_key True if type self type other return False for name field in self iter_fields if ignore_key and field is_key continue if not field is_equivalent self name other ignore_key ignore_key return False return True\\n',\n",
              " 'def compare self other from differ import compare_objects return compare_objects self serialize other serialize\\n',\n",
              " 'def is_modified self for name field in self iter_fields if field is_modified self name return True return False\\n',\n",
              " 'def generate_organization random_id gen_random_id 10 return name org 0 format random_id title Organization 0 format random_id description Description of organization 0 format random_id image_url gen_picture random_id\\n',\n",
              " 'def generate_group random_id gen_random_id 10 return name grp 0 format random_id title Group 0 format random_id description Description of group 0 format random_id image_url gen_picture random_id\\n',\n",
              " 'def generate_dataset random_id gen_random_id 15 license_id random choice cc by cc zero cc by sa notspecified resources for i in xrange random randint 1 8 resources append generate_resource return name dataset 0 format random_id title Dataset 0 format random_id url http www example com dataset 0 format random_id type dataset maintainer_email maintainer 0 example com format random_id maintainer Maintainer 0 format random_id author_email author 0 example com format random_id author Author 0 format random_id license_id license_id private False notes Notes for dataset 0 format random_id tags generate_tags random randint 0 10 extras generate_extras random randint 0 30 resources resources groups owner_org None relationships\\n',\n",
              " 'def generate_resource random_id gen_random_id fmt random choice csv json url http example com resource 0 1 format random_id fmt return url url resource_type random choice api file name resource 0 format random_id format fmt upper description Resource 0 format random_id\\n',\n",
              " 'def generate_tags amount return tag 0 03d format random randint 0 50 for _ in xrange amount\\n',\n",
              " 'def generate_extras amount pairs key 0 03d format random randint 0 50 value 0 03d format random randint 0 50 for _ in xrange amount return dict pairs\\n',\n",
              " 'def generate_data dataset_count 50 orgs_count 10 groups_count 15 data dataset organization group for _ in xrange orgs_count org generate_organization data organization org name org for _ in xrange groups_count group generate_group data group group name group for _ in xrange dataset_count dataset generate_dataset dataset groups random choice data group keys for x in xrange random randint 1 5 dataset owner_org random choice data organization keys data dataset dataset id dataset return data\\n',\n",
              " 'def test_groups_bad_behavior request ckan_client_ll client ckan_client_ll OMITTED object GROUP_TEST_CASES OMITTED 1 2 3 OMITTED 1 2 3 1 1 2 3 4 1 2 3 4 1 2 3 1 2 1 2 1 2 3 1 2 4 1 2 4 1 2 1 2 3 4 1 2 3 4 grp for x in xrange 5 code gen_random_id group client post_group name group 0 format code title Group 0 format code grp append group id request addfinalizer lambda client delete_group group id def _new_dataset groups code gen_random_id dataset name dataset 0 format code title Dataset 0 format code groups grp 1 grp 2 created client post_dataset dataset assert created name dataset name assert created title dataset title assert sorted created groups sorted grp 1 grp 2 dataset_id created id request addfinalizer lambda client delete_dataset dataset_id return created def _to_ids x if x is None or x is OMITTED return x return grp y 1 for y in x for state update result in GROUP_TEST_CASES state update result map _to_ids state update result dataset _new_dataset groups state _data id dataset id groups update if update is not OMITTED else id dataset id upd client put_dataset _data upd2 client get_dataset dataset id assert sorted upd groups sorted result assert sorted upd2 groups sorted result\\n',\n",
              " 'def freeze obj if obj is None return None if isinstance obj basestring int float bool return obj if isinstance obj dict return FrozenDict obj if isinstance obj list return FrozenList obj if isinstance obj tuple return FrozenTuple obj raise TypeError I don t know how to freeze this\\n',\n",
              " 'def to_ckan self source_id return self _source_to_ckan source_id\\n',\n",
              " 'def to_source self ckan_id return self _ckan_to_source ckan_id\\n',\n",
              " 'def add self pair if pair source_id in self _source_to_ckan if self _source_to_ckan pair source_id pair ckan_id raise ValueError Mismatching information if pair ckan_id in self _ckan_to_source if self _ckan_to_source pair ckan_id pair source_id raise ValueError Mismatching information self _source_to_ckan pair source_id pair ckan_id self _ckan_to_source pair ckan_id pair source_id\\n',\n",
              " 'def remove self pair if pair source_id in self _source_to_ckan if self _source_to_ckan pair source_id pair ckan_id raise ValueError Mismatching information if pair ckan_id in self _ckan_to_source if self _ckan_to_source pair ckan_id pair source_id raise ValueError Mismatching information del self _source_to_ckan pair source_id del self _ckan_to_source pair ckan_id\\n',\n",
              " 'def __init__ self base_dir day self base_dir base_dir self day day\\n',\n",
              " 'def __iter__ self folder os path join self base_dir self day for name in os listdir folder if name startswith continue path os path join folder name if not os path isdir path continue yield name\\n',\n",
              " 'def __iter__ self folder os path join self source base_dir self source day self name for name in os listdir folder if name startswith continue path os path join folder name if not os path isfile path continue yield name\\n',\n",
              " 'def test_string_fiel_multiple class MyObject BaseObject field1 StringField field2 StringField field3 StringField default F3 default field4 StringField default F4 default obj1 MyObject obj2 MyObject obj3 MyObject field1 F1 initial field3 F3 initial obj4 MyObject field2 F2 initial field4 F4 initial assert obj1 serialize field1 None field2 None field3 F3 default field4 F4 default assert obj2 serialize field1 None field2 None field3 F3 default field4 F4 default assert obj3 serialize field1 F1 initial field2 None field3 F3 initial field4 F4 default assert obj4 serialize field1 None field2 F2 initial field3 F3 default field4 F4 initial obj1 field1 F1 updated obj1 field2 F2 updated obj1 field3 F3 updated obj1 field4 F4 updated assert obj1 serialize field1 F1 updated field2 F2 updated field3 F3 updated field4 F4 updated assert obj2 serialize field1 None field2 None field3 F3 default field4 F4 default assert obj3 serialize field1 F1 initial field2 None field3 F3 initial field4 F4 default assert obj4 serialize field1 None field2 F2 initial field3 F3 default field4 F4 initial del obj1 field1 del obj1 field2 del obj1 field3 del obj1 field4 assert obj1 serialize field1 None field2 None field3 F3 default field4 F4 default\\n',\n",
              " 'def check_dataset dataset expected from ckan_api_client objects import CkanDataset return _check_obj CkanDataset dataset expected\\n',\n",
              " 'def check_group group expected from ckan_api_client objects import CkanGroup return _check_obj CkanGroup group expected\\n',\n",
              " 'def check_organization organization expected from ckan_api_client objects import CkanOrganization return _check_obj CkanOrganization organization expected\\n',\n",
              " 'def get self instance name if name not in instance _updates if name not in instance _values instance _values name self get_default value instance _values name instance _updates name copy deepcopy self validate instance name value return instance _updates name\\n',\n",
              " 'def write_to_temporary self string_buffer filename None tmp_file_tuple tempfile mkstemp prefix self file_prefix dir self temporary_dir tmp_fd tmp_file_tuple 0 filename tmp_file_tuple 1 os write tmp_fd string_buffer os close tmp_fd return filename\\n',\n",
              " 'def create_directory self directory_name os mkdir os path join self temporary_dir directory_name\\n',\n",
              " 'def write_to_file self file_path string_buffer full_file_path os path join self temporary_dir file_path if os path isfile full_file_path os unlink full_file_path f open full_file_path w f write string_buffer f close\\n',\n",
              " 'def _decompress self self file_ tempfile NamedTemporaryFile w b self compressed_file_ seek 0 self file_ write zlib decompress self compressed_file_ read self file_ flush self file_ seek 0 self compressed_file_ seek 0\\n',\n",
              " 'staticmethod def get_tzinfos if not TzInfos tzd TzInfos _generate_tzd return TzInfos tzd\\n',\n",
              " 'staticmethod def open manifest_path manifest_file open manifest_path manifest Manifest manifest_file manifest_file close return manifest\\n',\n",
              " 'def _read_line self line key_char line 0 data line 1 1 if key_char C self root_catalog data elif key_char R self root_hash data elif key_char B self root_catalog_size int data elif key_char X self certificate data elif key_char H self history_database data elif key_char T self last_modified datetime fromtimestamp int data tz tzutc elif key_char D self ttl int data elif key_char S self revision int data elif key_char N self repository_name data elif key_char L self micro_catalog data elif key_char G self garbage_collectable data yes else raise UnknownManifestField key_char\\n',\n",
              " 'def _check_validity self if not hasattr self root_catalog raise ManifestValidityError Manifest lacks a root catalog entry if not hasattr self root_hash raise ManifestValidityError Manifest lacks a root hash entry if not hasattr self ttl raise ManifestValidityError Manifest lacks a TTL entry if not hasattr self revision raise ManifestValidityError Manifest lacks a revision entry if not hasattr self repository_name raise ManifestValidityError Manifest lacks a repository name\\n',\n",
              " 'def parse_pac_file pacfile try f open pacfile pac_script f read except IOError print Could not read the pacfile s n s pacfile sys exc_info 1 return f close _pacparser parse_pac_string pac_script\\n',\n",
              " 'def find_proxy url host None if host is None m url_regex match url if not m print URL s is not a valid URL url return None if len m groups is 1 host m groups 0 else print URL s is not a valid URL url return None return _pacparser find_proxy url host\\n',\n",
              " 'def just_find_proxy pacfile url host None if os path isfile pacfile pass else print PAC file s doesn t exist pacfile return None if host is None m url_regex match url if not m print URL s is not a valid URL url return None if len m groups is 1 host m groups 0 else print URL s is not a valid URL url return None init parse_pac pacfile proxy find_proxy url host cleanup return proxy\\n',\n",
              " 'def serve_forever self poll_interval 0 5 self __is_shut_down clear try while not self __shutdown_request r w e select select self poll_interval if self in r self _handle_request_noblock finally self __shutdown_request False self __is_shut_down set\\n',\n",
              " 'def shutdown self self __shutdown_request True self __is_shut_down wait\\n',\n",
              " 'def _handle_request_noblock self try request client_address self get_request except socket error return if self verify_request request client_address try self process_request request client_address except self handle_error request client_address self close_request request\\n',\n",
              " 'def __init__ self file_object self has_signature False for line in file_object readlines if len line 0 continue if line 0 2 self has_signature True break self _read_line line if self has_signature self _read_signature file_object self _check_validity\\n',\n",
              " 'def _read_signature self file_object file_object seek 0 message_digest self _hash_over_content file_object self signature_checksum file_object readline rstrip if len self signature_checksum 40 raise IncompleteRootFileSignature Signature checksum malformed if message_digest self signature_checksum raise InvalidRootFileSignature Signature checksum doesn t match self signature file_object read if len self signature 0 raise IncompleteRootFileSignature Binary signature not found\\n',\n",
              " 'def get_openssl_certificate self return self openssl_certificate\\n',\n",
              " 'def _get_fingerprint self algorithm sha1 if LooseVersion M2Crypto version StrictVersion 0 17 der self openssl_certificate as_der md EVP MessageDigest algorithm md update der digest md final return hex util octx_to_num digest 2 1 upper else return self openssl_certificate get_fingerprint\\n',\n",
              " 'def _check_signature self pubkey signature if LooseVersion M2Crypto version StrictVersion 0 18 return m2 verify_final pubkey ctx signature pubkey pkey else return pubkey verify_final signature\\n',\n",
              " 'def get_fingerprint self algorithm sha1 fp self _get_fingerprint return join x y for x y in zip fp 0 2 fp 1 2\\n',\n",
              " 'def verify self signature message pubkey self openssl_certificate get_pubkey pubkey reset_context md sha1 pubkey verify_init pubkey verify_update message return self _check_signature pubkey signature\\n',\n",
              " 'def cd_to_firebaseauth_root root_dir os path dirname os path dirname os path abspath __file__ os chdir root_dir\\n',\n",
              " 'def get_files_with_suffix root suffix for root _ files in os walk root for file_name in files if file_name endswith suffix yield os path join root file_name\\n',\n",
              " 'pytest fixture def words words cake apple banana cherry chocolate return words\\n',\n",
              " 'pytest fixture def words_underscore words label type character foundation_garment return words\\n',\n",
              " 'pytest fixture def words_no_results words label type character subject discipline topic national familycolor fam glotto isoexception return words\\n',\n",
              " 'def test_get_df_for_words_underscore words_underscore palmetto Palmetto doc_ids palmetto get_df_for_words words_underscore for i in range 0 len words_underscore assert doc_ids i 0 words_underscore i\\n',\n",
              " 'def test_get_df_for_words_with_no_results words_no_results palmetto Palmetto doc_ids palmetto get_df_for_words words_no_results for i in range 0 len words_no_results assert doc_ids i 0 words_no_results i\\n',\n",
              " 'def get_df_for_words self words df_stream self _get_df words return self _parse_df_stream_to_doc_ids words df_stream\\n',\n",
              " 'def get_coherence self words coherence_type cv return self _get_coherence words coherence_type\\n',\n",
              " 'def get_coherence_fast self words doc_id_sets self get_df_for_words words return calculate_coherence_fast words doc_id_sets corpus_size 4264684\\n',\n",
              " 'def define_ha ha LinearHybridAutomaton ha variables x y loc1 ha new_mode loc1 a_matrix np array 0 1 1 1 0 1 c_vector np array 0 0 dtype float loc1 set_dynamics a_matrix c_vector return ha\\n',\n",
              " 'def define_init_states ha rv r HyperRectangle 6 5 0 1 rv append ha modes loc1 r return rv\\n',\n",
              " 'def define_settings plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_MATLAB plot_settings xdim 0 plot_settings ydim 1 s HylaaSettings step 0 2 max_time 20 0 plot_settings plot_settings return s\\n',\n",
              " 'def run_hylaa settings ha define_ha init define_init_states ha engine HylaaEngine ha settings engine run init return engine result\\n',\n",
              " 'def define_settings plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_IMAGE plot_settings xdim 0 plot_settings ydim 1 s HylaaSettings step 0 2 max_time 20 0 plot_settings plot_settings return s\\n',\n",
              " 'def almost_equals self other tol assert isinstance other LinearConstraint rv True if abs self value other value tol rv False else for i in xrange self vector shape 0 a self vector i b other vector i if abs a b tol rv False break return rv\\n',\n",
              " 'def clone self return LinearConstraint self vector copy self value\\n',\n",
              " 'def center self rv for d in self dims rv append d 0 d 1 2 0 return rv\\n',\n",
              " 'def diamond self center self center num_dims len self dims rv for index in xrange num_dims pt list center pt index self dims index 0 rv append pt pt list center pt index self dims index 1 rv append pt return rv\\n',\n",
              " 'def unique_corners self tol 1e 09 rv num_dims len self dims max_iterator 1 is_flat for d in xrange num_dims if abs self dims d 0 self dims d 1 tol is_flat append False max_iterator 2 else is_flat append True for it in xrange max_iterator point for d in xrange num_dims if is_flat d point append self dims d 0 else min_max_index it 2 point append self dims d min_max_index it 2 rv append point return rv\\n',\n",
              " 'def get_sim_bundle self settings star max_steps_remaining assert isinstance settings HylaaSettings if self _sim_bundle is None self sim_settings settings simulation if settings print_output is False self sim_settings stdout False self _sim_bundle SimulationBundle self a_matrix self c_vector self sim_settings if self sim_settings use_presimulation self presimulate star max_steps_remaining return self _sim_bundle\\n',\n",
              " 'def get_existing_sim_bundle self assert self _sim_bundle is not None return self _sim_bundle\\n',\n",
              " 'def presimulate self star max_steps_remaining sim_bundle self _sim_bundle if len self inv_list 0 num_presimulation_steps max_steps_remaining else pt_in_star np array star get_feasible_point dtype float origin_sim sim_bundle sim_until_inv_violated pt_in_star self inv_list max_steps_remaining num_presimulation_steps int len origin_sim 1 2 if num_presimulation_steps max_steps_remaining num_presimulation_steps max_steps_remaining sim_bundle presimulate num_presimulation_steps\\n',\n",
              " 'def get_gb_t self assert self sim_settings is not None get_sim_bundle must be called before get_gb_t if self _gbt_matrix is None self _gbt_matrix self _sim_bundle compute_gbt self b_matrix return self _gbt_matrix\\n',\n",
              " 'def set_dynamics self a_matrix c_vector None assert len self parent variables 0 automaton variables should be set before dynamics if c_vector is None c_vector np zeros len self parent variables assert isinstance a_matrix np ndarray assert len a_matrix shape 2 assert isinstance c_vector np ndarray assert len c_vector shape 1 assert a_matrix shape 0 c_vector shape 0 assert a_matrix shape 1 len self parent variables self a_matrix a_matrix self c_vector c_vector\\n',\n",
              " 'def set_inputs self u_constraints_a u_constraints_b b_matrix None assert self a_matrix is not None dynamics should be set before inputs if b_matrix is None b_matrix np identity len self parent variables assert isinstance b_matrix np ndarray assert isinstance u_constraints_a np ndarray assert isinstance u_constraints_b np ndarray assert len b_matrix shape 2 input B matrix should be 2 d assert len u_constraints_a shape 2 assert len u_constraints_b shape 1 assert b_matrix shape 0 len self parent variables the number of rows in the input B matrix must be equal to the number of variables in the automaton format b_matrix shape 0 len self parent variables self num_inputs b_matrix shape 1 assert u_constraints_a shape 0 u_constraints_b shape 0 input constraints a matrix and b vector must have the same number of rows assert u_constraints_a shape 1 self num_inputs the number of columns in the input constraint a matrix must equal the number of variables in the input B matrix format u_constraints_a shape 1 self num_inputs self b_matrix b_matrix self u_constraints_a u_constraints_a self u_constraints_a_t u_constraints_a transpose copy self u_constraints_b u_constraints_b\\n',\n",
              " 'def new_mode self name m LinearAutomatonMode self name self modes m name m return m\\n',\n",
              " 'def new_transition self from_mode to_mode t LinearAutomatonTransition self from_mode to_mode self transitions append t return t\\n',\n",
              " 'def do_guard_strengthening self for t in self transitions if t from_mode t to_mode continue inv_list t to_mode inv_list condition_list t condition_list for inv_constraint in inv_list already_in_cond_list False for guard_constraint in condition_list if guard_constraint almost_equals inv_constraint 1e 13 already_in_cond_list True break if not already_in_cond_list condition_list append inv_constraint\\n',\n",
              " 'def define_ha ha LinearHybridAutomaton ha variables x y loc1 ha new_mode loc1 a_matrix np array 0 1 1 0 dtype float c_vector np array 0 0 dtype float loc1 set_dynamics a_matrix c_vector u_constraints_a np array 1 0 1 0 0 1 0 1 dtype float u_constraints_b np array 0 5 0 5 0 5 0 5 dtype float b_matrix np array 1 0 0 1 dtype float loc1 set_inputs u_constraints_a u_constraints_b b_matrix return ha\\n',\n",
              " 'def define_init_states ha rv constraints constraints append LinearConstraint 1 0 5 constraints append LinearConstraint 1 0 5 constraints append LinearConstraint 0 1 0 constraints append LinearConstraint 0 1 0 rv append ha modes loc1 constraints return rv\\n',\n",
              " 'def define_settings plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_IMAGE plot_settings xdim 0 plot_settings ydim 1 settings HylaaSettings step 0 1 max_time 3 2 plot_settings plot_settings return settings\\n',\n",
              " 'def test_sync_motor self model sync_motor ha model define_ha init_list model define_init_states ha settings model define_settings engine HylaaEngine ha settings engine run init_list self assertTrue engine reached_error\\n',\n",
              " 'def test_violation_extends_axis self ha ball_string define_ha init_list ball_string define_init_states ha settings ball_string define_settings plot_settings settings plot plot_settings plot_mode PlotSettings PLOT_INTERACTIVE plot_settings skip_frames 21 plot_settings skip_show_gui True plot_settings num_angles 256 settings print_output False engine HylaaEngine ha settings engine run init_list self assertTrue engine plotman drawn_limits xmax 0\\n',\n",
              " 'def test_drivetrain self ha drivetrain define_ha init_list drivetrain define_init_states ha self assertEquals len init_list 1 settings drivetrain define_settings plot_settings settings plot settings print_output False plot_settings skip_frames 10 plot_settings skip_show_gui True engine HylaaEngine ha settings engine run init_list\\n',\n",
              " 'def has_openblas return openblas_lib is not None\\n',\n",
              " 'def set_num_threads n if has_openblas openblas_lib openblas_set_num_threads int n\\n',\n",
              " 'def get_num_threads rv 1 if has_openblas try rv openblas_lib openblas_get_num_threads except AttributeError pass return rv\\n',\n",
              " 'def define_ha ha LinearHybridAutomaton ha variables x v extension ha new_mode extension extension a_matrix np array 0 0 1 0 100 0 4 0 dtype float extension c_vector np array 0 0 9 81 dtype float extension inv_list append LinearConstraint 1 0 0 0 0 freefall ha new_mode freefall freefall a_matrix np array 0 0 1 0 0 0 0 0 dtype float freefall c_vector np array 0 0 9 81 dtype float freefall inv_list append LinearConstraint 1 0 0 0 0 0 freefall inv_list append LinearConstraint 1 0 0 0 1 0 trans ha new_transition extension freefall trans condition_list append LinearConstraint 0 0 1 0 0 0 return ha\\n',\n",
              " 'def define_init_states ha rv rv append ha modes extension HyperRectangle 1 05 0 95 0 1 0 1 return rv\\n',\n",
              " 'def define_settings plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_IMAGE plot_settings xdim 0 plot_settings ydim 1 settings HylaaSettings step 0 01 max_time 2 0 plot_settings plot_settings return settings\\n',\n",
              " 'def define_ha ha LinearHybridAutomaton ha variables x v extension ha new_mode extension extension a_matrix np array 0 0 1 0 100 0 4 0 dtype float extension c_vector np array 0 0 9 81 dtype float extension inv_list append LinearConstraint 1 0 0 0 0 freefall ha new_mode freefall freefall a_matrix np array 0 0 1 0 0 0 0 0 dtype float freefall c_vector np array 0 0 9 81 dtype float freefall inv_list append LinearConstraint 1 0 0 0 0 0 freefall inv_list append LinearConstraint 1 0 0 0 1 0 trans ha new_transition extension freefall trans condition_list append LinearConstraint 0 0 1 0 0 0 trans ha new_transition freefall freefall trans condition_list append LinearConstraint 1 0 0 0 1 0 return ha\\n',\n",
              " 'def define_settings plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_INTERACTIVE plot_settings xdim 0 plot_settings ydim 1 settings HylaaSettings step 0 01 max_time 2 0 plot_settings plot_settings return settings\\n',\n",
              " 'def define_ha ha LinearHybridAutomaton Trimmed Harmonic Oscillator w successor ha variables x y loc1 ha new_mode green loc1 a_matrix nparray 0 1 1 0 loc1 c_vector nparray 0 0 inv1 LinearConstraint 0 0 1 0 0 0 loc1 inv_list inv1 loc2 ha new_mode cyan loc2 a_matrix nparray 0 0 0 0 loc2 c_vector nparray 0 2 inv2 LinearConstraint 0 0 1 0 2 5 loc2 inv_list inv2 loc3 ha new_mode orange loc3 a_matrix nparray 0 0 0 0 loc3 c_vector nparray 0 2 inv3 LinearConstraint 0 0 1 0 4 0 loc3 inv_list inv3 guard LinearConstraint 0 0 1 0 0 0 trans ha new_transition loc1 loc2 trans condition_list guard guard1 LinearConstraint 0 0 1 0 0 guard2 LinearConstraint 1 0 0 0 5 guard3 LinearConstraint 1 0 0 0 5 trans ha new_transition loc2 loc3 trans condition_list guard1 guard2 guard3 return ha\\n',\n",
              " 'def define_init_states ha rv r HyperRectangle 5 5 4 5 0 1 rv append ha modes green r return rv\\n',\n",
              " 'def define_settings plot_settings PlotSettings plot_settings make_video deaggregation mp4 frames 150 fps 5 plot_settings xdim 0 plot_settings ydim 1 plot_settings extra_lines 0 5 4 0 5 0 0 5 0 0 5 4 settings HylaaSettings step 0 25 max_time 6 0 plot_settings plot_settings settings process_urgent_guards True settings skip_step_times True settings simulation threads 1 return settings\\n',\n",
              " 'def define_ha ha LinearHybridAutomaton ha variables x1 x2 x3 x4 x5 x6 x7 x8 x9 negAngle ha new_mode negAngle negAngle a_matrix np array 0 0 0 0 0 0 0 0833333333333333 0 1 13828 8888888889 26 6666666666667 60 60 0 0 5 60 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 714 285714285714 0 04 0 714 285714285714 0 2777 77777777778 3 33333333333333 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 100 0 0 0 1000 0 0 1000 0 dtype float negAngle c_vector np array 0 716 666666666667 0 5 0 0 83 3333333333333 0 3 dtype float negAngle inv_list append LinearConstraint 1 0 0 0 0 0 0 0 0 0 03 deadzone ha new_mode deadzone deadzone a_matrix np array 0 0 0 0 0 0 0 0833333333333333 0 1 60 26 6666666666667 60 60 0 0 5 60 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 714 285714285714 0 04 0 714 285714285714 0 0 3 33333333333333 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1000 0 0 1000 0 dtype float deadzone c_vector np array 0 300 0 5 0 0 0 0 0 dtype float deadzone inv_list append LinearConstraint 1 0 0 0 0 0 0 0 0 0 03 deadzone inv_list append LinearConstraint 1 0 0 0 0 0 0 0 0 0 03 posAngle ha new_mode posAngle posAngle a_matrix np array 0 0 0 0 0 0 0 0833333333333333 0 1 13828 8888888889 26 6666666666667 60 60 0 0 5 60 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 714 285714285714 0 04 0 714 285714285714 0 2777 77777777778 3 33333333333333 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 100 0 0 0 1000 0 0 1000 0 dtype float posAngle c_vector np array 0 116 666666666667 0 5 0 0 83 3333333333333 0 3 dtype float posAngle inv_list append LinearConstraint 1 0 0 0 0 0 0 0 0 0 03 trans ha new_transition negAngle deadzone trans condition_list append LinearConstraint 1 0 0 0 0 0 0 0 0 0 03 trans ha new_transition deadzone posAngle trans condition_list append LinearConstraint 1 0 0 0 0 0 0 0 0 0 03 return ha\\n',\n",
              " 'def define_init_states ha rv constraints constraints append LinearConstraint 1 0 0 0 00056 0 0 0 0 0 0 060000000000000005 constraints append LinearConstraint 1 0 0 0 00056 0 0 0 0 0 0 060000000000000005 constraints append LinearConstraint 0 1 0 0 467 0 0 0 0 0 25 009999999999998 constraints append LinearConstraint 0 1 0 0 467 0 0 0 0 0 25 009999999999998 constraints append LinearConstraint 0 0 1 0 0 0 0 0 0 0 constraints append LinearConstraint 0 0 1 0 0 0 0 0 0 0 constraints append LinearConstraint 0 0 0 0 1 0 0 0 0 0 constraints append LinearConstraint 0 0 0 0 1 0 0 0 0 0 constraints append LinearConstraint 0 0 0 1 0 1 0 0 0 0 constraints append LinearConstraint 0 0 0 1 0 1 0 0 0 0 constraints append LinearConstraint 0 0 0 12 0 0 1 0 0 0 constraints append LinearConstraint 0 0 0 12 0 0 1 0 0 0 constraints append LinearConstraint 0 0 0 6e 05 0 0 0 1 0 0 00312 constraints append LinearConstraint 0 0 0 6e 05 0 0 0 1 0 0 00312 constraints append LinearConstraint 0 0 0 1 0 0 0 0 1 0 constraints append LinearConstraint 0 0 0 1 0 0 0 0 1 0 constraints append LinearConstraint 0 0 0 1 0 0 0 0 0 20 constraints append LinearConstraint 0 0 0 1 0 0 0 0 0 40 rv append ha modes negAngle constraints return rv\\n',\n",
              " 'def define_settings plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_INTERACTIVE plot_settings xdim 0 plot_settings ydim 1 return HylaaSettings step 0 0005 max_time 2 0 plot_settings plot_settings\\n',\n",
              " 'def test_rectangular self ha LinearHybridAutomaton Harmonic Oscillator ha variables x y loc1 ha new_mode loc loc1 a_matrix np array 0 0 0 0 loc1 c_vector np array 1 2 init_list ha modes loc HyperRectangle 0 99 1 01 1 99 2 01 plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_NONE settings HylaaSettings step 0 1 max_time 1 1 plot_settings plot_settings settings print_output False engine HylaaEngine ha settings engine load_waiting_list init_list engine do_step for i in xrange 10 engine do_step t 0 1 i 1 star engine cur_state point 1 t 2 2 t self assertTrue star contains_point point\\n',\n",
              " 'def test_exp self ha LinearHybridAutomaton Harmonic Oscillator ha variables x a_matrix np array 1 dtype float c_vector np array 0 dtype float loc1 ha new_mode loc loc1 set_dynamics a_matrix c_vector init_list ha modes loc HyperRectangle 0 99 1 01 plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_NONE settings HylaaSettings step 0 1 max_time 1 1 plot_settings plot_settings settings print_output False engine HylaaEngine ha settings engine load_waiting_list init_list engine do_step for i in xrange 10 engine do_step t 0 1 i 1 star engine cur_state self assertTrue star contains_point math exp t\\n',\n",
              " 'def test_exp_plus_one self ha LinearHybridAutomaton Harmonic Oscillator ha variables x loc1 ha new_mode loc loc1 a_matrix np array 1 loc1 c_vector np array 1 init_list ha modes loc HyperRectangle 0 99 1 01 plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_NONE settings HylaaSettings step 0 1 max_time 1 1 plot_settings plot_settings settings print_output False engine HylaaEngine ha settings engine load_waiting_list init_list engine do_step for i in xrange 10 engine do_step t 0 1 i 1 star engine cur_state self assertTrue star contains_point 2 math exp t 1\\n',\n",
              " 'def test_cpp self start_op LpInstance total_optimizations start_it LpInstance total_iterations LpInstance test self assertGreater LpInstance total_iterations start_it 5 self assertGreater LpInstance total_optimizations start_op 1\\n',\n",
              " 'def test_simple self a_ub 1 2 3 1 b_ub 1 2 c 0 6 0 5 self compare_opt a_ub b_ub c\\n',\n",
              " 'def test_simple2 self a_ub 1 0 1 0 b_ub 1 0 1 0 num_vars len a_ub 0 c 1 0 for _ in xrange num_vars self compare_opt a_ub b_ub c\\n',\n",
              " 'def test_underconstrained self a_ub 1 0 0 0 1 0 0 0 b_ub 1 0 1 0 c 1 0 0 0 num_vars 2 lp LpInstance num_vars num_vars orthonormal_basis 1 0 if d index else 0 0 for d in xrange num_vars for index in xrange num_vars lp update_basis_matrix np array orthonormal_basis dtype float for row in xrange len b_ub vec np matrix a_ub row dtype float val b_ub row lp add_basis_constraint vec val res_glpk np zeros num_vars lp minimize np array c dtype float res_glpk self assertAlmostEqual res_glpk 0 1\\n',\n",
              " 'def test_tricky self a_ub 1 0 0 0 0 0 2 1954134149515525e 08 1 0000000097476742 0 0 1 0 0 0 0 0 2 1954134149515525e 08 1 0000000097476742 0 0 0 0 1 0 0 0 1 000000006962809 2 5063524589086228e 08 0 0 0 0 1 0 0 0 1 000000006962809 2 5063524589086228e 08 0 0 0 0 0 0 1 0 0 0 0 0 1 0000000000000009 0 0 0 0 1 0 0 0 0 0 1 0000000000000009 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 b_ub 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 5 1 0 1 0 0 0 0 0 num_vars len a_ub 0 c 1 0 if i 2 0 else 0 0 for i in xrange num_vars self compare_opt a_ub b_ub c\\n',\n",
              " 'def compare_opt self a_ub b_ub c a_ub float x for x in row for row in a_ub b_ub float x for x in b_ub c float x for x in c num_vars len a_ub 0 options show_progress False sol cvxopt solvers lp cvxopt matrix c cvxopt matrix a_ub T cvxopt matrix b_ub options options if sol status optimal raise RuntimeError cvxopt LP failed format sol status res_cvxopt float n for n in sol x lp LpInstance num_vars num_vars orthonormal_basis 1 0 if d index else 0 0 for d in xrange num_vars for index in xrange num_vars lp update_basis_matrix np array orthonormal_basis dtype float for row in xrange len b_ub vec np matrix a_ub row dtype float val b_ub row lp add_basis_constraint vec val res_glpk np zeros num_vars lp minimize np array c dtype float res_glpk self assertEqual num_vars len res_cvxopt self assertAlmostEqual np dot res_glpk c np dot res_cvxopt c places 5\\n',\n",
              " 'def test_ha self lp LpInstance 2 2 basis np array 0 1 1 0 dtype float lp update_basis_matrix basis lp add_basis_constraint np array 1 0 dtype float 1 0 lp add_basis_constraint np array 1 0 dtype float 1 0 lp add_basis_constraint np array 0 1 dtype float 0 lp add_basis_constraint np array 0 1 dtype float 0 res np ones 4 lp minimize np array 0 0 dtype float res self assertAlmostEqual res 0 0 0 self assertAlmostEqual res 1 1 0 self assertAlmostEqual res 2 1 0 self assertAlmostEqual res 3 0 0\\n',\n",
              " 'def test_damping self lp LpInstance 1 1 basis np array 0 5 dtype float lp update_basis_matrix basis lp add_basis_constraint np array 1 0 dtype float 1 0 lp add_basis_constraint np array 1 0 dtype float 1 0 res np zeros 2 lp minimize np array 0 dtype float res self assertLess res 0 1 0\\n',\n",
              " 'def define_ha ha LinearHybridAutomaton ha variables x1 x2 x3 x4 x5 x6 x7 x8 t Model ha new_mode Model a_matrix np array 0 1 0 0 0 0 0 0 0 0 1 0865 8487 2 0 0 0 0 0 0 2592 1 21 119 698 91 141399 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0865 8487 2 0 0 0 0 0 0 2592 1 21 119 698 91 141399 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 dtype float c_vector np array 0 0 0 0 0 0 0 0 1 dtype float Model set_dynamics a_matrix c_vector u_constraints_a np array 1 0 1 0 0 1 0 1 dtype float u_constraints_b np array 0 16 0 3 0 2 0 4 dtype float b_matrix np array 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 dtype float Model set_inputs u_constraints_a u_constraints_b b_matrix _error ha new_mode _error _error is_error True trans ha new_transition Model _error trans condition_list append LinearConstraint 0 0 0 0 1 0 0 0 0 0 001501 return ha\\n',\n",
              " 'def define_init_states ha rv constraints constraints append LinearConstraint 1 0 0 0 0 0 0 0 0 0 002 constraints append LinearConstraint 1 0 0 0 0 0 0 0 0 0 0025 constraints append LinearConstraint 0 1 0 0 0 0 0 0 0 0 constraints append LinearConstraint 0 1 0 0 0 0 0 0 0 0 constraints append LinearConstraint 0 0 1 0 0 0 0 0 0 0 constraints append LinearConstraint 0 0 1 0 0 0 0 0 0 0 constraints append LinearConstraint 0 0 0 1 0 0 0 0 0 0 constraints append LinearConstraint 0 0 0 1 0 0 0 0 0 0 constraints append LinearConstraint 0 0 0 0 1 0 0 0 0 0 001 constraints append LinearConstraint 0 0 0 0 1 0 0 0 0 0 0015 constraints append LinearConstraint 0 0 0 0 0 1 0 0 0 0 constraints append LinearConstraint 0 0 0 0 0 1 0 0 0 0 constraints append LinearConstraint 0 0 0 0 0 0 1 0 0 0 constraints append LinearConstraint 0 0 0 0 0 0 1 0 0 0 constraints append LinearConstraint 0 0 0 0 0 0 0 1 0 0 constraints append LinearConstraint 0 0 0 0 0 0 0 1 0 0 constraints append LinearConstraint 0 0 0 0 0 0 0 0 1 0 constraints append LinearConstraint 0 0 0 0 0 0 0 0 1 0 rv append ha modes Model constraints return rv\\n',\n",
              " 'def define_settings plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_NONE plot_settings xdim 0 plot_settings ydim 4 settings HylaaSettings step 0 001 max_time 0 001 plot_settings plot_settings settings print_output False settings counter_example_filename None return settings\\n',\n",
              " 'def make_video self filename frames 100 fps 20 self plot_mode PlotSettings PLOT_VIDEO self anim_delay_interval 0 self min_frame_time 0 self filename filename self num_angles 1024 self extend_plot_range_ratio 0 0 video VideoSettings video frames frames video fps fps self video video\\n',\n",
              " 'def big self size 30 self title_size size self label_size size self tick_label_size int 0 9 size\\n',\n",
              " 'def turn_off self self x_label self y_label self title\\n',\n",
              " 'def define_ha ha LinearHybridAutomaton Trimmed Harmonic Oscillator ha variables x y loc1 ha new_mode loc1 loc1 a_matrix nparray 0 2 1 1 0 2 loc1 c_vector nparray 0 0 inv1 LinearConstraint 0 0 1 0 4 0 loc1 inv_list inv1 return ha\\n',\n",
              " 'def define_init_states ha rv r HyperRectangle 5 9 4 9 0 1 rv append ha modes loc1 r return rv\\n',\n",
              " 'def define_settings plot_settings PlotSettings plot_settings plot_mode PlotSettings PLOT_IMAGE plot_settings xdim 0 plot_settings ydim 1 settings HylaaSettings step 0 1 max_time 10 0 plot_settings plot_settings return settings\\n',\n",
              " 'def write_matlab filename poly_data_dict plot_settings ha with open filename w as f f write Plot reachable region Code generated by Hylaa using PLOT_MATLAB h figure 1 set h Position 200 200 800 600 hold on data reachSet for name data in poly_data_dict iteritems fcol ecol polys data f write 0 n format name fcol 0 fcol 1 fcol 2 ecol 0 ecol 1 ecol 2 for poly in polys f write for pt in poly f write format pt 0 pt 1 f write n f write n f write plot all for i 1 size reachSet 1 face_color reachSet i 2 edge_color reachSet i 3 poly_list reachSet i 5 for p_index 1 size poly_list 1 pts poly_list p_index h fill pts 1 pts 2 face_color EdgeColor edge_color reachSet i 4 h add handle to reachSet data structure for use in legend end end optional legend if size reachSet 1 1 size reachSet 1 10 legend reachSet 3 reachSet 1 end labels and such l plot_settings label x_label l x_label if l x_label is not None else ha variables plot_settings xdim capitalize y_label l y_label if l y_label is not None else ha variables plot_settings ydim capitalize title l title if l title is not None else ha name f write xlabel FontSize FontName Serif Interpreter LaTex n format x_label l label_size f write ylabel FontSize FontName Serif Interpreter LaTex n format y_label l label_size f write title FontSize FontName Serif Interpreter LaTex n format title l title_size f write hold off n\\n',\n",
              " 'def get_script_path filename return os path dirname os path realpath filename\\n',\n",
              " 'def freeze_attrs self self _frozen True\\n',\n",
              " 'def create self keys headers content type application json headers update self headers r requests post self serverURL create data json dumps keys headers headers verify False if r status_code 201 or r status_code 207 return json loads r text return None\\n',\n",
              " 'def read self id_ headers content type application json headers update self headers r requests post self serverURL read data json dumps id_ headers headers verify False if r status_code 200 or r status_code 207 return json loads r text return None\\n',\n",
              " 'def update self keys headers content type application json headers update self headers r requests put self serverURL update data json dumps keys headers headers verify False if r status_code 200 or r status_code 207 return json loads r text return None\\n',\n",
              " 'def upsert self keys headers content type application json headers update self headers r requests post self serverURL upsert data json dumps keys headers headers verify False if r status_code 200 or r status_code 201 return json loads r text return None\\n',\n",
              " 'def delete self id_ headers content type application json headers update self headers r requests delete self serverURL delete data json dumps id_ headers headers verify False if r status_code 200 or r status_code 207 return json loads r text return None\\n',\n",
              " 'def search self query r requests get self serverURL search query headers self headers verify False if r status_code 200 return json loads r text return None\\n',\n",
              " 'def search_one self query r requests get self serverURL search_one query headers self headers verify False if r status_code 200 return json loads r text return None\\n',\n",
              " 'def graph self id_ headers content type application json headers update self headers r requests post self serverURL graph 0 data json dumps id_ headers headers verify False if r status_code 200 or r status_code 207 return json loads r text return None\\n',\n",
              " 'def lock self id_ headers content type application json headers update self headers r requests put self serverURL lock data json dumps id_ headers headers verify False return r status_code 200\\n',\n",
              " 'def publish self keys headers content type application json headers update self headers r requests post self serverURL publish data json dumps keys headers headers verify False if r status_code 201 or r status_code 207 return json loads r text return None\\n',\n",
              " 'def unlock self id_ headers content type application json headers update self headers r requests put self serverURL unlock data json dumps id_ headers headers verify False return r status_code 200\\n',\n",
              " 'def version self id_ keys headers content type application json headers update self headers r requests post s version s self serverURL id_ data json dumps keys headers headers verify False if r status_code 201 return json loads r text return None\\n',\n",
              " 'def comment self keys headers content type application json headers update self headers r requests post self serverURL comment data json dumps keys headers headers verify False if r status_code 201 or r status_code 207 return json loads r text return None\\n',\n",
              " 'def signIn self username password r requests post self serverURL signIn data username username password password verify False if r status_code 200 self token json loads r text self headers Authorization Bearer self token token return True return False\\n',\n",
              " 'def signOut self self token None del self headers Authorization\\n',\n",
              " 'def verify self r requests get self serverURL verify headers self headers verify False if r status_code 200 return True return False\\n',\n",
              " 'def __init__ self conf self conf conf self phab phabricator Phabricator self conf get PHAB_HOST self conf get PHAB_USER self conf get PHAB_CERT self rqueue rqueue RedisQueue conf get REDIS_QUEUE_NAME conf get REDIS_HOST self poll_last_seen_chrono_key 0 self raise_errors False\\n',\n",
              " 'functools lru_cache maxsize 200 def get_user_name self phid info self phab request user query phids phid return info 0 userName\\n',\n",
              " 'def maniphest_info self task_id task_id int task_id 1 info self phab request maniphest info task_id task_id logger debug maniphest info for r s task_id json dumps info return info\\n',\n",
              " 'functools lru_cache maxsize 200 def cached_phid_info self phid return self phid_info phid\\n',\n",
              " 'def get_transaction_info self task_id transaction_phids task_id int task_id 1 info self phab request maniphest gettasktransactions ids task_id transactions for trans in list info values 0 if trans transactionPHID in transaction_phids transactions trans transactionType old trans oldValue new trans newValue if trans comments is not None transactions trans transactionType comments trans comments logger debug get_transaction_info r r s task_id transaction_phids json dumps transactions return transactions\\n',\n",
              " 'def get_anchors_for_task self task_page data_dict_str task_page split script type text javascript JX Stratcom mergeData 0 1 split nJX onload 0 data_dict json loads data_dict_str return x phid x anchor for x in data_dict if phid in x and anchor in x\\n',\n",
              " 'def get_lowest_anchor_for_task_and_XACTs self task_page XACTs anchor_dict self get_anchors_for_task task_page anchors anchor_dict get phid None for phid in XACTs anchors anchor for anchor in anchors if anchor if anchors return anchor format anchor sorted anchors key lambda x int x 0 anchors sorted anchor_dict values key lambda x int x if anchors return anchor format anchor sorted anchors key lambda x int x 0 return\\n',\n",
              " 'def get_type_from_phid self phid return phid split 1\\n',\n",
              " 'def process_event self event_info phid_type self get_type_from_phid event_info data objectPHID if phid_type TASK logger debug Skipping s it is of type s event_info data objectPHID phid_type return logger debug Processing s json dumps event_info phid_info self phid_info event_info data objectPHID task_info self maniphest_info phid_info name task_page self get_task_page phid_info uri try anchor self get_lowest_anchor_for_task_and_XACTs task_page event_info data transactionPHIDs except Exception as e logger exception Could not retrieve anchor for s event_info data transactionPHIDs if self raise_errors raise open data project wikibugs errors XACT anchor event_info data objectPHID w write repr event_info n repr e anchor try projects self get_tags task_page except Exception as e logger exception Could not retrieve tags for s event_info data transactionPHIDs if self raise_errors raise open data project wikibugs errors scrape tags event_info data objectPHID w write repr event_info n repr e projects self get_project_name phid for phid in task_info projectPHIDs useful_event_metadata url phid_info uri anchor projects projects user self get_user_name event_info authorPHID if transactionPHIDs not in event_info data return transactions self get_transaction_info phid_info name event_info data transactionPHIDs ignored core subscribers core columns removed for event in ignored if event in transactions removed append event transactions pop event if not transactions logging debug Skipping PHID which only has an event of type ttype format PHID event_info data transactionPHIDs ttype join removed return if title in transactions useful_event_metadata title transactions title new if transactions title old is None useful_event_metadata new True else useful_event_metadata title phid_info fullName split 1 1 strip if core comment in transactions and comments in transactions core comment useful_event_metadata comment transactions core comment comments for _type in status priority if _type in transactions useful_event_metadata _type transactions _type if status in useful_event_metadata and useful_event_metadata status old is None useful_event_metadata new True useful_event_metadata url useful_event_metadata url split 0 if reassign in transactions trans transactions reassign info for _type in old new if trans _type is not None info _type self get_user_name trans _type else info _type None useful_event_metadata assignee info logger debug useful_event_metadata if not args ask_before_push or input Push y N lower strip y self rqueue put useful_event_metadata\\n',\n",
              " 'def put self item delaytime 2 diff time time self last_pushed if diff delaytime time sleep delaytime diff self redis rpush self key json dumps item self last_pushed time time\\n',\n",
              " 'def get self item self redis blpop self key if item return json loads item 1 decode return item\\n',\n",
              " 'def __init__ self conf builder channelfilter self rqueue rqueue RedisQueue conf get REDIS_QUEUE_NAME conf get REDIS_HOST self conf conf self builder builder self connected False self channelfilter channelfilter\\n',\n",
              " 'def get_channels_for_projects self projects channels set for proj in projects proj_channels self channelfilter channels_for proj if proj_channels channels union proj_channels return channels\\n',\n",
              " 'def escape self text return text replace n replace r\\n',\n",
              " 'def build_project_text self all_projects matched_projects projects for project in all_projects try info all_projects project except KeyError info shade green tagtype briefcase disabled False uri info matched project in matched_projects color self PHAB_COLORS get info shade teal info irc_text self ircformat project color projects project info matched_parts other_projects other_tags hidden_parts for project in sorted projects info projects project if info matched matched_parts append info irc_text elif info disabled hidden_parts append info irc_text elif info tagtype in self OUTPUT_PROJECT_TYPES other_projects append info irc_text else other_tags append info irc_text show_parts matched_parts other_projects other_tags if len show_parts 0 show_parts hidden_parts if len show_parts 0 show_parts self ircformat no projects red style bold overflow_parts show_parts self MAX_NUM_PROJECTS show_parts show_parts self MAX_NUM_PROJECTS if len overflow_parts 1 show_parts append overflow_parts 0 elif len overflow_parts 0 show_parts append and i others len overflow_parts return join show_parts\\n',\n",
              " 'def channels_for self projects channels collections defaultdict list for channel in self config channels for project in projects if self config channels channel match project channels channel append project if not channels channels self default_channel channels pop dev null None channels self firehose_channel return channels\\n',\n",
              " 'asyncio coroutine def handle_useful_info bot useful_info ignored gerritbot ReleaseTaggerBot Stashbot Phabricator_maintenance if useful_info user in ignored logger debug Skipped url s by user s useful_info return updated bot chanfilter update if updated bot privmsg wikimedia cloud log tools wikibugs Updated channels yaml to s updated logger info Updated channels yaml to s updated channels bot chanfilter channels_for useful_info projects for chan matched_projects in channels items useful_info channel chan useful_info matched_projects matched_projects text bot builder build_message useful_info bot privmsg chan text\\n',\n",
              " 'asyncio coroutine def redisrunner bot while True try yield from redislistener bot except Exception logger exception Redis listener crashed restarting in a few seconds yield from asyncio sleep 5\\n',\n",
              " 'asyncio coroutine def redislistener bot connection yield from asyncio_redis Connection create host bot conf get REDIS_HOST port 6379 while True try future yield from connection blpop bot conf get REDIS_QUEUE_NAME useful_info json loads future value if useful_info get raw asyncio Task bot privmsg_many useful_info channels useful_info msg else asyncio Task handle_useful_info bot useful_info except Exception logger exception Redis configuration failed retrying\\n',\n",
              " 'def __init__ self conf builder chanfilter kwargs self channels set kwargs autojoins super Redis2Irc self __init__ kwargs self _conf conf self _builder builder self _chanfilter chanfilter\\n',\n",
              " 'def get self try quarantine_user current_app config QUARANTINE_PHID if not quarantine_user return self respond fetched_data_from_phabricator False request PhabricatorClient request connect task_info request call maniphest query authorPHIDs quarantine_user user_info owner_phids t ownerPHID for t in task_info values if t get ownerPHID if owner_phids user_info request call phid query phids owner_phids return self respond fetched_data_from_phabricator True tasks task_info users user_info except requests exceptions ConnectionError return Unable to connect to Phabricator 503\\n',\n",
              " 'statsreporter timer task_duration_cleanup_task def cleanup_tasks now datetime utcnow pending_tasks Task query filter Task status Status finished Task date_modified now CHECK_TIME for task in pending_tasks task_func TrackedTask queue get_task task task_name task_func delay task_id task task_id hex parent_task_id task parent_id hex if task parent_id else None task data kwargs deleted Task query filter Task status Status finished Task date_modified now EXPIRE_TIME Task date_created now EXPIRE_TIME delete synchronize_session False statsreporter stats incr tasks_deleted deleted\\n',\n",
              " 'def run_migrations connection db engine connect context configure connection connection target_metadata target_metadata try with context begin_transaction context run_migrations finally connection close\\n',\n",
              " 'def error message problems None http_code 400 error_response error message if problems error_response problems problems return error_response http_code\\n',\n",
              " 'def cursor_paginate self queryset id_func lambda e e id kwargs my_request_args request args copy if fake_request in kwargs my_request_args update kwargs fake_request del kwargs fake_request after my_request_args get after before my_request_args get before per_page int my_request_args get per_page 25 if per_page 0 return self respond queryset kwargs start_pos None stop_pos None if after and before return Paging Error cannot pass both after and before as args 400 elif after or before which_token after if after else before encoded_item_id after if after else before item_id urlsafe_b64decode str encoded_item_id if not item_id return Paging Error s has an invalid value which_token 400 position next idx for idx e in enumerate queryset if id_func e item_id 1 if position 1 return Paging Error could not find s token in list which_token 400 elif position len queryset 1 and after return Paging Error cannot get values after the last element 400 if before start_pos max 0 position per_page stop_pos start_pos per_page else start_pos position 1 stop_pos position 1 per_page else start_pos 0 stop_pos per_page page_of_results queryset start_pos stop_pos links self make_cursor_links id_func queryset start_pos if start_pos 0 else None id_func queryset stop_pos 1 if stop_pos len queryset else None return self respond page_of_results links links kwargs\\n',\n",
              " 'def make_cursor_links self before_id None after_id None querystring join 0 1 format quote k quote v for k v in request args iteritems if k before and k after if querystring base_url 0 1 format request base_url querystring else base_url request base_url link_template uri pointer encoded_id rel name links if before_id links append link_template format uri base_url pointer before encoded_id urlsafe_b64encode before_id name previous if after_id links append link_template format uri base_url pointer after encoded_id urlsafe_b64encode after_id name next return links\\n',\n",
              " 'def match_expr pattern return Matcher pattern True\\n',\n",
              " 'def match_stmt pattern return Matcher pattern False\\n',\n",
              " 'def prettyprint_ast node show_attributes False more def format node indent if isinstance node ast AST body if node _fields body list s s field format getattr node field indent more for field in node _fields if show_attributes and node _attributes body list s s a format getattr node a indent more for a in node _attributes total_length sum len stmt for stmt in body len body 2 len indent if any n in stmt for stmt in body or total_length 80 inner indent more n indent more join body n indent return node __class__ __name__ n inner else return node __class__ __name__ join attr for attr in body elif isinstance node list if len node 0 return else return n n join indent more format x indent more for x in node n indent return repr node if not isinstance node ast AST raise TypeError expected AST got r node __class__ __name__ return format node\\n',\n",
              " 'def contains_active_autogenerated_plan project contains db session query literal True filter ItemOption query join Plan Plan id ItemOption item_id filter ItemOption name bazel autogenerate ItemOption value 1 Plan project_id project id Plan status PlanStatus active exists scalar return bool contains\\n',\n",
              " 'def build_patch_uri patch_id app current_app base app config get PATCH_BASE_URI or app config INTERNAL_BASE_URI return _concat_uri_path base api 0 patches 0 raw 1 format patch_id hex\\n',\n",
              " 'def enforce_is_subdir path root PROJECT_ROOT if not os path abspath path startswith root raise RuntimeError this path is not safe\\n',\n",
              " 'def get_state self app None if app is None app self get_app assert self ident in app extensions The extension was not registered to the current application Please make sure to call init_app first return app extensions self ident\\n',\n",
              " 'def get self args self get_parser parse_args cluster args cluster limit args limit with statsreporter stats timer jobstep_allocate_get available_allocations self find_next_jobsteps limit cluster jobstep_results self serialize available_allocations buildstep_for_job_id for jobstep jobstep_data in zip available_allocations jobstep_results if jobstep job_id not in buildstep_for_job_id buildstep_for_job_id jobstep job_id JobPlan get_build_step_for_job jobstep job_id 1 buildstep buildstep_for_job_id jobstep job_id limits buildstep get_resource_limits req_cpus limits get cpus DEFAULT_CPUS req_mem limits get memory DEFAULT_MEMORY_MB allocation_cmd buildstep get_allocation_command jobstep jobstep_data project jobstep project jobstep_data resources cpus req_cpus mem req_mem jobstep_data cmd allocation_cmd return self respond jobsteps jobstep_results\\n',\n",
              " 'def post self args json loads request data try jobstep_ids args jobstep_ids except KeyError return error Missing jobstep_ids attribute for id in jobstep_ids try UUID id except ValueError err Invalid jobstep id sent to jobstep_allocate s logging warning err id exc_info True return error err id cluster args get cluster with statsreporter stats timer jobstep_allocate_post try lock_key jobstep allocate if cluster lock_key lock_key cluster with redis lock lock_key nowait True jobsteps JobStep query filter JobStep id in_ jobstep_ids for jobstep in jobsteps if jobstep cluster cluster db session rollback err Jobstep is in cluster s but tried to allocate in cluster s id s project s err_args jobstep cluster cluster jobstep id hex jobstep project slug logging warning err err_args return error err err_args if jobstep status Status pending_allocation db session rollback err Jobstep s for project s was already allocated err_args jobstep id hex jobstep project slug logging warning err err_args return error err err_args http_code 409 jobstep status Status allocated jobstep last_heartbeat datetime utcnow db session add jobstep pending_seconds datetime utcnow jobstep date_created total_seconds statsreporter stats log_timing duration_pending_allocation pending_seconds 1000 db session commit return self respond allocated jobstep_ids except UnableToGetLock return error Another allocation is in progress http_code 409 except IntegrityError err Could not commit allocation logging warning err exc_info True return error err http_code 409\\n',\n",
              " 'def get self step_id if not JobStep query get step_id return self respond status_code 404 artifacts Artifact query filter_by step_id step_id all return self respond artifacts artifacts\\n',\n",
              " 'tracked_task on_abort abort_create max_retries 10 def create_job job_id job Job query get job_id if not job return if job project status ProjectStatus inactive current_app logger warn Project is not active s job project slug job status Status finished job result Result aborted db session add job db session flush return if job status Status finished return _ implementation JobPlan get_build_step_for_job job_id job id if implementation is None job status Status finished job result Result aborted db session add job db session flush current_app logger exception No build plan set s job_id return try implementation execute job job except UnrecoverableException job status Status finished job result Result infra_failed db session add job db session flush current_app logger exception Unrecoverable exception creating s job_id return sync_job delay job_id job id hex task_id job id hex parent_task_id job build_id hex\\n',\n",
              " 'property def duration self if self date_started and self date_finished duration self date_finished self date_started total_seconds 1000 else duration None return duration\\n',\n",
              " 'property def current_steps self return s for s in self steps if s replacement_id is None\\n',\n",
              " 'def files_changed_should_trigger_project files_changed project project_options sha diff None config_path project get_config_path if config_path in files_changed return True try config project get_config sha diff config_path except ProjectConfigError logging exception Project config for project s is not in a valid format blacklist is being ignored project slug config except Exception logging exception Exception occurred trying to parse project config for project s project slug config if not _time_based_exclusion_filter config project return False blacklist config get build file blacklist blacklist_patterns _compile_patterns blacklist files_changed filter lambda f not _match_file_patterns blacklist_patterns f files_changed if len files_changed 0 return _in_project_files_whitelist project_options files_changed else return False\\n',\n",
              " 'def handle_error self e if not hasattr e code and self app propagate_exceptions got_request_exception send self app exception e raise return super APIController self handle_error e\\n',\n",
              " 'def get_current_datetime return datetime utcnow\\n',\n",
              " 'def get_plans_for_cluster cluster plans set q db session query Plan Step q q filter Plan id Step plan_id Plan status PlanStatus active for plan step in q all if step data and step data get cluster None cluster plans add plan return plans\\n',\n",
              " 'def get_cached_snapshot_images cluster now get_current_datetime plan_ids plan id for plan in get_plans_for_cluster cluster if not plan_ids return return db session query SnapshotImage filter sqlalchemy or_ CachedSnapshotImage expiration_date None now CachedSnapshotImage expiration_date CachedSnapshotImage id SnapshotImage id SnapshotImage plan_id in_ plan_ids all\\n',\n",
              " 'def _cache_image snapshot_image return db session merge CachedSnapshotImage id snapshot_image id expiration_date None\\n',\n",
              " 'def cache_snapshot snapshot now get_current_datetime CachedSnapshotImage query filter CachedSnapshotImage expiration_date None CachedSnapshotImage id SnapshotImage id SnapshotImage snapshot_id Snapshot id Snapshot project_id snapshot project_id update dict expiration_date now current_app config CACHED_SNAPSHOT_EXPIRATION_DELTA synchronize_session fetch images SnapshotImage query filter SnapshotImage snapshot_id snapshot id cached_snapshot_images _cache_image image for image in images db session add_all cached_snapshot_images db session commit\\n',\n",
              " 'def get_relevant_snapshot_images snapshot_id steps Step query filter Plan id SnapshotImage plan_id Step plan_id Plan id SnapshotImage snapshot_id snapshot_id all clusters set for step in steps if step data and cluster in step data clusters add step data cluster return dict cluster get_cached_snapshot_images cluster for cluster in clusters\\n',\n",
              " 'def clear_expired now get_current_datetime CachedSnapshotImage query filter CachedSnapshotImage expiration_date now delete db session commit\\n',\n",
              " 'classmethod def check cls task_name parent_id child_tasks list db session query cls result Task status filter cls task_name task_name cls parent_id parent_id if any r status Status finished for r in child_tasks return Status in_progress return Status finished\\n',\n",
              " 'def auth_with_refresh_token cookies refresh_token Fernet current_app config COOKIE_ENCRYPTION_KEY decrypt str cookies refresh_token values grant_type refresh_token refresh_token refresh_token client_id current_app config GOOGLE_CLIENT_ID client_secret current_app config GOOGLE_CLIENT_SECRET headers content type application x www form urlencoded refresh_request requests post GOOGLE_TOKEN_URI data values headers headers resp refresh_request json return resp\\n',\n",
              " 'def get_orig_url_redirect state if state originating_url base64 urlsafe_b64decode state encode utf 8 else originating_url url_for index url_parts list urlparse urlparse originating_url query dict urlparse parse_qsl url_parts 4 query finished_login success url_parts 4 urllib urlencode query return redirect urlparse urlunparse url_parts\\n',\n",
              " 'def set_session_state access_token None email None user _ get_or_create User where email email if current_app config DEBUG user is_admin True db session add user session uid user id hex session access_token access_token session email email\\n',\n",
              " 'mock patch changes lib snapshot_garbage_collection get_current_datetime def test_get_current_datetime self get_current_datetime get_current_datetime return_value self mock_datetime self client get self get_endpoint_path cluster get_current_datetime assert_any_call\\n',\n",
              " 'mock patch changes lib snapshot_garbage_collection get_current_datetime def test_multiproject self get_current_datetime project1 self create_project project2 self create_project plan1_1 self create_plan project1 plan1_2 self create_plan project1 plan2_1 self create_plan project2 plan2_2 self create_plan project2 plan2_3 self create_plan project2 self create_step plan1_1 data cluster cluster1 self create_step plan1_2 data cluster cluster2 self create_step plan2_1 data cluster cluster2 self create_step plan2_2 data cluster cluster2 snapshot1 self create_snapshot project1 snapshot2 self create_snapshot project2 snapshot_image1_1 self create_snapshot_image snapshot1 plan1_1 snapshot_image1_2 self create_snapshot_image snapshot1 plan1_2 snapshot_image2_1 self create_snapshot_image snapshot2 plan2_1 snapshot_image2_2 self create_snapshot_image snapshot2 plan2_2 snapshot_image2_3 self create_snapshot_image snapshot2 plan2_3 self create_cached_snapshot_image snapshot_image1_1 self create_cached_snapshot_image snapshot_image1_2 expiration_date self mock_datetime datetime timedelta 0 1 self create_cached_snapshot_image snapshot_image2_1 expiration_date self mock_datetime datetime timedelta 0 1 self create_cached_snapshot_image snapshot_image2_2 self create_cached_snapshot_image snapshot_image2_3 get_current_datetime return_value self mock_datetime resp self client get self get_endpoint_path cluster2 assert resp status_code 200 data self unserialize resp assert len data 2 assert snapshot_image1_2 id hex in data assert snapshot_image2_2 id hex in data resp self client get self get_endpoint_path cluster3 assert resp status_code 200 data self unserialize resp assert data\\n',\n",
              " 'def create_or_update_revision_result revision_sha project_id propagation_limit project Project query get project_id revision Revision query filter Revision sha revision_sha Revision repository_id project repository_id first last_finished_build get_latest_finished_build_for_revision revision_sha project_id if not last_finished_build return unaffected_targets BazelTarget query join Job BazelTarget job_id Job id filter BazelTarget result_source ResultSource from_parent Job build_id last_finished_build id all if len unaffected_targets 0 and len revision parents 0 parent_revision_sha revision parents 0 parent_build get_latest_finished_build_for_revision parent_revision_sha project_id if parent_build unaffected_targets_groups defaultdict lambda for target in unaffected_targets unaffected_targets_groups target job_id target name target for job_id targets_dict in unaffected_targets_groups iteritems jobplan JobPlan query filter JobPlan project_id project_id JobPlan build_id last_finished_build id JobPlan job_id job_id first if not jobplan continue parent_targets BazelTarget query join Job BazelTarget job_id Job id join JobPlan BazelTarget job_id JobPlan job_id filter Job build_id parent_build id BazelTarget name in_ targets_dict JobPlan plan_id jobplan plan_id for parent_target in parent_targets targets_dict parent_target name result parent_target result db session add targets_dict parent_target name else logger info Revision s could not find a parent build for parent revision s revision_sha parent_revision_sha revision_result _ create_or_update RevisionResult where revision_sha revision_sha project_id project_id values build_id last_finished_build id result aggregate_result last_finished_build result t result for t in unaffected_targets db session commit fire_signal delay signal revision_result updated kwargs revision_result_id revision_result id hex if propagation_limit 0 for child_revision in get_child_revisions revision create_or_update_revision_result child_revision sha project_id propagation_limit propagation_limit 1\\n',\n",
              " 'def bounded_integer lower upper def parse s iv int s if iv lower or iv upper raise ValueError is not in format iv lower upper return iv return parse\\n',\n",
              " 'def update_local_repos repo_list list Repository query filter Repository status RepositoryStatus inactive for repo in repo_list vcs repo get_vcs db session commit if vcs is None logger warning Repository s has no VCS backend set repo id continue try if vcs exists vcs update else vcs clone except ConcurrentUpdateError pass except CommandError logging exception Failed to update s repo url\\n',\n",
              " 'classmethod def get_by_sha_prefix_query self repository_id sha_prefix return Revision query filter Revision repository_id repository_id Revision sha like format sha_prefix\\n',\n",
              " 'def post self args self parser parse_args if args repository is None return error Repository url is not recognized problems repository sync_repo delay repo_id args repository id hex continuous False return\\n',\n",
              " 'def find_failure_origins build test_failures project build project if not test_failures return last_pass Build query join Source Source id Build source_id filter Build project project Build date_created build date_created Build status Status finished Build result Result passed Build id build id Source patch None order_by Build date_created desc first if last_pass is None return previous_runs Build query join Source Source id build source_id filter Build project project Build date_created build date_created Build date_created last_pass date_created Build status Status finished Build result in_ Result failed Result passed Build id build id Build id last_pass id Source patch None order_by Build date_created desc 100 if not previous_runs return queryset db session query TestCase name_sha Job build_id join Job Job id TestCase job_id filter Job build_id in_ b id for b in previous_runs Job status Status finished Job result Result failed TestCase result Result failed TestCase name_sha in_ t name_sha for t in test_failures group_by TestCase name_sha Job build_id previous_test_failures defaultdict set for name_sha build_id in queryset previous_test_failures build_id add name_sha failures_at_build dict searching set t for t in test_failures last_checked_run build for p_build in previous_runs p_build_failures previous_test_failures p_build id for f_test in list searching if f_test name_sha not in p_build_failures failures_at_build f_test last_checked_run searching remove f_test last_checked_run p_build for f_test in searching failures_at_build f_test last_checked_run return failures_at_build\\n',\n",
              " 'def log self parent None branch None author None offset 0 limit 100 paths None start_time time cmd log template s LOG_FORMAT if parent and branch raise ValueError Both parent and branch cannot be set r_str None if branch cmd append b 0 format branch if parent r_str ancestors s parent if author r_str r and author 0 if r_str else author 0 format author r r_str if r_str cmd append r reverse 0 format r_str if limit cmd append limit d offset limit if paths cmd extend glob p strip for p in paths result self run cmd self log_timing log start_time for idx chunk in enumerate BufferParser result x02 if idx offset continue sha author author_date parents branches message chunk split x01 branches filter bool branches split or default parents filter lambda x x and x 0 40 parents split author_date datetime utcfromtimestamp mktime_tz parsedate_tz author_date yield RevisionResult id sha author author author_date author_date message message parents parents branches branches\\n',\n",
              " 'def export self id cmd diff g c s id result self run cmd return result\\n',\n",
              " 'def get_changed_files self id cmd status rev s s id id n output self run cmd return set x strip for x in output splitlines\\n',\n",
              " 'def get_known_branches self start_time time cmd branches results self run cmd branch_names set for line in results splitlines if line name line split None 1 if name 0 branch_names add name 0 self log_timing get_known_branches start_time return list branch_names\\n',\n",
              " 'def read_file self sha file_path diff None content self run cat r sha file_path if diff is None return content return self _selectively_apply_diff file_path content diff\\n',\n",
              " 'def get_patch_hash self rev_sha return None\\n',\n",
              " 'def postback_error self msg target problems None http_code 400 if target message An error occurred somewhere between Phabricator and Changes s Please contact s with any questions icon times color red msg current_app config SUPPORT_CONTACT post_comment target message return error msg problems problems http_code http_code\\n',\n",
              " 'def post_impl self args self parser parse_args if not args repository statsreporter stats incr diffs_repository_not_found return error Repository not found repository args repository projects list Project query options subqueryload_all plans filter Project status ProjectStatus active Project repository_id repository id if not projects return self respond options dict db session query ProjectOption project_id ProjectOption value filter ProjectOption project_id in_ p id for p in projects ProjectOption name in_ phabricator diff trigger projects p for p in projects if options get p id 1 1 and get_build_plans p if not projects return self respond statsreporter stats incr diffs_posted_from_phabricator label args label 128 author args author message args message sha args sha target D s args phabricator revisionID try identify_revision repository sha except MissingRevision logging error Diff s was posted for an unknown revision s s target sha repository url statsreporter stats incr diffs_missing_base_revision return self postback_error Unable to find base revision revision in repo on Changes Some possible reasons You may be working on multiple stacked diffs in your local repository revision only exists in your local copy Changes thus cannot apply your patch If you are sure that s not the case it s possible you applied your patch to an extremely recent revision which Changes hasn t picked up yet Retry in a minute format revision sha repo repository url target problems sha repository source_data phabricator buildTargetPHID args phabricator buildTargetPHID phabricator diffID args phabricator diffID phabricator revisionID args phabricator revisionID phabricator revisionURL args phabricator revisionURL patch Patch repository repository parent_revision_sha sha diff join line decode utf 8 for line in args patch_file db session add patch source Source patch patch repository repository revision_sha sha data source_data db session add source phabricatordiff try_create PhabricatorDiff diff_id args phabricator diffID revision_id args phabricator revisionID url args phabricator revisionURL source source if phabricatordiff is None logging warning Diff s Revision s already exists args phabricator diffID args phabricator revisionID statsreporter stats incr diffs_already_exists return error Diff already exists within Changes project_options ProjectOptionsHelper get_options projects build file whitelist diff_parser DiffParser patch diff files_changed diff_parser get_changed_files collection_id uuid uuid4 builds for project in projects plan_list get_build_plans project assert plan_list No plans defined for project format project slug if not files_changed_should_trigger_project files_changed project project_options project id sha diff patch diff logging info No changed files matched project trigger for project s project slug continue selective_testing_policy SelectiveTestingPolicy disabled if args selective_testing and project_lib contains_active_autogenerated_plan project selective_testing_policy SelectiveTestingPolicy enabled builds append create_build project project collection_id collection_id sha sha target target label label message message author author patch patch tag phabricator selective_testing_policy selective_testing_policy statsreporter stats incr diffs_successfully_processed_from_phabricator return self respond builds\\n',\n",
              " 'def _datetime_to_timestamp dt return int dt datetime utcfromtimestamp 0 total_seconds\\n',\n",
              " 'def _get_phabricator_revision_url build source_data build source data or rev_url source_data get phabricator revisionURL if rev_url return rev_url if build message matches _REV_URL_RE findall build message if matches and len matches 1 return matches 0 return None\\n',\n",
              " 'def _get_build_failure_reasons build failure_reasons r for r in db session query distinct FailureReason reason join JobStep JobStep id FailureReason step_id filter FailureReason build_id build id JobStep replacement_id is_ None all return sorted failure_reasons\\n',\n",
              " 'def post_analytics_data url data try resp requests post url headers Content Type application json data json dumps data timeout 10 resp raise_for_status except Exception logger exception Failed to post to Analytics\\n',\n",
              " 'def _get_itemstat_dict iid return dict s name s value for s in ItemStat query filter ItemStat item_id iid\\n',\n",
              " 'def _categorize_step_logs job tags_by_step defaultdict set rules _get_rules if rules for ls in _get_failing_log_sources job logdata _get_log_data ls tags applicable categorize categorize job project slug rules logdata tags_by_step ls step_id update tags _incr failing log processed if not tags and applicable _incr failing log uncategorized else for tag in tags _incr failing log category format tag return tags_by_step\\n',\n",
              " 'def _get_job_failure_reasons_by_jobstep job reasons r for r in db session query FailureReason reason FailureReason step_id filter FailureReason job_id job id all reasons_by_jobsteps defaultdict list for reason in reasons reasons_by_jobsteps reason step_id append reason reason for step_id in reasons_by_jobsteps reasons_by_jobsteps step_id sort return reasons_by_jobsteps\\n',\n",
              " 'def _get_rules rules_file current_app config get CATEGORIZE_RULES_FILE if not rules_file return None return categorize load_rules rules_file\\n',\n",
              " 'def _incr name statsreporter stats incr name\\n',\n",
              " 'def upgrade conn op get_bind conn execute text SELECT pg_terminate_backend procpid FROM pg_stat_activity WHERE current_query autovacuum VACUUM public test to prevent wraparound BEGIN ALTER TABLE test ADD COLUMN owner TEXT UPDATE alembic_version SET version_num revision COMMIT revision revision\\n',\n",
              " 'def downgrade conn execute text SELECT pg_terminate_backend procpid FROM pg_stat_activity WHERE current_query autovacuum VACUUM public test to prevent wraparound BEGIN ALTER TABLE test DROP COLUMN owner UPDATE alembic_version SET version_num down_revision COMMIT down_revision down_revision\\n',\n",
              " 'def test_commands_snapshot_with_script self builder self get_builder snapshot_script cache data project self create_project build self create_build project job self create_job build jobphase self create_jobphase job jobstep self create_jobstep jobphase plan self create_plan project self create_job_plan job plan snapshot self create_snapshot project self create_snapshot_image snapshot plan job job params builder get_job_parameters job jobstep id hex path foo assert params SCRIPT cache data builder create_commands jobstep params db session commit for command in jobstep commands assert command env SCRIPT cache data\\n',\n",
              " 'def import_string path result _cache path return result\\n',\n",
              " 'def is_collector self return self in set CommandType collect_steps CommandType collect_tests CommandType collect_bazel_targets\\n',\n",
              " 'def is_setup self return self in set CommandType setup CommandType infra_setup\\n',\n",
              " 'def is_valid_for_default self return not self is_collector and self CommandType snapshot\\n',\n",
              " 'def is_valid_for_snapshot self return self is_setup or self in set CommandType snapshot CommandType teardown\\n',\n",
              " 'def is_valid_for_collection self return self is_collector or self CommandType infra_setup\\n',\n",
              " 'classmethod def from_command cls command return cls command script command cwd command artifacts command env command label command type\\n',\n",
              " 'def as_command self jobstep order return Command jobstep_id jobstep id script self script cwd self path artifacts self artifacts env self env label self label order order status Status queued type self type\\n',\n",
              " 'def get_coverage_by_source_id source_id source Source query get source_id projects Project query filter Project repository_id source repository_id newest_build_ids set for project in projects b_id db session query Build id filter Build project_id project id Build source_id source_id Build status Status finished order_by Build date_created desc first if b_id newest_build_ids add b_id 0 return get_coverage_by_build_ids newest_build_ids\\n',\n",
              " 'def get_coverage_by_build_ids build_ids if not build_ids return all_job_ids db session query Job id filter Job build_id in_ build_ids return get_coverage_by_job_ids all_job_ids\\n',\n",
              " 'def get_coverage_by_job_ids job_ids if not job_ids return return FileCoverage query filter FileCoverage job_id in_ job_ids\\n',\n",
              " 'def merge_coverage old new cov_data for lineno in range max len old len new try old_cov old lineno except IndexError old_cov N try new_cov new lineno except IndexError new_cov N if old_cov C or new_cov C cov_data append C elif old_cov U or new_cov U cov_data append U else cov_data append N return join cov_data\\n',\n",
              " 'def merged_coverage_data coverages coverage for c in coverages data coverage get c filename if data data merge_coverage data c data else data c data coverage c filename data return coverage\\n',\n",
              " 'def get_coverage_stats diff_lines data lines_covered 0 lines_uncovered 0 diff_lines_covered 0 diff_lines_uncovered 0 for lineno code in enumerate data line_in_diff bool lineno 1 in diff_lines if code C lines_covered 1 if line_in_diff diff_lines_covered 1 elif code U lines_uncovered 1 if line_in_diff diff_lines_uncovered 1 return CoverageStats lines_covered lines_uncovered diff_lines_covered diff_lines_uncovered\\n',\n",
              " 'def get_config self revision_sha diff None config_path None from changes vcs base import CommandError ContentReadError MissingFileError ConcurrentUpdateError UnknownRevision if config_path is None config_path self get_config_path vcs self repository get_vcs if vcs is None raise NotImplementedError else try try config_content vcs read_file revision_sha config_path diff diff except UnknownRevision try vcs update except ConcurrentUpdateError vcs update config_content vcs read_file revision_sha config_path diff diff except CommandError as err logging warning Git invocation failed for project s s self slug str err exc_info True config_content except MissingFileError config_content except ContentReadError as err logging warning Config for project s cannot be read s self slug str err exc_info True config_content try config yaml safe_load config_content if not isinstance config dict logging warning Config for project s is not a dict using default config self slug extra data revision revision_sha diff diff config except yaml YAMLError raise ProjectConfigError Invalid project config file format config_path for k v in self _default_config iteritems config setdefault k v return config\\n',\n",
              " 'def _filter_coverage_for_added_lines self diff coverage if not diff return None diff_lines diff splitlines current_file None line_number None coverage_by_added_line for line in diff_lines if line startswith diff current_file None line_number None elif current_file is None and line_number is None and line startswith or line startswith if line startswith b line line split t 0 current_file unicode line 6 elif line startswith line_num_info line split 1 line_num_info line_num_info rstrip line_number int line_num_info split 0 1 elif current_file is not None and line_number is not None if line startswith cov N if current_file in coverage try cov coverage current_file line_number except IndexError logger logging getLogger coverage logger info Missing code coverage for line d of file s line_number current_file coverage_by_added_line append cov if not line startswith line_number 1 return coverage_by_added_line\\n',\n",
              " 'def _get_files_from_raw_diff self diff files set diff_lines diff split n for line in diff_lines if line startswith b line line split t 0 files add line 6 return files\\n',\n",
              " 'contextmanager def lock self lock_key expire None blocking_timeout 3 nowait False conn self redis if expire is None expire blocking_timeout delay 0 01 random 10 lock conn lock lock_key timeout expire sleep delay acquired lock acquire blocking not nowait blocking_timeout blocking_timeout start time time self logger info Acquiring lock on s lock_key if not acquired raise UnableToGetLock Unable to fetch lock on s lock_key try yield finally self logger info Releasing lock on s lock_key try lock release except Exception self logger exception Error releasing lock s acquired around ss ago lock_key time time start\\n',\n",
              " 'def get_selective_testing_policy project sha diff None try config project get_config sha diff except ProjectConfigError logger exception Project config for project s is not in a valid format Selective testing will not be done project slug return SelectiveTestingPolicy disabled Project config file not in valid format except InvalidDiffError logger exception Unable to apply diff for project s Selective testing will not be done project slug return SelectiveTestingPolicy disabled Unable to apply diff except Exception logger exception Exception occurred trying to parse project config for project s project slug return SelectiveTestingPolicy disabled Unknown error while trying to read project config file results f project project project_config config sha sha diff diff for f in SELECTIVE_TESTING_RULES policy aggregate_selective_testing_policy p for p _ in results reasons r for _ r in results if r is not None return policy reasons\\n',\n",
              " 'def parse_revision_id message match re search Differential Revision D d message re MULTILINE return int match group 1 if match else None\\n',\n",
              " 'def get_test_failures_in_base_commit build commit_sources s id for s in Source query filter Source revision_sha build source revision_sha if s is_commit base_builds Build query filter Build source_id in_ commit_sources Build project_id build project_id if not list base_builds logger info Unable to find base build for s build source revision_sha return None jobs list Job query filter Job build_id in_ b id for b in base_builds if not list jobs logger info Unable to find jobs matching build for s build source revision_sha return None test_failures TestCase query options joinedload job innerjoin True filter TestCase job_id in_ j id for j in jobs TestCase result Result failed return test name for test in test_failures\\n',\n",
              " 'tracked_task def sync_artifact artifact_id None kwargs artifact Artifact query get artifact_id if artifact is None return step artifact step if step result Result aborted return _ implementation JobPlan get_build_step_for_job job_id step job_id if artifact file try implementation get_artifact_manager step process artifact except UnrecoverableException current_app logger exception Unrecoverable exception processing artifact s s artifact step_id artifact else try implementation fetch_artifact artifact artifact except UnrecoverableException current_app logger exception Unrecoverable exception fetching artifact s s artifact step_id artifact\\n',\n",
              " 'def validate self raise NotImplementedError\\n',\n",
              " 'def expand self job max_executors kwargs raise NotImplementedError\\n',\n",
              " 'def default_phase_name self raise NotImplementedError\\n',\n",
              " 'def _deduplicate_testresults results result_dict deduped for result in results key result package result name existing_result result_dict get key if existing_result is not None e r existing_result result e duration _careful add e duration r duration e result aggregate_result e result r result if e message and r message e message n n r message elif not e message e message r message or e reruns _careful max e reruns r reruns e artifacts _careful add e artifacts r artifacts e message_offsets _careful add e message_offsets r message_offsets else result_dict key result deduped append result return deduped\\n',\n",
              " 'def _careful op a b if a is None return b if b is None return a return op a b\\n',\n",
              " 'def truncate_message msg limit if msg is None or len msg limit return msg nl msg find n len msg limit if nl 1 msg else msg msg nl 1 return _TRUNCATION_HEADER msg\\n',\n",
              " 'def sync_grouper try admin_emails _get_admin_emails_from_grouper _sync_admin_users admin_emails project_admin_mapping _get_project_admin_mapping_from_grouper _sync_project_admin_users project_admin_mapping except Exception logger exception An error occurred during Grouper sync statsreporter stats set_gauge grouper_sync_error 1 raise else statsreporter stats set_gauge grouper_sync_error 0\\n',\n",
              " 'def _get_admin_emails_from_grouper url urlparse urljoin current_app config GROUPER_API_URL permissions format current_app config GROUPER_PERMISSIONS_ADMIN response requests get url json if errors in response message n join x message for x in response errors raise GrouperApiError message groups response data groups admin_users set for _ group in groups iteritems for email user in group users iteritems if user rolename not in current_app config GROUPER_EXCLUDED_ROLES admin_users add email return admin_users\\n',\n",
              " 'def _sync_admin_users admin_emails assert len admin_emails 0 User query filter User email in_ admin_emails User is_admin is_ True update is_admin False synchronize_session False for email in admin_emails create_or_update User where email email values is_admin True db session commit\\n',\n",
              " 'def _get_project_admin_mapping_from_grouper url urlparse urljoin current_app config GROUPER_API_URL permissions format current_app config GROUPER_PERMISSIONS_PROJECT_ADMIN response requests get url json if errors in response if len response errors 1 and response errors 0 code 404 return dict message n join x message for x in response errors raise GrouperApiError message groups response data groups mapping dict for _ group in groups iteritems pattern_set set for p in group permissions if p permission current_app config GROUPER_PERMISSIONS_PROJECT_ADMIN pattern p argument if len pattern 0 pattern_set add pattern if len pattern_set 0 for email user in group users iteritems if user rolename not in current_app config GROUPER_EXCLUDED_ROLES existing_pattern_set mapping get email set mapping email existing_pattern_set union pattern_set return mapping\\n',\n",
              " 'def _sync_project_admin_users project_admin_mapping args User project_permissions is_ None if len project_admin_mapping 0 args append User email in_ project_admin_mapping keys User query filter args update project_permissions None synchronize_session False for email project_permissions in project_admin_mapping iteritems create_or_update User where email email values project_permissions list project_permissions db session commit\\n',\n",
              " 'def _construct_subprocess self args kwargs env os environ copy for key value in self get_default_env iteritems env setdefault key value env setdefault CHANGES_SSH_REPO self url for key value in kwargs pop env env key value kwargs env env kwargs close_fds True kwargs setdefault stdout PIPE kwargs setdefault stderr PIPE kwargs setdefault stdin PIPE proc Popen args kwargs return proc\\n',\n",
              " 'def _execute_subproccess self proc args kwargs stdout stderr proc communicate kwargs if proc returncode 0 raise CommandError args 0 proc returncode stdout stderr return stdout\\n',\n",
              " 'classmethod def get_repository_name cls repository_url path repository_url split 1 strip return os path basename path\\n',\n",
              " 'def log self parent None branch None author None offset 0 limit 100 raise NotImplementedError\\n',\n",
              " 'def export self id raise NotImplementedError\\n',\n",
              " 'def get_changed_files self id diff self export id diff_parser DiffParser diff return diff_parser get_changed_files\\n',\n",
              " 'def get_known_branches self raise NotImplementedError\\n',\n",
              " 'def read_file self sha file_path diff None raise NotImplementedError\\n',\n",
              " 'def get_patch_hash self rev_sha raise NotImplementedError\\n',\n",
              " 'def _selectively_apply_diff self file_path file_content diff parser DiffParser diff selected_diff None for file_info in parser parse if file_info new_filename is not None and file_info new_filename 2 file_path selected_diff parser reconstruct_file_diff file_info if selected_diff is None return file_content temp_patch_file_path None temp_dir None try fd temp_patch_file_path tempfile mkstemp os write fd selected_diff os close fd dir_name _ os path split file_path temp_dir tempfile mkdtemp if len dir_name 0 os makedirs os path join temp_dir dir_name temp_file_path os path join temp_dir file_path with open temp_file_path w as f f write file_content try check_call patch strip 1 unified directory format temp_dir input format temp_patch_file_path except CalledProcessError raise InvalidDiffError with open temp_file_path r as f patched_content f read finally if temp_patch_file_path and os path exists temp_patch_file_path os remove temp_patch_file_path if temp_dir and os path exists temp_dir shutil rmtree temp_dir return patched_content\\n',\n",
              " 'def load_rules path with open path as file return parse_rules file read path\\n',\n",
              " 'def _parse_rule line line line strip if not line or line startswith return None try tag project regexp line split 2 except ValueError raise ParseError syntax error regexp _parse_regexp regexp return tag strip project strip regexp\\n',\n",
              " 'def categorize project rules output output output replace r n n matched applicable set set for tag rule_project regexp in rules if not rule_project or rule_project project applicable add tag if re search regexp output re MULTILINE re DOTALL matched add tag return matched applicable\\n',\n",
              " 'tracked_task on_abort abort_build def sync_build build_id build Build query get build_id if not build return if build status Status finished return all_jobs list Job query filter Job build_id build_id is_finished sync_build verify_all_children Status finished if any p status Status finished for p in all_jobs is_finished False prev_started build date_started build date_started safe_agg min j date_started for j in all_jobs if j date_started if not prev_started and build date_started queued_time build date_started build date_created statsreporter stats log_timing build_start_latency _timedelta_to_millis queued_time if is_finished build date_finished safe_agg max j date_finished for j in all_jobs if j date_finished datetime utcnow else build date_finished None if build date_started and build date_finished build duration _timedelta_to_millis build date_finished build date_started else build duration None if any j result is Result failed for j in all_jobs build result Result failed elif is_finished build result aggregate_result j result for j in all_jobs else build result Result unknown if is_finished build status Status finished else new_status aggregate_status j status for j in all_jobs if new_status Status finished build status new_status if is_finished build date_decided datetime utcnow decided_latency build date_decided build date_finished statsreporter stats log_timing build_decided_latency _timedelta_to_millis decided_latency else build date_decided None if db session is_modified build build date_modified datetime utcnow db session add build db session commit if not is_finished raise sync_build NotFinished with statsreporter stats timer build_stat_aggregation try aggregate_build_stat build test_count aggregate_build_stat build test_duration aggregate_build_stat build test_failures aggregate_build_stat build test_rerun_count aggregate_build_stat build tests_missing aggregate_build_stat build lines_covered aggregate_build_stat build lines_uncovered aggregate_build_stat build diff_lines_covered aggregate_build_stat build diff_lines_uncovered except Exception current_app logger exception Failing recording aggregate stats for build s build id fire_signal delay signal build finished kwargs build_id build id hex queue delay update_project_stats kwargs project_id build project_id hex countdown 1\\n',\n",
              " 'classmethod def build_jobplan cls plan job snapshot_id None from changes models option import ItemOption from changes models snapshot import SnapshotImage plan_steps sorted plan steps key lambda x x order option_item_ids s id for s in plan_steps option_item_ids append plan id options defaultdict dict options_query db session query ItemOption item_id ItemOption name ItemOption value filter ItemOption item_id in_ option_item_ids for item_id opt_name opt_value in options_query options item_id opt_name opt_value snapshot steps HistoricalImmutableStep from_step s options s id to_json for s in plan_steps options options plan id snapshot_image_id None allow_snapshot 1 options plan id get snapshot allow 1 or plan snapshot_plan if allow_snapshot and snapshot_id is not None snapshot_image SnapshotImage get plan snapshot_id if snapshot_image is not None snapshot_image_id snapshot_image id if snapshot_image is None logging warning Failed to find snapshot_image for s s s plan project slug plan label instance cls plan_id plan id job_id job id build_id job build_id project_id job project_id snapshot_image_id snapshot_image_id data snapshot snapshot return instance\\n',\n",
              " 'def _logsource_display_priority ls priority 1 if ls in_artifact_store priority 1 if ls is_infrastructural priority 2 return priority\\n',\n",
              " 'def _gather data collected if isinstance data _PASSTHROUGH return data elif isinstance data dict keys k if isinstance k _PASSTHROUGH else _gather k collected for k in data iterkeys values _gather v collected for v in data itervalues return dict izip keys values elif isinstance data list tuple set frozenset data cast Iterable data return _gather item collected for item in data else future Future data data collected append future return future\\n',\n",
              " 'def _finalize_futures needs_crumble extended_registry while needs_crumble fetches_by_class defaultdict list for future in needs_crumble fetches_by_class future data __class__ append future needs_crumble for cls futures in fetches_by_class iteritems crumbler get_crumbler futures 0 data extended_registry if crumbler is None for future in futures future final future data continue extra_attrs crumbler get_extra_attrs_from_db future data for future in futures for future in futures item future data crumbled crumbler crumble item extra_attrs get item future final _gather crumbled needs_crumble\\n',\n",
              " 'def _expand data if isinstance data _PASSTHROUGH return data elif isinstance data dict for k v in data iteritems if isinstance k _PASSTHROUGH data k _expand v else keys _expand k for k in data iterkeys values _expand v for v in data itervalues return dict izip keys values return data elif isinstance data list return _expand item for item in data elif isinstance data Future return _expand data final else return data\\n',\n",
              " 'def serialize data extended_registry None needs_crumble initial _gather data needs_crumble if not needs_crumble return initial _finalize_futures needs_crumble extended_registry return _expand initial\\n',\n",
              " 'def get_extra_attrs_from_db self item_set return\\n',\n",
              " 'def crumble self item attrs return\\n',\n",
              " 'def post self snapshot_id snapshot Snapshot query get snapshot_id if snapshot is None return 404 gc cache_snapshot snapshot response gc get_relevant_snapshot_images snapshot id return self respond self unpack_snapshot_ids response\\n',\n",
              " 'def get_mesos_master zkClient KazooClient hosts current_app config ZOOKEEPER_HOSTS read_only True zkClient start LEADER_PREFIX current_app config ZOOKEEPER_MESOS_MASTER_PATH all_mesos_masters child for child in zkClient get_children LEADER_PREFIX if child startswith json info_ if len all_mesos_masters 0 return None leader_zk_item sorted all_mesos_masters 0 leader_json _ zkClient get LEADER_PREFIX leader_zk_item leader json loads leader_json zkClient stop return leader address ip leader address port\\n',\n",
              " 'def _get_slaves master state _master_api_request master STATE_ENDPOINT json return state slaves\\n',\n",
              " 'def _load_maintenance_schedule master return _master_api_request master SCHEDULE_ENDPOINT json\\n',\n",
              " 'def _update_maintenance_schedule master maint_data _master_api_request master SCHEDULE_ENDPOINT post_data json dumps maint_data\\n',\n",
              " 'def is_active_slave master node slaves _get_slaves master for slave in slaves if slave hostname node return slave active return False\\n',\n",
              " 'def get self user get_current_user if user is None context authenticated False else context authenticated True user user return self respond context\\n',\n",
              " 'def post self args json loads request data try jobstep_ids args jobstep_ids except KeyError return error Missing jobstep_ids attribute for id in jobstep_ids try UUID id except ValueError err Invalid jobstep id sent to jobstep_needs_abort s logging warning err id exc_info True return error err id if len jobstep_ids 0 return self respond needs_abort with statsreporter stats timer jobstep_needs_abort finished db session query JobStep id JobStep result JobStep data filter JobStep status Status finished JobStep id in_ jobstep_ids all needs_abort for step_id result data in finished if result Result aborted or data get timed_out needs_abort append step_id return self respond needs_abort needs_abort\\n',\n",
              " 'def collect_bazel_targets collect_targets_executable bazel_targets bazel_exclude_tags bazel_test_flags max_jobs skip_list_patterns return COLLECT_BAZEL_TARGETS format bazel_apt_pkgs join current_app config BAZEL_APT_PKGS bazel_root current_app config BAZEL_ROOT_PATH bazel_targets join target patterns format t for t in bazel_targets collect_targets_executable collect_targets_executable bazel_exclude_tags join exclude tags format t for t in bazel_exclude_tags bazel_test_flags join test flags format tf for tf in bazel_test_flags max_jobs max_jobs skip_list_patterns join selective testing skip list format p for p in skip_list_patterns\\n',\n",
              " 'def _get_target_name self artifact assert artifact name startswith ARTIFACTSTORE_PREFIX path artifact name len ARTIFACTSTORE_PREFIX dirname os path dirname path dirname target os path split dirname target dirname target return target lstrip\\n',\n",
              " 'def _report_snapshot_downgrade project statsreporter stats incr downgrade logging warning Snapshot downgrade for project s project slug\\n',\n",
              " 'def get_release_id source vcs if source repository backend RepositoryBackend hg return vcs run log r s source revision_sha limit 1 template rev node short elif source repository backend RepositoryBackend git counter vcs run rev list source revision_sha count strip return s s counter source revision_sha else return d s time source revision_sha\\n',\n",
              " 'lock def revision_result_updated_handler revision_result_id kwargs revision_result RevisionResult query get revision_result_id if revision_result is None return build revision_result build if build is None return if revision_result result Result passed return source build source vcs source repository get_vcs if vcs is None logger info Repository has no VCS set s source repository id return if vcs exists try vcs update except ConcurrentUpdateError vcs update else vcs clone _set_latest_green_build_for_each_branch build source vcs url current_app config get GREEN_BUILD_URL if not url logger info GREEN_BUILD_URL not set return auth current_app config GREEN_BUILD_AUTH if not auth logger info GREEN_BUILD_AUTH not set return options get_options build project_id if options get green build notify 1 1 logger info green build notify disabled for project s build project_id return branch_names filter bool options get build branch names split if not source revision should_build_branch branch_names return release_id get_release_id source vcs project options get green build project or build project slug committed_timestamp_sec calendar timegm source revision date_committed utctimetuple logging info Making green_build request to s url try requests post url auth auth timeout 10 data project project id release_id build_url build_web_uri projects 0 builds 1 format build project slug build id hex build_server changes author_name source revision author name author_email source revision author email commit_timestamp committed_timestamp_sec revision_message source revision message raise_for_status except HTTPError as ex if ex response is not None and ex response status_code 409 logger warning Conflict when reporting green build extra data project project release_id release_id build_id build id hex else logger exception Failed to report green build status fail except Exception logger exception Failed to report green build status fail else status success create_or_update Event where type EventType green_build item_id build id values data status status date_modified datetime utcnow\\n',\n",
              " 'def generate_diff self diff None if self patch diff self patch diff else vcs self repository get_vcs if vcs try diff vcs export self revision_sha except Exception logger exception Error getting diff from VCS for source id s self id hex if isinstance diff bytes try diff diff decode utf 8 except UnicodeDecodeError logger exception Error parsing unicode for source id s self id hex diff None return diff\\n',\n",
              " 'def _record_duplicate_testcase duplicate original TestCase query filter_by job_id duplicate job_id name_sha duplicate name_sha with_for_update first prefix _DUPLICATE_TEST_COMPLAINT if original message is None or not original message startswith prefix original message n format prefix original step label original result Result failed if duplicate step label not in original message original message n format duplicate step label return original\\n',\n",
              " 'def clear self TestCase query filter TestCase step_id self step id delete synchronize_session False\\n',\n",
              " 'def register_linter node_type None path_pattern None enforce_pattern None exclude_path_pattern None def foo linter all_linters append Linter node_type path_pattern exclude_path_pattern linter enforce_pattern return linter return foo\\n',\n",
              " 'def check_repos repo_list list Repository query filter Repository backend RepositoryBackend unknown for repo in repo_list sync_repo delay_if_needed task_id repo id hex repo_id repo id hex\\n',\n",
              " 'def post self diff_id diff self _get_diff_by_id diff_id if not diff return error Diff with ID s does not exist diff_id diff_parser DiffParser diff source patch diff files_changed diff_parser get_changed_files try projects self _get_projects_for_diff diff files_changed except InvalidDiffError return error Patch does not apply except ProjectConfigError return error Project config is not in a valid format collection_id uuid uuid4 builds self _get_builds_for_diff diff new_builds for project in projects builds_for_project x for x in builds if x project_id project id if not builds_for_project logging warning Project with id s does not have a build project id continue build max builds_for_project key lambda x x number if build status is not Status finished continue if build result is Result passed continue new_build create_build project project collection_id collection_id label build label target build target message build message author build author source diff source cause Cause retry selective_testing_policy build selective_testing_policy new_builds append new_build return self respond new_builds\\n',\n",
              " 'classmethod def get_target_stats cls project_slug response api_client get projects project format project project_slug last_build response lastPassingBuild if not last_build last_build response lastBuild if not last_build return 0 job_list db session query Job id filter Job build_id last_build id if job_list target_durations dict db session query BazelTarget name BazelTarget duration filter BazelTarget job_id in_ job_list BazelTarget duration is_ None else target_durations dict total_duration sum target_durations itervalues if total_duration 0 avg_test_time int total_duration len target_durations else avg_test_time 0 return target_durations avg_test_time\\n',\n",
              " 'def test_identity self passthrough None 44 2 string 1 2 3 one 2 3 x 11 y 22 x yes no y k 1 2 3 for val in passthrough assert serialize val val\\n',\n",
              " 'def _setup_crumblers self get_crumbler foo_crumbler mock Mock spec Crumbler foo_crumbler crumble side_effect lambda item attrs item inner foo_crumbler get_extra_attrs_from_db return_value bar_crumbler mock Mock spec Crumbler bar_crumbler crumble side_effect lambda item attrs item inner bar_crumbler get_extra_attrs_from_db return_value crumbler_mapping SerializeTest _Foo foo_crumbler SerializeTest _Bar bar_crumbler get_crumbler side_effect lambda item registry crumbler_mapping item __class__ return foo_crumbler bar_crumbler\\n',\n",
              " 'def _assert_crumble_called_for self crumbler items any_order False attrs None crumbler crumble assert_has_calls mock call item attrs for item in items any_order any_order assert crumbler crumble call_count len items\\n',\n",
              " 'tracked_task on_abort abort_job def sync_job job_id job Job query get job_id if not job return if job status Status finished return jobplan implementation JobPlan get_build_step_for_job job_id job id try implementation update job job except UnrecoverableException job status Status finished job result Result infra_failed current_app logger exception Unrecoverable exception syncing s job id all_phases list job phases sync_job_phases job all_phases implementation is_finished sync_job verify_all_children Status finished if any p status Status finished for p in all_phases is_finished False job date_started safe_agg min j date_started for j in all_phases if j date_started if is_finished job date_finished safe_agg max j date_finished for j in all_phases if j date_finished else job date_finished None if job date_started and job date_finished job duration int job date_finished job date_started total_seconds 1000 else job duration None if any j result is Result failed for j in all_phases job result Result failed elif TestCase query join JobStep JobStep id TestCase step_id filter TestCase result Result failed TestCase job_id job id JobStep result Result infra_failed first job result Result failed elif is_finished implementation validate job job else job result Result unknown if is_finished job status Status finished else new_status aggregate_status j status for j in all_phases if new_status Status finished job status new_status elif job status Status finished job status Status in_progress current_app logger exception Job incorrectly marked as finished s job id if db session is_modified job job date_modified datetime utcnow db session add job db session commit if not is_finished raise sync_job NotFinished try aggregate_job_stat job test_count aggregate_job_stat job test_duration aggregate_job_stat job test_failures aggregate_job_stat job test_rerun_count aggregate_job_stat job tests_missing aggregate_job_stat job lines_covered aggregate_job_stat job lines_uncovered aggregate_job_stat job diff_lines_covered aggregate_job_stat job diff_lines_uncovered except Exception current_app logger exception Failing recording aggregate stats for job s job id fire_signal delay signal job finished kwargs job_id job id hex if jobplan queue delay update_project_plan_stats kwargs project_id job project_id hex plan_id jobplan plan_id hex countdown 1\\n',\n",
              " 'def contains_autogenerated_plan build contains db session query literal True filter ItemOption query join JobPlan JobPlan plan_id ItemOption item_id filter ItemOption name bazel autogenerate ItemOption value 1 JobPlan build_id build id exists scalar return bool contains\\n',\n",
              " 'def setUp self self mock_datetime datetime datetime utcnow super TestSnapshotGCTestCase self setUp\\n',\n",
              " 'mock patch changes lib snapshot_garbage_collection get_plans_for_cluster def test_cached_snapshot_images_gets_plans self get_plans project self create_project plan self create_plan project snapshot self create_snapshot project snapshot_image self create_snapshot_image snapshot plan cached_snapshot_image self create_cached_snapshot_image snapshot_image get_plans return_value set gc get_cached_snapshot_images cluster get_plans assert_called_with cluster\\n',\n",
              " 'mock patch changes lib snapshot_garbage_collection get_current_datetime mock patch changes lib snapshot_garbage_collection get_plans_for_cluster def test_cached_snapshot_images_get_current_datetime self get_plans get_current_datetime project self create_project plan self create_plan project snapshot self create_snapshot project snapshot_image self create_snapshot_image snapshot plan cached_snapshot_image self create_cached_snapshot_image snapshot_image get_plans return_value set get_current_datetime return_value self mock_datetime gc get_cached_snapshot_images cluster get_current_datetime assert_any_call\\n',\n",
              " 'mock patch changes lib snapshot_garbage_collection get_current_datetime mock patch changes lib snapshot_garbage_collection get_plans_for_cluster def test_cached_snapshot_images_all self get_plans get_current_datetime project self create_project plan1 self create_plan project plan2 self create_plan project plan3 self create_plan project plan4 self create_plan project get_current_datetime return_value self mock_datetime get_plans return_value plan1 plan2 plan3 snapshot self create_snapshot project snapshot_image1 self create_snapshot_image snapshot plan1 snapshot_image2 self create_snapshot_image snapshot plan2 snapshot_image3 self create_snapshot_image snapshot plan3 snapshot_image4 self create_snapshot_image snapshot plan4 self create_cached_snapshot_image snapshot_image1 expiration_date None self create_cached_snapshot_image snapshot_image2 expiration_date self mock_datetime datetime timedelta 0 1 self create_cached_snapshot_image snapshot_image3 expiration_date self mock_datetime datetime timedelta 0 1 self create_cached_snapshot_image snapshot_image4 expiration_date None cached_snapshot_ids s id for s in gc get_cached_snapshot_images cluster assert len cached_snapshot_ids 2 assert snapshot_image1 id in cached_snapshot_ids assert snapshot_image2 id in cached_snapshot_ids\\n',\n",
              " 'mock patch changes lib snapshot_garbage_collection get_current_datetime def test_recache_existing_cached_snapshot self get_current_datetime project self create_project plan self create_plan project snapshot self create_snapshot project snapshot_image self create_snapshot_image snapshot plan get_current_datetime return_value self mock_datetime self create_cached_snapshot_image snapshot_image expiration_date self mock_datetime datetime timedelta 0 1 gc cache_snapshot snapshot cached_snapshot_image CachedSnapshotImage query get snapshot_image id assert cached_snapshot_image is not None assert cached_snapshot_image expiration_date is None assert CachedSnapshotImage query count 1\\n',\n",
              " 'def test_simple self project self create_project plan self create_plan project self create_step plan data cluster cluster snapshot self create_snapshot project snapshot_image self create_snapshot_image snapshot plan resp self client post self get_endpoint_path snapshot id assert resp status_code 200 data self unserialize resp assert data cluster snapshot_image id hex\\n',\n",
              " 'mock patch changes lib snapshot_garbage_collection get_current_datetime def test_expire self get_current_datetime project self create_project plan self create_plan project self create_step plan data cluster cluster old_snapshot self create_snapshot project old_snapshot_image self create_snapshot_image old_snapshot plan self create_cached_snapshot_image old_snapshot_image snapshot self create_snapshot project snapshot_image self create_snapshot_image snapshot plan get_current_datetime return_value self mock_datetime resp self client post self get_endpoint_path snapshot id assert resp status_code 200 data self unserialize resp assert len data 1 assert cluster in data assert len data cluster 2 assert snapshot_image id hex in data cluster assert old_snapshot_image id hex in data cluster\\n',\n",
              " 'mock patch changes lib snapshot_garbage_collection get_current_datetime def test_multiproject self get_current_datetime project1 self create_project project2 self create_project plan1_1 self create_plan project1 plan1_2 self create_plan project1 plan1_3 self create_plan project1 plan2_1 self create_plan project2 plan2_2 self create_plan project2 self create_step plan1_1 data cluster cluster1 self create_step plan1_2 data cluster cluster2 self create_step plan2_1 data cluster cluster2 self create_step plan2_2 data cluster cluster3 snapshot1 self create_snapshot project1 snapshot2 self create_snapshot project2 snapshot_image1_1 self create_snapshot_image snapshot1 plan1_1 snapshot_image1_2 self create_snapshot_image snapshot1 plan1_2 snapshot_image1_3 self create_snapshot_image snapshot1 plan1_3 snapshot_image2_1 self create_snapshot_image snapshot2 plan2_1 snapshot_image2_2 self create_snapshot_image snapshot2 plan2_2 self create_cached_snapshot_image snapshot_image2_1 self create_cached_snapshot_image snapshot_image2_2 get_current_datetime return_value self mock_datetime resp self client post self get_endpoint_path snapshot1 id assert resp status_code 200 data self unserialize resp assert len data 2 assert cluster1 in data assert cluster2 in data assert len data cluster1 1 assert snapshot_image1_1 id hex in data cluster1 assert len data cluster2 2 assert snapshot_image1_2 id hex in data cluster2 assert snapshot_image2_1 id hex in data cluster2\\n',\n",
              " 'def get self args self get_parser parse_args buildstep_for_job_id None default count 0 created None jobstep_id None if args check_resources buildstep_for_job_id default update cpus 0 mem 0 def process_row agg jobstep status jobstep status name current agg get status or default copy current count 1 if args check_resources if jobstep job_id not in buildstep_for_job_id buildstep_for_job_id jobstep job_id JobPlan get_build_step_for_job jobstep job_id 1 buildstep buildstep_for_job_id jobstep job_id limits buildstep get_resource_limits req_cpus limits get cpus DEFAULT_CPUS req_mem limits get memory DEFAULT_MEMORY_MB current cpus req_cpus current mem req_mem if current created is None or jobstep date_created current created current created jobstep date_created current jobstep_id jobstep id agg status current jobsteps JobStep query filter JobStep status in_ args status by_cluster by_project defaultdict dict defaultdict dict by_global for jobstep in jobsteps process_row by_cluster jobstep cluster jobstep process_row by_project jobstep project slug jobstep process_row by_global jobstep output jobsteps by_cluster by_cluster by_project by_project global by_global return self respond output\\n',\n",
              " 'def get_coverage self fp try parser etree XMLParser target CoverageParser self return etree parse fp parser except etree XMLSyntaxError self logger warn Failed to parse coverage exc_info True return\\n',\n",
              " 'def get self build_id build Build query get build_id if build is None return self respond status_code 404 tags build tags if build tags else return self respond tags tags\\n',\n",
              " 'def post self build_id args self post_parser parse_args if args tags and not all len tag 16 for tag in args tags return error Tags must be 16 characters or less build Build query get build_id if build is None return self respond status_code 404 build tags args tags db session add build db session commit return self respond\\n',\n",
              " 'staticmethod def _get_last_testcase project_id test_name_sha most_recent_test TestCase query join TestCase job join Job build join Build source filter TestCase project_id project_id TestCase name_sha test_name_sha Source patch_id is_ None Source revision_sha isnot None order_by TestCase date_created desc first if not most_recent_test most_recent_test TestCase query filter TestCase project_id project_id TestCase name_sha test_name_sha order_by TestCase date_created desc first return most_recent_test\\n',\n",
              " 'def get self def aggregate task agg if agg count is None agg count 0 agg count 1 if agg max_retries is None or task num_retries agg max_retries agg max_retries task num_retries agg max_retries_id task id if agg oldest_created_time is None or task date_created agg oldest_created_time agg oldest_created_time task date_created agg oldest_created_id task id if agg oldest_modified_time is None or task date_modified agg oldest_modified_time agg oldest_modified_time task date_modified agg oldest_modified_id task id tasks Task query filter Task status Status finished stats defaultdict lambda defaultdict lambda defaultdict lambda None for task in tasks for name in task task_name all for status in task status name all aggregate task stats name status return self respond stats\\n',\n",
              " 'def chunked iterator chunk_size result for chunk in iterator result chunk while len result chunk_size newline_pos result rfind n 0 chunk_size if newline_pos 1 newline_pos chunk_size else newline_pos 1 yield result newline_pos result result newline_pos if result yield result\\n',\n",
              " 'def break_long_lines text args kwargs result for line in text split n result append textwrap fill line args kwargs return n join result\\n',\n",
              " 'def is_infrastructural self return self name infralog\\n',\n",
              " 'def get_url self job self job build job build project build project return projects builds jobs logs format project slug build id hex job id hex self id hex\\n',\n",
              " 'def _pretty_json_dump d return json dumps d sort_keys True indent 3\\n',\n",
              " 'def get_collection_context builds def sort_builds builds_context result_priority_order Result passed Result skipped Result unknown Result aborted Result infra_failed Result failed return sorted builds_context key lambda build result_priority_order index build build result build failing_tests_count build failing_logs_count reverse True builds_context sort_builds map _get_build_context builds result Result unknown if all map lambda build build is_passing builds_context result Result passed elif any imap lambda build build is_failing builds_context result Result failed build builds 0 target target_uri _get_build_target build date_created min _build date_created for _build in builds return CollectionContext title _get_title target build label result builds builds_context result result target_uri target_uri target target label build label date_created date_created author build author commit_message build message or failing_tests_count _aggregate_count builds_context failing_tests_count\\n',\n",
              " 'def _get_build_target build source_data build source data or phabricator_rev_id source_data get phabricator revisionID phabricator_uri source_data get phabricator revisionURL if phabricator_rev_id and phabricator_uri target D format phabricator_rev_id target_uri phabricator_uri else target None target_uri build_web_uri _get_source_uri build build source return target target_uri\\n',\n",
              " 'def clean_project_tests project from_date chunk_size num_days None if chunk_size timedelta minutes 0 logger warning The minutes worth of tests to delete is s but it must be positive chunk_size return 0 test_retention_days num_days or float ProjectOptionsHelper get_option project history test retention days or DEFAULT_TEST_RETENTION_DAYS if test_retention_days MINIMUM_TEST_RETENTION_DAYS logger warning Test retention days for project s is d which is less than the minimum of d Not cleaning tests for this project project slug test_retention_days MINIMUM_TEST_RETENTION_DAYS return 0 test_delete_date from_date timedelta days test_retention_days test_delete_date_limit test_delete_date chunk_size rows_deleted db session query TestCase filter TestCase project_id project id TestCase date_created test_delete_date TestCase date_created test_delete_date_limit delete db session commit statsreporter stats incr count_tests_deleted rows_deleted return rows_deleted\\n',\n",
              " 'def post self project_id project Project get project_id if not project return 404 args self post_parser parse_args repository project repository try revision identify_revision repository args sha except MissingRevision return error Unable to find a matching revision if revision sha revision sha else sha args sha plan_list get_snapshottable_plans project if not plan_list return error No snapshottable plans associated with project source _ get_or_create Source where repository repository revision_sha sha patch_id None build Build source_id source id source source project_id project id project project label Create Snapshot status Status queued cause Cause snapshot target sha 12 tags snapshot priority BuildPriority high db session add build snapshot Snapshot project_id project id source_id source id build_id build id status SnapshotStatus pending db session add snapshot jobs for plan in plan_list job Job build build build_id build id project project project_id project id source build source source_id build source_id status build status label Create Snapshot s plan label db session add job jobplan JobPlan build_jobplan plan job db session add jobplan image SnapshotImage job job snapshot snapshot plan plan db session add image jobs append job db session commit for job in jobs create_job delay job_id job id hex task_id job id hex parent_task_id job build_id hex db session commit sync_build delay build_id build id hex task_id build id hex return self respond snapshot\\n',\n",
              " 'def assertRevision self revision author None message None subject None branches None if author self assertEquals author revision author if message self assertIn message revision message if subject self assertIn subject revision subject if branches self assertListEqual branches revision branches\\n',\n",
              " 'def get self job_id source_id source LogSource query get source_id if source is None or source job_id job_id return 404 offset int request args get offset 1 limit int request args get limit 1 raw request args get raw if raw and limit 1 limit 0 elif limit 1 limit LOG_BATCH_SIZE queryset LogChunk query filter LogChunk source_id source id order_by LogChunk offset desc if offset 1 tail queryset limit 1 first if tail is None logchunks else if limit queryset queryset filter LogChunk offset LogChunk size max tail offset tail size limit 0 logchunks list queryset else queryset queryset filter LogChunk offset offset if limit queryset queryset filter LogChunk offset offset limit logchunks list queryset logchunks sort key lambda x x date_created if logchunks next_offset logchunks 1 offset logchunks 1 size 1 else next_offset offset if raw return Response join l text for l in logchunks mimetype text plain context self serialize source source chunks logchunks nextOffset next_offset context source step self serialize source step if source step context source step phase self serialize source step phase return self respond context serialize False\\n',\n",
              " 'def shard objects max_shards object_stats avg_time normalize_object_name cast Callable str Normalized lambda x x def get_object_duration test_name normalized normalize_object_name test_name result object_stats get normalized if result is None if object_stats current_app logger info No existing duration found for test r test_name result avg_time return result num_shards min len objects max_shards groups 0 for _ in range num_shards heapq heapify groups weighted_tests get_object_duration t t for t in objects for weight test in sorted weighted_tests reverse True group_weight group_tests heapq heappop groups group_weight 1 weight group_tests append test heapq heappush groups group_weight group_tests return groups\\n',\n",
              " 'def filter_recipients email_list domain_whitelist None if domain_whitelist is None domain_whitelist current_app config MAIL_DOMAIN_WHITELIST if not domain_whitelist return email_list return e for e in email_list if parseaddr e 1 split 1 1 in domain_whitelist\\n',\n",
              " 'def get_collection_recipients self collection_context recipient_lists map lambda build_context self get_build_recipients build_context build collection_context builds return list set r for rs in recipient_lists for r in rs\\n',\n",
              " 'def get_build_recipients self build if build result Result passed return recipients options self get_build_options build if options mail notify author author build author if author recipients append s s author name author email recipients extend options mail notify addresses if build_type is_initial_commit_build build recipients extend options mail notify addresses revisions return recipients\\n',\n",
              " 'def get_build_options self build default_options mail notify author 1 mail notify addresses mail notify addresses revisions build_options dict default_options dict db session query ProjectOption name ProjectOption value filter ProjectOption project_id build project_id ProjectOption name in_ default_options keys jobs_options for job in list Job query filter Job build_id build id if job result Result passed jobs_options append dict build_options self get_job_options job all_options jobs_options or build_options merged_options mail notify author any imap lambda options options get mail notify author 1 all_options mail notify addresses set mail notify addresses revisions set recipient_keys mail notify addresses mail notify addresses revisions for options in all_options for key in recipient_keys merged_options key set x strip for x in options key split if x strip return merged_options\\n',\n",
              " 'def _post_analytics_data url table data try resp requests post url params source table headers Content Type application json data json dumps data timeout 10 resp raise_for_status except Exception logging exception Failed to post project data to Analytics\\n',\n",
              " 'def post self args self parser parse_args master args master_url remove args remove warning if remove if 0 redis srem MASTER_BLACKLIST_KEY master warning The master was already not on the blacklist elif 0 redis sadd MASTER_BLACKLIST_KEY master warning The master was already on the blacklist response dict blacklist list redis smembers MASTER_BLACKLIST_KEY response blacklist blacklist if warning response warning warning return self respond response serialize True\\n',\n",
              " 'def get self response dict blacklist list redis smembers MASTER_BLACKLIST_KEY response blacklist blacklist return self respond response serialize True\\n',\n",
              " 'def list_buckets self return Bucket b for b in self _simple_retry_request get buckets json\\n',\n",
              " 'def create_bucket self bucket_name owner changes return Bucket self _simple_retry_request post buckets data json dumps id bucket_name owner owner json\\n',\n",
              " 'def close_bucket self bucket_name return Bucket self _simple_retry_request post buckets s close bucket_name json\\n',\n",
              " 'def list_artifacts self bucket_name return Artifact a for a in self _simple_retry_request get buckets s artifacts bucket_name json\\n',\n",
              " 'def create_chunked_artifact self bucket_name artifact_name return Artifact self _simple_retry_request post buckets s artifacts bucket_name data json dumps name artifact_name chunked True json\\n',\n",
              " 'def post_artifact_chunk self bucket_name artifact_name offset chunk return Artifact self _simple_retry_request post buckets s artifacts s bucket_name artifact_name data json dumps size len chunk byteoffset offset bytes b64encode chunk randomize_sleep False json\\n',\n",
              " 'def close_chunked_artifact self bucket_name artifact_name self _simple_retry_request post buckets s artifacts s close bucket_name artifact_name\\n',\n",
              " 'def write_streamed_artifact self bucket_name artifact_name data path None path path or artifact_name if len data 0 self _logger warning Cannot save empty artifact buckets s artifacts s from path s bucket_name artifact_name path raise ValueError Cannot save empty artifact artifact Artifact self _simple_retry_request post buckets s artifacts bucket_name data json dumps name artifact_name chunked False size len data relativePath path json artifact_name artifact name return Artifact self _simple_retry_request post buckets s artifacts s bucket_name artifact_name data data json\\n',\n",
              " 'def get_artifact_content self bucket_name artifact_name offset None limit None headers if offset is not None if limit is not None and limit 1 headers Range bytes d d offset offset limit 1 else headers Range bytes d offset return StringIO self _simple_retry_request get buckets s artifacts s content bucket_name artifact_name headers headers content\\n',\n",
              " 'def get_revision_info use_cache True global _cached_revision_info if use_cache if _cached_revision_info return _cached_revision_info package_dir os path dirname __file__ checkout_dir os path normpath os path join package_dir os pardir path os path join checkout_dir git if os path exists path info _get_git_revision_info checkout_dir if use_cache _cached_revision_info info return info return None\\n',\n",
              " 'def list_buckets self return list ArtifactStoreMock buckets values\\n',\n",
              " 'def create_bucket self bucket_name owner changes if bucket_name in ArtifactStoreMock buckets raise Exception bucket already exists b Bucket id bucket_name owner owner dateCreated datetime now isoformat dateClosed datetime min isoformat state BucketState OPEN ArtifactStoreMock buckets bucket_name b ArtifactStoreMock artifacts bucket_name ArtifactStoreMock artifact_content bucket_name return b\\n',\n",
              " 'def close_bucket self bucket_name b ArtifactStoreMock buckets bucket_name if b state BucketState OPEN for artifact_name in ArtifactStoreMock artifacts bucket_name keys self close_chunked_artifact bucket_name artifact_name b date_closed datetime now b state BucketState CLOSED ArtifactStoreMock buckets bucket_name b return b\\n',\n",
              " 'def list_artifacts self bucket_name return list ArtifactStoreMock artifacts bucket_name values\\n',\n",
              " 'def create_chunked_artifact self bucket_name artifact_name if ArtifactStoreMock buckets bucket_name state BucketState OPEN raise Exception bucket already closed while artifact_name in ArtifactStoreMock artifacts bucket_name artifact_name dup a Artifact name artifact_name relativePath artifact_name size 0 state ArtifactState APPENDING bucketId bucket_name s3URL s s bucket_name artifact_name dateCreated datetime now isoformat deadlineMins 30 ArtifactStoreMock artifacts bucket_name artifact_name a ArtifactStoreMock artifact_content bucket_name artifact_name return a\\n',\n",
              " 'def post_artifact_chunk self bucket_name artifact_name offset chunk if ArtifactStoreMock buckets bucket_name state BucketState OPEN raise Exception bucket already closed a ArtifactStoreMock artifacts bucket_name artifact_name if a state ArtifactState APPENDING raise Exception artifact not open for appending if offset a size raise Exception incorrect offset ArtifactStoreMock artifact_content bucket_name artifact_name chunk a size len chunk ArtifactStoreMock artifacts bucket_name artifact_name a return a\\n',\n",
              " 'def close_chunked_artifact self bucket_name artifact_name if ArtifactStoreMock buckets bucket_name state BucketState OPEN raise Exception bucket already closed a ArtifactStoreMock artifacts bucket_name artifact_name if a state ArtifactState APPENDING a state ArtifactState UPLOADED ArtifactStoreMock artifacts bucket_name artifact_name a\\n',\n",
              " 'def write_streamed_artifact self bucket_name artifact_name data path None if ArtifactStoreMock buckets bucket_name state BucketState OPEN raise Exception bucket already closed path path or artifact_name while artifact_name in ArtifactStoreMock artifacts bucket_name artifact_name dup a Artifact name artifact_name relativePath path size len data state ArtifactState UPLOADED bucketId bucket_name s3URL s s bucket_name artifact_name dateCreated datetime now isoformat deadlineMins 30 ArtifactStoreMock artifacts bucket_name artifact_name a ArtifactStoreMock artifact_content bucket_name artifact_name data return a\\n',\n",
              " 'def get_artifact_content self bucket_name artifact_name offset None limit None start_offset end_offset None None if offset is not None start_offset offset if limit is not None end_offset offset limit return StringIO ArtifactStoreMock artifact_content bucket_name artifact_name start_offset end_offset\\n',\n",
              " 'def get self user get_current_user auth authenticated False if user auth authenticated True user user messages list AdminMessage query options joinedload user admin_message None if messages if len messages 1 logging warning Multiple messages found else admin_message messages 0 return self respond auth auth admin_message admin_message\\n',\n",
              " 'def reconstruct_file_diff self file_info def no_newline_marker line if line ends_with_newline return else return n No newline at end of file action_character_dict add del unmod if not file_info chunks return chunk_strings for chunk chunk_marker in zip file_info chunks file_info chunk_markers lines action_character_dict l action l line no_newline_marker l for l in chunk chunk_strings append chunk_marker n lines format chunk_marker chunk_marker lines n join lines diff n old_filename n new_filename n chunks n format old_filename file_info old_filename if file_info old_filename is not None else dev null new_filename file_info new_filename if file_info new_filename is not None else dev null chunks n join chunk_strings return diff\\n',\n",
              " 'def get_changed_files self results set for info in self parse if info new_filename results add info new_filename 2 if info old_filename results add info old_filename 2 return results\\n',\n",
              " 'def get_lines_by_file self lines_by_file defaultdict set for file_diff in self parse for diff_chunk in file_diff chunks if not file_diff new_filename continue lines_by_file file_diff new_filename 2 update d new_lineno for d in diff_chunk if d action add return lines_by_file\\n',\n",
              " 'def __init__ self master_urls None setup_script teardown_script artifacts reset_script path workspace snapshot_script None clean True cluster None args kwargs self setup_script setup_script self script kwargs pop script self teardown_script teardown_script self snapshot_script snapshot_script self reset_script reset_script self path path self workspace workspace self artifacts artifacts self clean clean self build_type kwargs pop build_type current_app config CHANGES_CLIENT_DEFAULT_BUILD_TYPE if self build_type is None self build_type current_app config CHANGES_CLIENT_DEFAULT_BUILD_TYPE self artifact_server_base_url current_app config ARTIFACTS_SERVER or self build_desc self load_build_desc self build_type super JenkinsGenericBuilder self __init__ master_urls args cluster cluster kwargs\\n',\n",
              " 'def get_expected_image self job_id return db session query SnapshotImage id filter SnapshotImage job_id job_id scalar\\n',\n",
              " 'def get_lxc_config self jobstep build_desc self _get_build_desc jobstep if build_desc get uses_client and build_desc get adapter lxc app_cfg current_app config snapshot_bucket app_cfg get SNAPSHOT_S3_BUCKET default_pre self debug_config get prelaunch_script or app_cfg get LXC_PRE_LAUNCH default_post app_cfg get LXC_POST_LAUNCH default_release app_cfg get LXC_RELEASE trusty return LXCConfig s3_bucket snapshot_bucket compression lz4 prelaunch build_desc get pre launch default_pre postlaunch build_desc get post launch default_post release build_desc get release default_release template None mirror None security_mirror None return None\\n',\n",
              " 'def get_job_parameters self job changes_bid setup_script None script None teardown_script None path None params super JenkinsGenericBuilder self get_job_parameters job changes_bid changes_bid if path is None path self path if setup_script is None setup_script self setup_script if script is None script self script if teardown_script is None teardown_script self teardown_script project job project repository project repository vcs repository get_vcs if vcs repo_url vcs remote_url else repo_url repository url snapshot_bucket current_app config get SNAPSHOT_S3_BUCKET default_pre self debug_config get prelaunch_script or current_app config get LXC_PRE_LAUNCH default_post current_app config get LXC_POST_LAUNCH default_release current_app config get LXC_RELEASE trusty build_desc self build_desc expected_image self get_expected_image job id snapshot_id if expected_image snapshot_id expected_image hex script self snapshot_script or build_desc self get_snapshot_build_desc setup_script self get_snapshot_setup_script teardown_script self get_snapshot_teardown_script params update CHANGES_PID project slug PROJECT_CONFIG project get_config_path REPO_URL repo_url SETUP_SCRIPT setup_script SCRIPT script TEARDOWN_SCRIPT teardown_script RESET_SCRIPT self reset_script REPO_VCS repository backend name WORK_PATH path C_WORKSPACE self workspace ARTIFACTS_SERVER_BASE_URL self artifact_server_base_url if bind_mounts in self debug_config params bind mounts self debug_config bind_mounts if build_desc get uses_client False params update JENKINS_COMMAND build_desc jenkins command CHANGES_CLIENT_ADAPTER build_desc adapter CHANGES_CLIENT_SERVER build_internal_uri api 0 CHANGES_CLIENT_SNAPSHOT_BUCKET snapshot_bucket CHANGES_CLIENT_SNAPSHOT_ID snapshot_id CHANGES_CLIENT_LXC_PRE_LAUNCH build_desc get pre launch default_pre CHANGES_CLIENT_LXC_POST_LAUNCH build_desc get post launch default_post CHANGES_CLIENT_LXC_RELEASE build_desc get release default_release return params\\n',\n",
              " 'def get_future_commands self env commands artifacts return map lambda command FutureCommand command script artifacts artifacts env env commands\\n',\n",
              " 'def create_commands self jobstep env commands self build_desc get commands artifacts self artifacts_for_jobstep jobstep env env copy if not self clean env SKIP_GIT_CLEAN 1 index 0 for future_command in self get_future_commands env commands artifacts db session add future_command as_command jobstep index index 1\\n',\n",
              " 'def can_snapshot self return self build_desc get can_snapshot False\\n',\n",
              " 'def artifacts_for_jobstep self jobstep return self artifacts\\n',\n",
              " 'def get_resource_limits self return\\n',\n",
              " 'def get_lxc_config self jobstep return None\\n',\n",
              " 'def get_test_stats_from self return None\\n',\n",
              " 'def execute self job raise NotImplementedError\\n',\n",
              " 'def create_replacement_jobstep self step return None\\n',\n",
              " 'def validate self job job result aggregate_result p result for p in job phases\\n',\n",
              " 'def validate_phase self phase phase result aggregate_result s result for s in phase current_steps\\n',\n",
              " 'def get_artifact_manager self jobstep raise NotImplementedError\\n',\n",
              " 'def prefer_artifactstore self return False\\n',\n",
              " 'def verify_final_artifacts self jobstep artifacts\\n',\n",
              " 'staticmethod def handle_debug_infra_failures jobstep debug_config phase_type infra_failures debug_config get infra_failures if phase_type in infra_failures percent jobstep id int 100 jobstep data debugForceInfraFailure percent infra_failures phase_type 100 db session add jobstep\\n',\n",
              " 'tracked_task def fire_signal signal kwargs for listener l_signal in current_app config EVENT_LISTENERS if l_signal signal run_event_listener delay listener listener signal signal kwargs kwargs\\n',\n",
              " 'tracked_task def run_event_listener listener signal kwargs if not any l listener for l _ in current_app config EVENT_LISTENERS raise SuspiciousOperation s is not a registered event listener listener func import_string listener func kwargs\\n',\n",
              " 'def might_be_diffusion_iden text return DIFFUSION_REGEX match text is not None\\n',\n",
              " 'def get_hash_from_diffusion_iden text match DIFFUSION_REGEX match text if match is None return None return match group 1\\n',\n",
              " 'def post_comment target message request None try if request is None request PhabricatorClient if not request connect return logger info Posting build results to s target revision_id target 1 post_diff_comment revision_id message request except requests exceptions ConnectionError logger exception Failed to post to target s target\\n',\n",
              " 'def post self method_name params if not self connect raise RuntimeError Failed to connect see logs return self call method_name params\\n',\n",
              " 'def try_create model where instance model for key value in where iteritems setattr instance key value try with db session begin_nested db session add instance except IntegrityError return None return instance\\n',\n",
              " 'def _merge_dicts first second return k v for k v in itertools chain second iteritems first iteritems\\n',\n",
              " 'classmethod def can_process cls filepath filepath ArtifactHandler _sanitize_path filepath for pattern in cls FILENAMES if in pattern if pattern startswith pattern pattern 1 if fnmatch filepath pattern return True elif fnmatch os path basename filepath pattern return True return False\\n',\n",
              " 'def process self fp artifact\\n',\n",
              " 'def swallow_exceptions exn_logger def decor func def wrapper args kwargs try return func args kwargs except Exception as e exn_logger exception e return wrapper return decor\\n',\n",
              " 'def stats self if self _stats return self _stats return Stats client None\\n',\n",
              " 'def timer self key Stats _check_key key def wrapper fn wraps fn def inner args kwargs with self stats timer key return fn args kwargs return inner return wrapper\\n',\n",
              " 'def __init__ self client self _client client\\n',\n",
              " 'swallow_exceptions logger def set_gauge self key value assert isinstance value int float long Stats _check_key key if self _client self _client gauge key value\\n',\n",
              " 'swallow_exceptions logger def incr self key delta 1 assert isinstance delta int float long assert delta 0 Stats _check_key key if self _client self _client incr key delta\\n',\n",
              " 'swallow_exceptions logger def log_timing self key duration_ms assert isinstance duration_ms int float long Stats _check_key key if self _client self _client timing key duration_ms\\n',\n",
              " 'contextmanager def timer self key t0 time time try yield finally duration_ms int 1000 time time t0 self log_timing key duration_ms\\n',\n",
              " 'classmethod def _check_key cls key if not cls _KEY_RE match key raise Exception Invalid key format repr key\\n',\n",
              " 'classmethod def get_current cls project_id from changes models project import ProjectOption current_id db session query ProjectOption value filter ProjectOption project_id project_id ProjectOption name snapshot current scalar if not current_id return return Snapshot query get current_id\\n',\n",
              " 'def change_status self status self status status db session add self db session flush inactive_image_query SnapshotImage query filter SnapshotImage status SnapshotStatus active SnapshotImage snapshot_id self snapshot id exists if not db session query inactive_image_query scalar if self snapshot status SnapshotStatus pending self snapshot status SnapshotStatus active elif self snapshot status SnapshotStatus active self snapshot status SnapshotStatus invalidated db session commit\\n',\n",
              " 'classmethod def get cls plan snapshot_id snapshot_plan plan if plan snapshot_plan is not None snapshot_plan plan snapshot_plan snapshot Snapshot query filter Snapshot id snapshot_id scalar if snapshot is not None return SnapshotImage query filter SnapshotImage snapshot_id snapshot id SnapshotImage plan_id snapshot_plan id scalar return None\\n',\n",
              " 'def _update self kwargs assert self task_id count Task query filter Task task_name self task_name Task task_id self task_id Task parent_id self parent_id update kwargs synchronize_session False return bool count\\n',\n",
              " 'def _retry self assert self task_id task Task query filter Task task_name self task_name Task task_id self task_id Task parent_id self parent_id first if task and self max_retries and task num_retries self max_retries date_finished datetime utcnow self _update Task date_finished date_finished Task date_modified date_finished Task status Status finished Task result Result failed db session commit raise TooManyRetries s failed after d retries self task_name task num_retries self _update Task date_modified datetime utcnow Task status Status in_progress Task num_retries Task num_retries 1 db session commit kwargs self kwargs copy kwargs task_id self task_id kwargs parent_task_id self parent_id retry_number db session query Task num_retries filter Task task_name self task_name Task task_id self task_id Task parent_id self parent_id scalar or 0 retry_countdown min BASE_RETRY_COUNTDOWN retry_number 2 300 queue delay self task_name kwargs kwargs countdown retry_countdown\\n',\n",
              " 'def delay_if_needed self kwargs kwargs setdefault task_id uuid4 hex fn_kwargs dict k v for k v in kwargs iteritems if k not in task_id parent_task_id task created get_or_create Task where task_name self task_name task_id kwargs task_id defaults parent_id kwargs get parent_task_id data kwargs fn_kwargs status Status queued if created or self needs_requeued task if not created task date_modified datetime utcnow db session add task db session commit queue delay self task_name kwargs kwargs if created self _report_created\\n',\n",
              " 'def delay self kwargs kwargs setdefault task_id uuid4 hex fn_kwargs dict k v for k v in kwargs iteritems if k not in task_id parent_task_id task created get_or_create Task where task_name self task_name task_id kwargs task_id defaults parent_id kwargs get parent_task_id data kwargs fn_kwargs status Status queued if not created task date_modified datetime utcnow db session add task db session commit if created self _report_created queue delay self task_name kwargs kwargs\\n',\n",
              " 'def _report_created self statsreporter stats incr new_task_created_ self task_name\\n',\n",
              " 'def _report_lag self first_run_time t Task query filter Task task_name self task_name Task task_id self task_id Task parent_id self parent_id first if t and t date_created t date_modified lag_ms first_run_time t date_created total_seconds 1000 statsreporter stats log_timing first_execution_lag_ self task_name lag_ms\\n',\n",
              " 'def is_initial_commit_build build if not build source is_commit return False if build cause Cause snapshot return False if build tags and set build tags intersection _KEY_TAGS commit return False return build tags and commit in build tags\\n',\n",
              " 'def is_any_commit_build build if not build source is_commit return False if build cause Cause snapshot return False if build tags and set build tags intersection _KEY_TAGS commit return False return True\\n',\n",
              " 'def get_any_commit_build_filters non_commit_tags _KEY_TAGS commit return Build cause Cause snapshot Source patch_id None Build tags any tag for tag in non_commit_tags\\n',\n",
              " 'def is_arc_test_build build return build tags and arc test in build tags\\n',\n",
              " 'def is_phabricator_diff_build build target build target return target and target startswith D and not is_arc_test_build build\\n',\n",
              " 'mock patch changes jobs sync_repo fire_signal mock patch changes models repository Repository get_vcs mock patch changes config queue delay def test_with_existing_revision self queue_delay get_vcs_backend mock_fire_signal vcs_backend mock MagicMock spec Vcs vcs_backend get_patch_hash return_value a 40 get_vcs_backend return_value vcs_backend repo self create_repo backend RepositoryBackend git existing_revision_branch_changed RevisionResult id str 3 40 message latest commit author Example foo example com author_date datetime 2013 9 19 22 15 22 branches b existing_revision_branch_changed save repo existing_revision_branch_changed branches b c existing_revision RevisionResult id str 4 40 message latest commit author Example foo example com author_date datetime 2013 9 19 22 15 22 branches b r _ _ existing_revision save repo r date_created_signal datetime utcnow db session commit existing_revision_no_branches RevisionResult id str 5 40 message latest commit author Example foo example com author_date datetime 2013 9 19 22 15 22 branches existing_revision_no_branches save repo db session commit def log parent limit first_parent yield existing_revision yield existing_revision_branch_changed yield existing_revision_no_branches for i in range 3 yield RevisionResult id str i 40 message commit number str i author Example foo example com author_date datetime 2013 9 19 22 15 22 branches b vcs_backend log side_effect log with mock patch object sync_repo allow_absent_from_db True sync_repo repo_id repo id hex task_id repo id hex assert get_vcs_backend called vcs_backend log assert_any_call parent None limit NUM_RECENT_COMMITS first_parent False db session expire_all repo Repository query get repo id assert repo last_update_attempt is not None assert repo last_update is not None for i in range 4 mock_fire_signal delay assert_any_call signal revision created kwargs repository_id repo id hex revision_sha str i 40 assert mock_fire_signal delay call_count 4 mock_fire_signal delay call_count 0 with mock patch object sync_repo allow_absent_from_db True sync_repo repo_id repo id hex task_id repo id hex assert mock_fire_signal delay call_count 0\\n',\n",
              " 'def requires_auth method wraps method def wrapped args kwargs user get_current_user if user is None return 401 return method args kwargs return wrapped\\n',\n",
              " 'def requires_admin method wraps method def wrapped args kwargs user get_current_user if user is None return 401 if not user is_admin return 403 return method args kwargs return wrapped\\n',\n",
              " 'def get_project_slug_from_project_id args kwargs project_id kwargs project_id project Project get project_id if project is None raise ResourceNotFound Project with ID not found format project_id return project slug\\n',\n",
              " 'def get_project_slug_from_plan_id args kwargs plan_id kwargs plan_id plan Plan query get plan_id if plan is None raise ResourceNotFound Plan with ID not found format plan_id return plan project slug\\n',\n",
              " 'def get_project_slug_from_step_id args kwargs step_id kwargs step_id step Step query get step_id if step is None raise ResourceNotFound Step with ID not found format step_id return step plan project slug\\n',\n",
              " 'def user_has_project_permission user project_slug if user is_admin return True if user project_permissions is not None for p in user project_permissions if fnmatch project_slug p return True return False\\n',\n",
              " 'def requires_project_admin get_project_slug def decorator method wraps method def wrapped self args kwargs user get_current_user if user is None return error Not logged in http_code 401 try slug get_project_slug self args kwargs except ResourceNotFound as e return error format e http_code 404 if user_has_project_permission user slug return method self args kwargs return error User does not have access to this project http_code 403 return wrapped return decorator\\n',\n",
              " 'def get_current_user if getattr request current_user NOT_SET is NOT_SET if current_app config get PP_AUTH False email request headers get X PP USER if email is None request gcurrent_user None else request gcurrent_user User query filter_by email email first elif session get uid is None request gcurrent_user None else request gcurrent_user User query get session uid if request gcurrent_user is None del session uid return request gcurrent_user\\n',\n",
              " 'def sync repo vcs repo get_vcs if vcs is None logger warning Repository s has no VCS backend set repo id return False if repo status RepositoryStatus active logger info Repository s is not active repo id return False Repository query filter Repository id repo id update last_update_attempt datetime utcnow synchronize_session False db session commit if vcs exists try vcs update except ConcurrentUpdateError pass else vcs clone if repo backend RepositoryBackend git revisions vcs log parent None limit NUM_RECENT_COMMITS first_parent False else revisions vcs log parent None limit NUM_RECENT_COMMITS for commit in revisions known_revision Revision query filter Revision repository_id repo id Revision sha commit id with_for_update scalar if known_revision and known_revision date_created_signal db session commit continue revision created _ commit save repo db session commit revision Revision query filter Revision repository_id repo id Revision sha commit id with_for_update scalar if revision branches and not revision date_created_signal revision date_created_signal datetime utcnow fire_signal delay signal revision created kwargs repository_id repo id hex revision_sha revision sha db session commit db session commit Repository query filter Repository id repo id update last_update datetime utcnow synchronize_session False db session commit return True\\n',\n",
              " 'def get self messages list AdminMessage query options joinedload user if not messages return self respond None if len messages 1 logging warning Multiple messages found return self respond messages 0\\n',\n",
              " 'requires_admin def post self args self post_parser parse_args message _ create_or_update AdminMessage where values message args message user_id get_current_user id date_created datetime utcnow return self respond message\\n',\n",
              " 'def __init__ self job_name None jenkins_url None jenkins_diff_url None auth_keyname None verify True cluster None debug_config None cpus 4 memory 8 1024 if job_name is None raise ValueError Missing required config need job_name if any int_field and type int_field int for int_field in cpus memory raise ValueError cpus and memory fields must be JSON ints if not isinstance jenkins_url list tuple if jenkins_url jenkins_url jenkins_url else jenkins_url if not isinstance jenkins_diff_url list tuple if jenkins_diff_url jenkins_diff_url jenkins_diff_url else jenkins_diff_url self job_name job_name self jenkins_urls jenkins_url self jenkins_diff_urls jenkins_diff_url self auth_keyname auth_keyname self verify verify self cluster cluster self debug_config debug_config or self _resources if cpus self _resources cpus cpus if memory self _resources memory memory\\n',\n",
              " 'def get_builder self app current_app kwargs args self get_builder_options copy args update kwargs return self builder_cls app app args\\n',\n",
              " 'def get_builder_options self return master_urls self jenkins_urls diff_urls self jenkins_diff_urls job_name self job_name verify self verify auth_keyname self auth_keyname cluster self cluster debug_config self debug_config\\n',\n",
              " 'def execute self job builder self get_builder builder create_job job\\n',\n",
              " 'def fetch_artifact self artifact builder self get_builder builder sync_artifact artifact\\n',\n",
              " 'def can_snapshot self return self get_builder can_snapshot\\n',\n",
              " 'def __init__ self job_name None script None cluster None path workspace reset_script build_type None setup_script teardown_script clean True artifacts XunitHandler FILENAMES CoverageHandler FILENAMES SERVICE_LOG_FILE_PATTERNS snapshot_script None kwargs if None in job_name script cluster raise ValueError Missing required config need job_name script and cluster self setup_script setup_script self script script self teardown_script teardown_script self reset_script reset_script self snapshot_script snapshot_script self path path self workspace workspace self build_type build_type self artifacts artifacts self clean clean super JenkinsGenericBuildStep self __init__ job_name job_name cluster cluster kwargs\\n',\n",
              " 'def get_lxc_config self jobstep return self get_builder get_lxc_config jobstep\\n',\n",
              " 'def Linear source target if target frame source frame raise ValueError Target frame must be greater than source frame source 0 target 1 format source frame target frame fdiff float target frame source frame xtlr target xtl source xtl fdiff ytlr target ytl source ytl fdiff xbrr target xbr source xbr fdiff ybrr target ybr source ybr fdiff results for i in range source frame target frame 1 off i source frame xtl source xtl xtlr off ytl source ytl ytlr off xbr source xbr xbrr off ybr source ybr ybrr off generated int i source frame and i target frame lost source lost or target lost results append Box xtl ytl xbr ybr frame i lost lost occluded source occluded generated generated attributes list source attributes return results\\n',\n",
              " 'def LinearFill path method Linear result for x y in zip path path 1 result extend method x y 1 result append path 1 return result\\n',\n",
              " 'def buildslidingwindows base space skip nextframe base frame w base get_width h base get_height xstart space 0 ystart space 1 xstop space 2 w ystop space 3 h boxes for i in range xstart xstop skip for j in range ystart ystop skip boxes append annotations Box i j i w j h nextframe return boxes\\n',\n",
              " 'def request method parameters if len keys 0 raise RuntimeError No Flickr API keys defined apikey random choice keys parameters urllib urlencode parameters url services rest method 0 format rest api_key 1 2 url url format method apikey parameters conn httplib HTTPConnection api flickr com conn request GET url response conn getresponse read response ElementTree fromstring response conn close return response\\n',\n",
              " 'def search tags perpage 100 page pages 1 2 while page pages photos request flickr photos search tags tags page page per_page perpage extras all_extras photos photos find photos pages int photos get pages page 1 for photo in photos yield Photo fromapi photo attrib\\n',\n",
              " 'def recent perpage 500 photos request flickr photos getRecent per_page perpage extras all_extras for photo in photos getiterator photo yield Photo fromapi photo attrib\\n',\n",
              " 'def pascal tags range 2003 2010 if isinstance tags str tags tags split perpage 10 while True start int time mktime range 0 1 1 0 0 0 0 0 1 stop int time mktime range 1 12 31 23 59 59 0 0 1 rtime time localtime random randint start stop 0 3 0 0 0 0 0 1 rtime time mktime rtime stm etm int rtime int rtime 86400 tag random choice tags r request flickr photos search text tag min_upload_date stm max_upload_date etm per_page perpage page 1 totpages int r find photos get pages if totpages 0 continue page random randint 1 totpages r request flickr photos search text tag min_upload_date stm max_upload_date etm per_page perpage page page extras all_extras photos x attrib for x in r find photos if len photos 0 yield Photo fromapi photos random randint 0 len photos 1\\n',\n",
              " 'def filtersizes iterator size medium sizes small 0 medium 1 large 2 original 3 required sizes size for x in iterator if x format in sizes and sizes x format required yield x\\n',\n",
              " 'def delay iterator wait 1 every 1 for i photo in enumerate iterator if i 0 and i every 0 time sleep wait yield photo\\n',\n",
              " 'def scrape iterator location limit for photo in iterator filepath 0 1 2 jpg format location photo flickrid 100 photo flickrid if os path exists filepath log info Skipping duplicate 0 format photo flickrid continue try log info Downloading 0 1 format photo flickrid photo format image photo download try image save filepath except IOError os makedirs os path dirname filepath image save filepath except KeyboardInterrupt raise except pass limit 1 if limit 0 break\\n',\n",
              " 'def download self data urllib urlopen self remoteurl read s StringIO StringIO data return Image open s\\n',\n",
              " 'classmethod def fromapi cls attrib if url_o in attrib url attrib url_o size attrib width_o attrib height_o format original elif url_l in attrib url attrib url_l size attrib width_l attrib height_l format large elif url_m in attrib url attrib url_m size attrib width_m attrib height_m format medium elif url_s in attrib url attrib url_s size attrib width_s attrib height_s format small else raise RuntimeError Photo does not have URL return Photo None url size format attrib id\\n',\n",
              " 'def which program import os def is_exe fpath return os path isfile fpath and os access fpath os X_OK fpath fname os path split program if fpath if is_exe program return program else for path in os environ PATH split os pathsep path path strip exe_file os path join path program if is_exe exe_file return exe_file return None\\n',\n",
              " 'def read inputdata reading_cameras False reading_points False cameras points camera_state 0 camera_rotation camera_current None point_state 0 point_current None numcameras None numpoints None for line in inputdata if line 0 continue if not reading_cameras and not reading_points numcameras numpoints int x for x in line split reading_cameras True elif reading_cameras data float x for x in line split if camera_state 0 focal radial0 radial1 data camera_current Camera len cameras camera_current focal focal camera_current radialdist radial0 radial1 camera_state 1 elif camera_state 1 camera_rotation append data if len camera_rotation 3 camera_current rotation camera_rotation camera_rotation camera_state 2 elif camera_state 2 camera_current translation data camera_state 0 cameras append camera_current if len cameras numcameras reading_cameras False reading_points True elif reading_points if point_state 0 point_current Point point_current position float x for x in line split point_state 1 elif point_state 1 point_current color int x for x in line split point_state 2 elif point_state 2 data line split views viewcameras int x for x in data 1 4 keys int x for x in data 2 4 xs float x for x in data 3 4 ys float x for x in data 4 4 for camera key x y in zip viewcameras keys xs ys point_current views append PointView cameras camera key x y point_state 0 points append point_current assert point_state 0 assert camera_state 0 assert reading_cameras False assert reading_points True return cameras points\\n',\n",
              " 'def imageset self imageset trainval imageset 0 txt format imageset path os path join self root ImageSets Main imageset for line in open path yield line strip\\n',\n",
              " 'def seed s 0 random seed s\\n',\n",
              " 'def add self toy self toys append toy self frames max self frames toy frames\\n',\n",
              " 'def render self frame if frame self frames raise ValueError Requested frame 0 but there are only 1 format frame self frames canvas Image new RGB self size self background for toy in self toys toy render frame canvas return canvas\\n',\n",
              " 'def write self frame location self render frame save location\\n',\n",
              " 'def export self location format jpg self frames max x frames for x in self toys if self cap 0 self frames min self cap self frames for frame in range self frames self write frame location frame format format location location frame frame format format\\n',\n",
              " 'def __getitem__ self frame return self render frame\\n',\n",
              " 'def linear self position frame chaos 0 if frame self frames raise ValueError Target frame is behind current time index fdiff float frame self frames rx position 0 self lastposition 0 fdiff ry position 1 self lastposition 1 fdiff for i in range self frames 1 frame x self lastposition 0 rx i self frames x random randint chaos chaos y self lastposition 1 ry i self frames y random randint chaos chaos y max y 0 x max x 0 self positions append int x int y self positions append position self frames frame self lastposition position return self\\n',\n",
              " 'def stationary self frame for i in range frame self frames self positions append self lastposition self frames frame return self\\n',\n",
              " 'def disappear self frame reappear True if frame self frames raise ValueError Target frame is behind current time index amount frame self frames if reappear amount 1 self positions extend None amount if reappear self positions append self lastposition self frames frame return self\\n',\n",
              " 'def random self frame estate 720 480 for _ in range frame self frames self positions append random randint 0 estate 0 self size 0 random randint 0 estate 1 self size 1 self frames frame return self\\n',\n",
              " 'def set self position self positions append position self lastposition position self frames 1 return self\\n',\n",
              " 'def render self frame canvas if frame self frames and self positions frame self draw frame canvas\\n',\n",
              " 'def __getitem__ self frame if frame 0 frame len self frame pos self positions frame if not pos return Box 0 0 1 1 frame 1 return Box pos 0 pos 1 pos 0 self size 0 pos 1 self size 1 frame 0\\n',\n",
              " 'def highlight_box image box color colors 0 width defaultwidth font None draw ImageDraw Draw image if not box occluded width width 2 for i in range width draw rectangle box 0 i box 1 i box 2 i box 3 i outline color if font ypos box ytl for attribute in box attributes attribute str attribute size draw textsize attribute font font xpos max box xtl size 0 3 0 draw text xpos ypos 1 attribute fill black font font draw text xpos 1 ypos 1 attribute fill black font font draw text xpos 1 ypos attribute fill black font font draw text xpos ypos 1 attribute fill black font font draw text xpos 1 ypos 1 attribute fill black font font draw text xpos 1 ypos attribute fill black font font draw text xpos ypos attribute fill white font font ypos size 1 3 return image\\n',\n",
              " 'def highlight_boxes image boxes colors colors width defaultwidth font None for box color in zip boxes itertools cycle colors highlight_box image box color width font return image\\n',\n",
              " 'def highlight_path images path color colors 0 width defaultwidth font None logger info Visualize path of length 0 format len path for box in path try lost box lost except lost False image images box frame if not lost highlight_box image box color width font yield image box frame\\n',\n",
              " 'def highlight_paths images paths colors colors width defaultwidth font None logger info Visualize 0 paths format len paths boxmap paths zip paths itertools cycle colors for path color in paths for box in path if box frame not in boxmap boxmap box frame box color else boxmap box frame append box color for frame boxes in sorted boxmap items im images frame for box color in boxes try lost box lost except lost False if not lost highlight_box im box color width font yield im frame\\n',\n",
              " 'def save images output for image frame in images image save output frame\\n',\n",
              " 'def eventually cb args kwargs _theSimpleQueue append cb args kwargs\\n',\n",
              " 'def fireEventually value None d defer Deferred eventually d callback value return d\\n',\n",
              " 'def flushEventualQueue value None d _theSimpleQueue flush d addBoth lambda _ value return d\\n',\n",
              " 'def flush self if not self _events return defer succeed None d defer Deferred self _flushObservers append d return d\\n',\n",
              " 'def to_ascii s_bytes prefix encoding base64 if encoding base64 s_ascii base64 b64encode s_bytes rstrip elif encoding base32 s_ascii base64 b32encode s_bytes rstrip lower elif encoding in base16 hex s_ascii base64 b16encode s_bytes lower else raise NotImplementedError return prefix s_ascii\\n',\n",
              " 'def from_ascii s_ascii prefix encoding base64 s_ascii remove_prefix s_ascii strip prefix if encoding base64 s_ascii 4 len s_ascii 4 4 s_bytes base64 b64decode s_ascii elif encoding base32 s_ascii 8 len s_ascii 8 8 s_bytes base64 b32decode s_ascii upper elif encoding in base16 hex s_bytes base64 b16decode s_ascii upper else raise NotImplementedError return s_bytes\\n',\n",
              " 'def count_up seconds return _count_helper seconds 0 lambda x y x y\\n',\n",
              " 'def count_down seconds return _count_helper seconds seconds lambda x y seconds x y\\n',\n",
              " 'def on_memory_data self args data self memory_data if not data log debug No data showing blank self blackout if data with self canvas if not self memory_data log debug No data showing blank self blackout return byte_array data byte_array seek 0 img CoreImage byte_array ext jpg byte_array flush self texture img texture\\n',\n",
              " 'def emit self record self output self format record\\n',\n",
              " 'def push self action_name image_arr pass\\n',\n",
              " 'def combine self image_paths filename direction if not image_paths raise IndexError image_paths need to have atleast one value opened_imgs list map Image open image_paths widths heights zip img size for img in opened_imgs total_width total_height direction total_size widths heights new_img Image new RGB total_width total_height offsets direction offset 0 0 offset offsets send None for img in opened_imgs new_img paste img offset offset offsets send img size new_img save filename return filename\\n',\n",
              " 'def _read_columns_file f try columns json loads open f r read object_pairs_hook collections OrderedDict except Exception as err raise InvalidColumnsFileError There was an error while reading 0 1 format f err if __options in columns del columns __options return columns\\n',\n",
              " 'def _write_csv f table_ fieldnames table_ 0 keys set_fieldname set table_ 0 keys for row in table_ set_fieldname update set row keys additional_fields sorted set_fieldname set table_ 0 keys fieldnames additional_fields writer unicodecsv DictWriter f fieldnames encoding utf 8 writer writeheader for dict_ in table_ for key value in dict_ items if type value in list tuple dict_ key join unicode v for v in value writer writerows table_\\n',\n",
              " 'def _table_to_csv table_ f cStringIO StringIO try _write_csv f table_ return f getvalue finally f close\\n',\n",
              " 'def table dicts columns csv False pretty False if isinstance columns basestring columns _read_columns_file columns for column in columns values if pattern in column assert pattern_path not in column A column must have either a pattern or a pattern_path but not both column pattern_path column pattern del column pattern table_ for d in dicts row collections OrderedDict for column_title column_spec in columns items if not column_spec get return_multiple_columns False row column_title query dict_ d column_spec else multiple_columns query dict_ d column_spec for k v in multiple_columns items row k v table_ append row if pretty return tabulate tabulate table_ tablefmt grid headers keys elif csv return _table_to_csv table_ else return table_\\n',\n",
              " 'def query pattern_path dict_ max_length None strip False case_sensitive False unique False deduplicate False string_transformations None hyperlink False return_multiple_columns False if string_transformations is None string_transformations if max_length string_transformations append lambda x x max_length if hyperlink string_transformations append lambda x HYPERLINK 0 format x if isinstance pattern_path basestring pattern_path pattern_path original_pattern_path pattern_path pattern_path pattern_path pattern_path reverse result _process_object pattern_path dict_ string_transformations string_transformations strip strip case_sensitive case_sensitive return_multiple_columns return_multiple_columns if not result return None elif isinstance result dict return _flatten result elif len result 1 return result 0 else if unique msg pattern_path 0 n n format original_pattern_path msg msg pprint pformat dict_ raise UniqueError msg if deduplicate new_result for item in result if item not in new_result new_result append item result new_result return result\\n',\n",
              " 'def test_no_columns_argument table_function mock Mock nose tools assert_raises cli NoColumnsError cli do args table_function table_function assert not table_function called\\n',\n",
              " 'def test_help table_function mock Mock try cli do args h table_function table_function assert False losser h should raise an exception except cli CommandLineExit as err assert err code 0 assert not table_function called\\n',\n",
              " 'def test_long_help table_function mock Mock try cli do args help table_function table_function assert False losser help should raise an exception except cli CommandLineExit as err assert err code 0 assert not table_function called\\n',\n",
              " 'def test_help_and_other_args table_function mock Mock try cli do args h columns test_columns json table_function table_function assert False losser h should raise an exception except cli CommandLineExit as err assert err code 0 assert not table_function called\\n',\n",
              " 'def test_columns table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar cli do args columns test_columns json table_function table_function in_ mock_stdin table_function assert_called_once_with foobar test_columns json csv True pretty False\\n',\n",
              " 'mock patch sys stderr DEVNULL def test_columns_with_no_arg table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar try cli do args columns table_function table_function in_ mock_stdin assert False It should raise if given columns with no arg except cli CommandLineExit as err assert err code 2 assert not table_function called\\n',\n",
              " 'mock patch sys stderr DEVNULL def test_unrecognized_argument table_function mock Mock try cli do args columns test_columns json foobar table_function table_function assert False It should raise if given an unrecognized argument except cli CommandLineExit as err assert err code 2 assert not table_function called\\n',\n",
              " 'def test_input mock_stdin mock Mock mock_stdin read return_value foobar for arg in i input table_function mock Mock cli do args columns test_columns json arg _absolute_path test_input json table_function table_function in_ mock_stdin assert not mock_stdin called table_function assert_called_once_with foobar test_columns json csv True pretty False\\n',\n",
              " 'def test_with_one_column_argument table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar cli do args column Title pattern title table_function table_function in_ mock_stdin table_function assert_called_once_with foobar collections OrderedDict Title pattern title csv True pretty False\\n',\n",
              " 'def test_with_many_column_arguments table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar cli do args column Title pattern title case sensitive strip no column Description pattern notes unique column Owner pattern author max length 255 deduplicate True table_function table_function in_ mock_stdin expected_columns collections OrderedDict expected_columns Title pattern title case_sensitive True strip False expected_columns Description pattern notes unique True expected_columns Owner pattern author max_length 255 deduplicate True table_function assert_called_once_with foobar expected_columns csv True pretty False\\n',\n",
              " 'def test_with_repeated_column_option table_function mock Mock args column Title pattern title pattern repeated nose tools assert_raises cli DuplicateColumnOptionError cli do args args table_function table_function assert not table_function called\\n',\n",
              " 'def test_with_implicit_true mock_stdin mock Mock mock_stdin read return_value foobar expected_columns Title pattern title unique True table_function mock Mock cli do args column Title pattern title unique table_function table_function in_ mock_stdin table_function assert_called_once_with foobar expected_columns csv True pretty False\\n',\n",
              " 'def test_with_explicit_true mock_stdin mock Mock mock_stdin read return_value foobar expected_columns Title pattern title case_sensitive True for value in true True y Y yes Yes YES table_function mock Mock cli do args column Title pattern title case sensitive value table_function table_function in_ mock_stdin table_function assert_called_once_with foobar expected_columns csv True pretty False\\n',\n",
              " 'def test_with_explicit_false mock_stdin mock Mock mock_stdin read return_value foobar expected_columns Title pattern title strip False for value in false False n N no No NO table_function mock Mock cli do args column Title pattern title strip value table_function table_function in_ mock_stdin table_function assert_called_once_with foobar expected_columns csv True pretty False\\n',\n",
              " 'def test_column_with_no_pattern table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar nose tools assert_raises cli ColumnWithoutPatternError cli do args column Title table_function table_function in_ mock_stdin assert not table_function called\\n',\n",
              " 'def test_column_option_with_no_column table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar for option in unique strip deduplicate case sensitive pattern max length nose tools assert_raises cli ColumnOptionWithNoPrecedingColumnError cli do args option foo table_function table_function in_ mock_stdin assert not table_function called\\n',\n",
              " 'def test_boolean_option_with_invalid_arg table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar for option in unique strip deduplicate case sensitive nose tools assert_raises cli InvalidColumnOptionArgument cli do args column foo option invalid table_function table_function in_ mock_stdin assert not table_function called\\n',\n",
              " 'def test_max_length table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar cli do args column Title pattern title max length 255 table_function table_function in_ mock_stdin table_function assert_called_once_with foobar collections OrderedDict Title pattern title max_length 255 csv True pretty False\\n',\n",
              " 'mock patch sys stderr DEVNULL def test_max_length_with_no_arg table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar try cli do args column foo pattern foo max length table_function table_function in_ mock_stdin assert False It should raise if given max length with no arg except cli CommandLineExit as err assert err code 2 assert not table_function called\\n',\n",
              " 'def test_max_length_with_invalid_arg table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar args column foo pattern foo max length invalid nose tools assert_raises cli InvalidColumnOptionArgument cli do args args table_function table_function in_ mock_stdin assert not table_function called\\n',\n",
              " 'def test_pattern_with_multiple_arguments table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar cli do args column Formats pattern resources format table_function table_function in_ mock_stdin table_function assert_called_once_with foobar collections OrderedDict Formats pattern resources format csv True pretty False\\n',\n",
              " 'def test_column_and_columns_together table_function mock Mock mock_stdin mock Mock mock_stdin read return_value foobar args column foo pattern foo columns columns json nose tools assert_raises cli ColumnsAndColumnsFileError cli do args args table_function table_function in_ mock_stdin assert not table_function called\\n',\n",
              " 'def make_parser add_help True exclude_args None if exclude_args is None exclude_args parser argparse ArgumentParser add_help add_help parser description Filter transform and export a list of JSON objects on stdin to JSON or CSV on stdout if columns not in exclude_args parser add_argument columns dest columns_file help the JSON file specifying the columns to be output if i not in exclude_args and input not in exclude_args parser add_argument i input help read input from the given file instead of from stdin dest input_data if c not in exclude_args and column not in exclude_args parser add_argument c column action ColumnsAction if pattern not in exclude_args parser add_argument pattern action ColumnsAction nargs if max length not in exclude_args parser add_argument max length action ColumnsAction if strip not in exclude_args parser add_argument strip nargs action ColumnsAction if deduplicate not in exclude_args parser add_argument deduplicate nargs action ColumnsAction if case sensitive not in exclude_args parser add_argument case sensitive nargs action ColumnsAction if unique not in exclude_args parser add_argument unique nargs action ColumnsAction if p not in exclude_args and pretty not in exclude_args parser add_argument p pretty action store_true return parser\\n',\n",
              " 'def parse parser None args None if not parser parser make_parser try parsed_args parser parse_args args except SystemExit as err raise CommandLineExit err code try columns parsed_args columns except AttributeError columns collections OrderedDict parsed_args columns columns for title spec in columns items if pattern not in spec raise ColumnWithoutPatternError Column 0 needs a pattern format title if len spec pattern 1 spec pattern spec pattern 0 if columns and parsed_args columns_file raise ColumnsAndColumnsFileError You can t use the column and columns options together yet elif parsed_args columns_file and not columns parsed_args columns parsed_args columns_file elif not columns and not parsed_args columns_file raise NoColumnsError You must give either a columns or at least one c column argument else assert columns return parsed_args\\n',\n",
              " 'def do parser None args None in_ None table_function None in_ in_ or sys stdin table_function table_function or losser table parsed_args parse parser parser args args if parsed_args input_data input_data open parsed_args input_data r read else input_data in_ read dicts json loads input_data csv_string table_function dicts parsed_args columns csv True pretty parsed_args pretty return csv_string\\n',\n",
              " 'def main parser make_parser try output do parser parser except CommandLineExit as err sys exit err code except CommandLineError as err if err message parser error err message sys stdout write output\\n',\n",
              " 'def connect self server None port None aprs_filter None if not self aprsis_sock self __reset_buffer server server or aprs_constants APRSIS_SERVER port port or aprs_constants APRSIS_FILTER_PORT aprs_filter aprs_filter or join p self user self full_auth join self _auth filter aprs_filter self server server self port port self aprsis_sock socket socket socket AF_INET socket SOCK_STREAM self aprsis_sock connect server port self logger info Connected to server s port s server port self logger debug Sending full_auth s self full_auth self aprsis_sock sendall ord c for c in self full_auth n r\\n',\n",
              " 'def write self frame_decoded headers None protocol TCP frame aprs_util encode_frame frame_decoded if TCP in protocol self aprsis_sock sendall ord c for c in frame return True elif HTTP in protocol content n join self _auth frame headers headers or aprs_constants APRSIS_HTTP_HEADERS result requests post self _url data content headers headers return 204 result status_code elif UDP in protocol content n join self _auth frame sock socket socket socket AF_INET socket SOCK_DGRAM sock sendto content aprs_constants APRSIS_SERVER aprs_constants APRSIS_RX_PORT return True\\n',\n",
              " 'def read self filter_logresp True read_more True while read_more selected select select self aprsis_sock 0 if len selected 0 0 recvd_data self aprsis_sock recv aprs_constants RECV_BUFFER if recvd_data self data_buffer recvd_data else read_more False else read_more False if r n in self data_buffer partial True if self data_buffer endswith r n partial False packets recvd_data split r n if partial self data_buffer str packets pop 1 else self data_buffer for packet in packets self packet_buffer str packet while len self packet_buffer packet self packet_buffer pop 0 if filter_logresp and packet startswith and logresp in packet pass else return aprs_util decode_frame packet return None\\n',\n",
              " 'def test_escape_special_codes_fend self fend apex kiss Kiss _Kiss__escape_special_codes apex kiss constants FEND self assertEqual fend apex kiss constants FESC_TFEND\\n',\n",
              " 'def test_escape_special_codes_fesc self fesc apex kiss Kiss _Kiss__escape_special_codes apex kiss constants FESC self assertEqual fesc apex kiss constants FESC_TFESC\\n',\n",
              " 'staticmethod def __decode_frame raw_frame logging debug raw_frame s raw_frame frame frame_len len raw_frame if frame_len 16 for raw_slice in range 0 frame_len 2 if raw_frame raw_slice 1 and raw_slice 1 7 0 i raw_slice 1 7 if 1 i 11 if raw_frame raw_slice 1 3 is 3 and raw_frame raw_slice 2 in 240 207 frame text join map chr raw_frame raw_slice 3 frame destination AprsKiss __identity_as_string AprsKiss __extract_callsign raw_frame frame source AprsKiss __identity_as_string AprsKiss __extract_callsign raw_frame 7 frame path AprsKiss __extract_path int i raw_frame return frame logging debug frame s frame return frame\\n',\n",
              " 'staticmethod def __extract_path start raw_frame full_path for i in range 2 start path AprsKiss __identity_as_string AprsKiss __extract_callsign raw_frame i 7 if path if raw_frame i 7 6 128 full_path append join path else full_path append path return full_path\\n',\n",
              " 'staticmethod def __extract_callsign raw_frame callsign join chr x 1 for x in raw_frame 6 strip ssid raw_frame 6 1 15 return callsign callsign ssid ssid\\n',\n",
              " 'staticmethod def __identity_as_string identity if identity ssid 0 return join identity callsign str identity ssid else return identity callsign\\n',\n",
              " 'staticmethod def __encode_frame frame enc_frame AprsKiss __encode_callsign AprsKiss __parse_identity_string frame destination AprsKiss __encode_callsign AprsKiss __parse_identity_string frame source for p in frame path enc_frame AprsKiss __encode_callsign AprsKiss __parse_identity_string p return enc_frame 1 enc_frame 1 1 apex kiss constants SLOT_TIME 240 ord c for c in frame text\\n',\n",
              " 'staticmethod def __encode_callsign callsign call_sign callsign callsign enc_ssid callsign ssid 1 96 if in call_sign call_sign call_sign replace enc_ssid 128 while len call_sign 6 call_sign join call_sign encoded for p in call_sign encoded ord p 1 return encoded enc_ssid\\n',\n",
              " 'staticmethod def __parse_identity_string identity_string if identity_string endswith identity_string identity_string 1 if in identity_string call_sign ssid identity_string split else call_sign identity_string ssid 0 return callsign call_sign ssid int ssid\\n',\n",
              " 'def write self frame args kwargs with self lock encoded_frame AprsKiss __encode_frame frame if AprsKiss __valid_frame encoded_frame self data_stream write encoded_frame args kwargs\\n',\n",
              " 'def read self args kwargs with self lock frame self data_stream read args kwargs if frame is not None and len frame return AprsKiss __decode_frame frame else return None\\n',\n",
              " 'def connect self mode_init None kwargs self logger debug kwargs s kwargs address self host self tcp_port self socket socket create_connection address\\n',\n",
              " 'def test_latitude_north self test_lat 37 7418096 aprs_lat apex aprs util dec2dm_lat test_lat self logger debug aprs_lat s aprs_lat lat_deg int aprs_lat split 0 1 self assertTrue len aprs_lat 8 self assertTrue lat_deg 0 self assertTrue lat_deg 90 self assertTrue aprs_lat endswith N\\n',\n",
              " 'def test_latitude_south self test_lat 37 7418096 aprs_lat apex aprs util dec2dm_lat test_lat self logger debug aprs_lat s aprs_lat lat_deg int aprs_lat split 0 1 self assertTrue len aprs_lat 8 self assertTrue lat_deg 0 self assertTrue lat_deg 90 self assertTrue aprs_lat endswith S\\n',\n",
              " 'def test_longitude_west self test_lng 122 38833 aprs_lng apex aprs util dec2dm_lng test_lng self logger debug aprs_lng s aprs_lng lng_deg int aprs_lng split 0 2 self assertTrue len aprs_lng 9 self assertTrue lng_deg 0 self assertTrue lng_deg 180 self assertTrue aprs_lng endswith W\\n',\n",
              " 'def test_longitude_east self test_lng 122 38833 aprs_lng apex aprs util dec2dm_lng test_lng self logger debug aprs_lng s aprs_lng lng_deg int aprs_lng split 0 2 self assertTrue len aprs_lng 9 self assertTrue lng_deg 0 self assertTrue lng_deg 180 self assertTrue aprs_lng endswith E\\n',\n",
              " 'def test_valid_callsign_valid self for i in VALID_CALLSIGNS self assertTrue apex aprs util valid_callsign i s is a valid call i\\n',\n",
              " 'def test_valid_callsign_invalid self for i in INVALID_CALLSIGNS self assertFalse apex aprs util valid_callsign i s is an invalid call i\\n',\n",
              " 'def test_decode_aprs_ascii_frame self ascii_frame W2GMD 9 APOTC1 WIDE1 1 WIDE2 1 3745 94N 12228 05W 118 010 A 000269 38C Temp http w2gmd org Twitter ampledata frame apex aprs util decode_frame ascii_frame self assertEqual source W2GMD 9 destination APOTC1 path APOTC1 WIDE1 1 WIDE2 1 text 3745 94N 12228 05W 118 010 A 000269 38C Temp http w2gmd org Twitter ampledata frame\\n',\n",
              " 'def test_format_aprs_frame self frame source W2GMD 1 destination OMG path WIDE1 1 text test_format_aprs_frame formatted_frame apex aprs util encode_frame frame self assertEqual formatted_frame W2GMD 1 OMG WIDE1 1 test_format_aprs_frame\\n',\n",
              " 'def connect self mode_init None kwargs self logger debug kwargs s kwargs self serial serial Serial port self com_port baudrate self baud parity self parity stopbits self stop_bits bytesize self byte_size self serial timeout kiss_constants SERIAL_TIMEOUT if mode_init is not None self serial write mode_init self exit_kiss True else self exit_kiss False if kwargs for name value in kwargs items super KissSerial self _write_setting name value\\n',\n",
              " 'def dec2dm_lat dec dec_min apex aprs decimal_degrees decimal2dm dec deg dec_min 0 abs_deg abs deg if not deg abs_deg suffix S else suffix N return join str abs_deg 2f dec_min 1 suffix\\n',\n",
              " 'def dec2dm_lng dec dec_min apex aprs decimal_degrees decimal2dm dec deg dec_min 0 abs_deg abs deg if not deg abs_deg suffix W else suffix E return join str abs_deg 2f dec_min 1 suffix\\n',\n",
              " 'def decode_frame frame logging debug frame s frame decoded_frame frame_so_far for char in frame if in char and source not in decoded_frame decoded_frame source frame_so_far frame_so_far elif in char and path not in decoded_frame decoded_frame path frame_so_far frame_so_far else frame_so_far join frame_so_far char decoded_frame text frame_so_far decoded_frame destination decoded_frame path split 0 return decoded_frame\\n',\n",
              " 'def encode_frame frame formatted_frame join frame source frame destination if frame path formatted_frame join formatted_frame format_path frame path formatted_frame formatted_frame frame text return formatted_frame\\n',\n",
              " 'def format_path path_list return join path_list\\n',\n",
              " 'def valid_callsign callsign logging debug callsign s callsign if in callsign if not callsign count 1 return False else callsign ssid callsign split else ssid 0 logging debug callsign s ssid s callsign ssid if len callsign 2 or len callsign 6 or len str ssid 1 or len str ssid 2 return False for char in callsign if not char isalpha or char isdigit return False if not str ssid isdigit return False if int ssid 0 or int ssid 15 return False return True\\n',\n",
              " 'def run_doctest import doctest import apex aprs util return doctest testmod apex aprs util\\n',\n",
              " 'def hash_frame frame hashing 0 index 0 frame_string_prefix frame source frame destination for frame_chr in frame_string_prefix hashing ord frame_chr 8 index 4 hashing index 1 for char in frame text hashing ord char 8 index 4 hashing index 1 return hashing\\n',\n",
              " 'def decimal2dms decimal_degrees degrees D int decimal_degrees decimal_minutes libdecimal getcontext multiply D str decimal_degrees degrees copy_abs D 60 minutes D int decimal_minutes seconds libdecimal getcontext multiply decimal_minutes minutes D 60 return degrees minutes seconds\\n',\n",
              " 'def decimal2dm decimal_degrees degrees D int decimal_degrees minutes libdecimal getcontext multiply D str decimal_degrees degrees copy_abs D 60 return degrees minutes\\n',\n",
              " 'def dms2decimal degrees minutes seconds decimal D 0 degs D str degrees mins libdecimal getcontext divide D str minutes D 60 secs libdecimal getcontext divide D str seconds D 3600 if degrees D 0 decimal degs mins secs else decimal degs mins secs return libdecimal getcontext normalize decimal\\n',\n",
              " 'def dm2decimal degrees minutes return dms2decimal degrees minutes 0\\n',\n",
              " 'def run_doctest import doctest if six PY2 import decimaldegrees return doctest testmod decimaldegrees\\n',\n",
              " 'def test_encode_callsign self encoded_callsign apex aprs AprsKiss _AprsKiss__encode_callsign DECODED_CALLSIGN self assertEqual ENCODED_CALLSIGN encoded_callsign\\n',\n",
              " 'def test_encode_callsign_digipeated self encoded_callsign apex aprs AprsKiss _AprsKiss__encode_callsign DECODED_CALLSIGN_DIGIPEATED self assertEqual ENCODED_CALLSIGN_DIGIPEATED encoded_callsign\\n',\n",
              " 'def test_decode_callsign self decoded_callsign apex aprs AprsKiss _AprsKiss__extract_callsign ENCODED_CALLSIGN self assertEqual DECODED_CALLSIGN decoded_callsign\\n',\n",
              " 'def test_decode_callsign_digipeated self decoded_callsign apex aprs AprsKiss _AprsKiss__extract_callsign ENCODED_CALLSIGN_DIGIPEATED self assertEqual DECODED_CALLSIGN decoded_callsign\\n',\n",
              " 'def test_encode_frame self encoded_frame apex aprs AprsKiss _AprsKiss__encode_frame DECODED_FRAME self assertEqual ENCODED_FRAME encoded_frame\\n',\n",
              " 'def test_encode_frame_recorded self encoded_frame apex aprs AprsKiss _AprsKiss__encode_frame DECODED_FRAME_RECORDED self assertEqual ENCODED_FRAME_RECORDED encoded_frame\\n',\n",
              " 'def test_encode_frame_multipath self encoded_frame apex aprs AprsKiss _AprsKiss__encode_frame DECODED_FRAME_MULTIPATH self assertEqual ENCODED_FRAME_MULTIPATH encoded_frame\\n',\n",
              " 'def test_decode_frame self decoded_frame apex aprs AprsKiss _AprsKiss__decode_frame ENCODED_FRAME self assertEqual DECODED_FRAME decoded_frame\\n',\n",
              " 'def test_decode_frame_recorded self decoded_frame apex aprs AprsKiss _AprsKiss__decode_frame ENCODED_FRAME_RECORDED self assertEqual DECODED_FRAME_RECORDED decoded_frame\\n',\n",
              " 'def test_decode_frame_multipath self decoded_frame apex aprs AprsKiss _AprsKiss__decode_frame ENCODED_FRAME_MULTIPATH self assertEqual DECODED_FRAME_MULTIPATH decoded_frame\\n',\n",
              " 'def test_extract_path self extracted_path apex aprs AprsKiss _AprsKiss__extract_path 3 ENCODED_FRAME self assertEqual DECODED_FRAME path 0 extracted_path 0\\n',\n",
              " 'def test_idwentity_as_string_with_ssid self callsign callsign W2GMD ssid 1 full_callsign apex aprs AprsKiss _AprsKiss__identity_as_string callsign self assertEqual full_callsign W2GMD 1\\n',\n",
              " 'def test_identity_as_string_sans_ssid self callsign callsign W2GMD ssid 0 full_callsign apex aprs AprsKiss _AprsKiss__identity_as_string callsign self assertEqual full_callsign W2GMD\\n',\n",
              " 'staticmethod def __strip_df_start frame while frame 0 is kiss_constants DATA_FRAME del frame 0 while chr frame 0 isspace del frame 0 while chr frame 1 isspace del frame 1 return frame\\n',\n",
              " 'staticmethod def __escape_special_codes raw_code_bytes encoded_bytes for raw_code_byte in raw_code_bytes if raw_code_byte is kiss_constants FESC encoded_bytes kiss_constants FESC_TFESC elif raw_code_byte is kiss_constants FEND encoded_bytes kiss_constants FESC_TFEND else encoded_bytes raw_code_byte return encoded_bytes\\n',\n",
              " 'staticmethod def __command_byte_combine port command_code if port 127 or port 0 raise Exception port out of range elif command_code 127 or command_code 0 raise Exception command_Code out of range return port 4 command_code\\n',\n",
              " 'def connect self mode_init None kwargs pass\\n',\n",
              " 'def _write_setting self name value self logger debug Configuring s s name repr value if isinstance value int value chr value return self _write_interface kiss_constants FEND getattr kiss_constants name upper Kiss __escape_special_codes value kiss_constants FEND\\n',\n",
              " 'def __fill_buffer self new_frames read_buffer read_data self _read_interface while read_data is not None and len read_data split_data for read_byte in read_data if read_byte is kiss_constants FEND split_data append else split_data 1 append read_byte len_fend len split_data if len_fend 1 read_buffer split_data 0 elif len_fend 2 if split_data 0 new_frames append read_buffer split_data 0 read_buffer else new_frames append read_buffer read_buffer split_data 1 elif len_fend 3 for i in range 0 len_fend 1 read_buffer_tmp read_buffer split_data i if len read_buffer_tmp is not 0 new_frames append read_buffer_tmp read_buffer if split_data len_fend 1 read_buffer split_data len_fend 1 read_data self _read_interface for new_frame in new_frames if len new_frame and new_frame 0 0 if self strip_df_start new_frame Kiss __strip_df_start new_frame self frame_buffer append new_frame\\n',\n",
              " 'def write self frame_bytes port 0 with self lock kiss_packet kiss_constants FEND Kiss __command_byte_combine port kiss_constants DATA_FRAME Kiss __escape_special_codes frame_bytes kiss_constants FEND return self _write_interface kiss_packet\\n',\n",
              " 'def __init__ self regressor_name dataset None outcome_index 0 self regressor_name regressor_name self input_dimensions None self model None self mean_facts_vector None self outcome_index None self dataset None try facts MultiClassSVM get_ordered_weights regressor_name important_facts facts MultiClassSVM get_ordered_weights regressor_name additional_facts facts_index TagPrecedents get_intent_index facts_vector self important_facts_index x 0 for x in facts_index if x 1 in facts except pass if dataset is not None self __filter_dataset dataset outcome_index else self load\\n',\n",
              " 'def predict self precedent_vector precedent_vector precedent_vector self important_facts_index data self mean_facts_vector copy data data self important_facts_index for i in range len precedent_vector if precedent_vector i 0 data i precedent_vector i return self model predict data\\n',\n",
              " 'def save self regressor_name self regressor_name file_path os path join Path binary_directory _regressor bin format regressor_name Log write saving _regressor bin format regressor_name to file_path Log write _regressor bin format regressor_name saved to file_path self model steps 1 1 model save file_path Save save_binary _scaler bin format regressor_name self model steps 0 1 Save save_binary model_metrics bin self data_metrics self dataset None\\n',\n",
              " 'def load self regressor_name self regressor_name Log write Loading _regressor bin format regressor_name file_path os path join Path binary_directory _regressor bin format regressor_name Log write _regressor bin format regressor_name is successfully loaded regressor load_model file_path scaler Load load_binary _scaler bin format regressor_name self model AbstractRegressor _create_pipeline scaler regressor self mean_facts_vector Load load_binary model_metrics bin regressor regressor_name mean_facts_vector\\n',\n",
              " 'staticmethod def _create_pipeline scaler regressor estimators estimators append standardize scaler estimators append mlp regressor return Pipeline estimators\\n',\n",
              " 'def test self X np array precedent facts_vector self important_facts_index for precedent in self dataset y_pred self model predict X y_true np array precedent outcomes_vector self outcome_index for precedent in self dataset r2 metrics r2_score y_true y_pred variance metrics explained_variance_score y_true y_pred mean_abs_error metrics mean_absolute_error y_true y_pred mean_squared_error metrics mean_squared_error y_true y_pred Log write R2 0 2f format r2 Log write Explained Variance 0 2f format variance Log write Mean Absolute Error 0 2f format mean_abs_error Log write Mean Squared Error 0 2f format mean_squared_error\\n',\n",
              " 'def data_metrics self facts_vector x facts_vector for x in self dataset outcomes_vector x outcomes_vector self outcome_index for x in self dataset model_metrics Load load_binary model_metrics bin if model_metrics is None model_metrics regressor self regressor_name elif regressor not in model_metrics model_metrics regressor self mean_facts_vector np mean facts_vector axis 0 model_metrics regressor self regressor_name mean_facts_vector self mean_facts_vector std np std outcomes_vector variance np var outcomes_vector mean np mean outcomes_vector return model_metrics\\n',\n",
              " 'def __filter_dataset self dataset outcome_index outcomes x outcomes_vector outcome_index for x in dataset if x outcomes_vector outcome_index 1 std np std outcomes mean np mean outcomes self dataset precedent for precedent in dataset if precedent outcomes_vector outcome_index 1 and abs precedent outcomes_vector outcome_index mean 2 std self outcome_index outcome_index\\n',\n",
              " 'def _get_image_from_file_storage file_storage in_memory_file io BytesIO file_storage save in_memory_file np_data np fromstring in_memory_file getvalue dtype np uint8 return cv2 imdecode np_data 0\\n',\n",
              " 'def _binarize img img _filter_blur img _ img cv2 threshold img 0 255 cv2 THRESH_BINARY cv2 THRESH_OTSU return img\\n',\n",
              " 'def _sort_corners pts diff np diff pts axis 1 summ pts sum axis 1 return np array pts np argmin summ pts np argmax diff pts np argmax summ pts np argmin diff np float32\\n',\n",
              " 'def _get_transformed_dimensions_of_tilted_document corners corners _sort_corners corners 0 height max np linalg norm corners 0 corners 1 np linalg norm corners 2 corners 3 width max np linalg norm corners 1 corners 2 np linalg norm corners 3 corners 0 return height width\\n',\n",
              " 'def generate_report conversation ml_prediction similar_precedents probabilities_dict report ml_statistics ml_service get_statistics conversation_facts fact_service get_resolved_fact_keys conversation report accuracy 0 relevant_probabilities_dict k v for k v in probabilities_dict items if k in ml_prediction accuracy_mean statistics mean float v for k v in relevant_probabilities_dict items report accuracy accuracy_mean report data_set ml_statistics data_set size report similar_case len similar_precedents report curves possible_curve_outcomes ml_statistics regressor for outcome in ml_prediction if outcome in possible_curve_outcomes report curves outcome dict possible_curve_outcomes outcome report curves outcome outcome_value ml_prediction outcome report outcomes for outcome in ml_prediction if int ml_prediction outcome 1 report outcomes outcome True elif int ml_prediction outcome 0 report outcomes outcome False else report outcomes outcome ml_prediction outcome report similar_precedents for precedent in similar_precedents filtered_fact_dict k v for k v in precedent facts items if k in conversation_facts precedent facts __dict_values_to_int filtered_fact_dict filtered_outcome_dict k v for k v in precedent outcomes items if k in ml_prediction precedent outcomes __dict_values_to_int filtered_outcome_dict report similar_precedents append precedent return report\\n',\n",
              " 'def get_outcome_facts global outcome_facts if not outcome_facts outcome_facts requests get format ML_URL weights json return outcome_facts\\n',\n",
              " 'def get_anti_facts global anti_facts if not anti_facts anti_facts requests get format ML_URL antifacts json return anti_facts\\n',\n",
              " 'def get_statistics global ml_statistics if not ml_statistics ml_statistics requests get format ML_URL statistics json return ml_statistics\\n',\n",
              " 'def submit_resolved_fact_list conversation fact_dict generate_fact_dict conversation log debug Submitting Resolved Fact Dict n tFact Dict format fact_dict req_dict facts fact_dict res requests post format ML_URL predict json req_dict return res json\\n',\n",
              " 'def extract_prediction claim_category ml_response essential_outcomes fact_service outcome_mapping claim_category claim_category lower essential_outcome_list if claim_category in essential_outcomes essential_outcome_list essential_outcomes claim_category all_outcomes ml_response outcomes_vector relevant_outcomes for outcome in all_outcomes if outcome in essential_outcome_list or int all_outcomes outcome 0 relevant_outcomes outcome int all_outcomes outcome return relevant_outcomes\\n',\n",
              " 'def generate_fact_dict conversation resolved_facts for fact_entity in conversation fact_entities fact_entity_name fact_entity fact name fact_entity_type fact_entity fact type if fact_entity_type FactType BOOLEAN if fact_entity value true resolved_facts fact_entity_name True elif fact_entity value false resolved_facts fact_entity_name False elif fact_entity_type FactType MONEY or fact_entity_type FactType DURATION_MONTHS resolved_facts fact_entity_name float fact_entity value if conversation person_type is PersonType LANDLORD resolved_facts asker_is_landlord True resolved_facts asker_is_tenant False else resolved_facts asker_is_landlord False resolved_facts asker_is_tenant True anti_facts get_anti_facts for fact in list resolved_facts if fact in anti_facts keys anti_fact_key_name anti_facts fact resolved_facts anti_fact_key_name not resolved_facts fact elif fact in anti_facts values anti_fact_key_name k for k v in anti_facts items if v fact 0 resolved_facts anti_fact_key_name not resolved_facts fact for fact_entity in resolved_facts if resolved_facts fact_entity is True resolved_facts fact_entity 1 elif resolved_facts fact_entity is False resolved_facts fact_entity 0 return resolved_facts\\n',\n",
              " 'staticmethod def execute command_list checkpoint command_list 1 if checkpoint CommandEnum PRE_PROCESSING feature_extraction_driver run command_list 1 elif checkpoint CommandEnum POST_PROCESSING feature_extraction_driver run command_list 1 elif checkpoint CommandEnum TRAIN_FACTS training_driver run command_list 2 else Log write Command not recognized command_list 1 return False return True\\n',\n",
              " 'def __init__ self train False dataset if not train self model Load load_binary similarity_model bin self case_numbers Load load_binary similarity_case_numbers bin self scaler Load load_binary similarity_scaler bin elif len dataset 0 sample_set np concatenate vec facts_vector vec outcomes_vector for vec in dataset for i in range len sample_set sample_set i sample_set i astype np int64 self scaler StandardScaler sample_set self scaler fit_transform sample_set self model NearestNeighbors 5 metric euclidean self model fit sample_set self case_numbers vec name for vec in dataset save Save save save_binary similarity_model bin self model save save_binary similarity_case_numbers bin self case_numbers save save_binary similarity_scaler bin self scaler else raise ValueError Please train or load the classifier first\\n',\n",
              " 'def get_most_similar self sample input_vector self scaler transform np concatenate sample facts_vector sample outcomes_vector nearest self model kneighbors input_vector names self case_numbers index for index in nearest 1 0 return list zip names nearest 0 0\\n',\n",
              " 'def train self force_train False initialize_interpreters True self __train_interpreter self fact_data_dir self fact_interpreters force_train force_train initialize_interpreters initialize_interpreters self __train_interpreter self category_data_dir self category_interpreters force_train force_train initialize_interpreters initialize_interpreters self __train_interpreter self acknowledgement_data_dir self acknowledgement_interpreters force_train force_train initialize_interpreters initialize_interpreters\\n',\n",
              " 'def classify_problem_category self message person_type if person_type lower tenant return self category_interpreters category_tenant parse message lower elif person_type lower landlord return self category_interpreters category_landlord parse message lower\\n',\n",
              " 'def classify_fact self fact_name message if fact_name in self fact_interpreters return self fact_interpreters fact_name parse message lower return None\\n',\n",
              " 'def classify_acknowledgement self message return self acknowledgement_interpreters additional_fact_acknowledgement parse message lower\\n',\n",
              " 'def __train_interpreter self training_data_dir interpreter_dict force_train initialize_interpreters print Starting training with data directory format training_data_dir if force_train is False print No force train using saved models format training_data_dir if initialize_interpreters is False print No interpreter initialization Will only create model data format training_data_dir training_start timeit default_timer fact_files os listdir training_data_dir for filename in fact_files fact_key os path splitext filename 0 if force_train training_data load_data training_data_dir filename self trainer train training_data model_directory self trainer persist path self model_dir fixed_model_name fact_key else model_directory self model_dir default fact_key print Model data directory for fact format fact_key model_directory if initialize_interpreters interpreter_dict fact_key Interpreter load model_directory self rasa_config self builder training_end timeit default_timer total_training_time round training_end training_start 2 print Training Finished Took s for facts format total_training_time len fact_files\\n',\n",
              " 'def __init__ self directory self directory directory\\n',\n",
              " 'def save_binary self filename content file_path os path join Path binary_directory filename Log write saving filename to file_path joblib dump content file_path Log write filename saved to file_path\\n',\n",
              " 'def save_text self data_tuple labels protocol a unique_labels set labels for label in unique_labels text for i sent in enumerate data_tuple InformationType SENTENCE value labels label text append sent text append n n for i filename in enumerate data_tuple InformationType FILE_NAME value labels label text append filename target_file_name str label txt file_path os path join Path cluster_directory self directory target_file_name Log write saving target_file_name to file_path file open file_path protocol for lines in text file writelines lines file writelines n file close Log write target_file_name saved to file_path\\n',\n",
              " 'staticmethod def load_binary filename None try Log write Loading filename file_path os path join Path binary_directory filename file open file_path rb binary joblib load file Log write filename is successfully loaded return binary except BaseException Log write filename not found\\n',\n",
              " 'def __init__ self data_set None self data_set data_set self model None self mlb None self classifier_labels None self label_column_index None\\n',\n",
              " 'def weights_to_csv self try if self model is None self model Load load_binary multi_class_svm_model bin self classifier_labels Load load_binary classifier_labels bin except BaseException return None index TagPrecedents get_intent_index fact_header for header in index facts_vector fact_header append header 1 with open weights csv w as outcsv writer csv writer outcsv writer writerow fact_header for i in range len self model estimators_ outcome_list self classifier_labels i estimator self model estimators_ i try weights estimator coef_ 0 for j in range len weights outcome_list append weights j writer writerow outcome_list except AttributeError pass Log write Weights saved to csv\\n',\n",
              " 'def get_ordered_weights self if self model is None self model Load load_binary multi_class_svm_model bin self classifier_labels Load load_binary classifier_labels bin self label_column_index TagPrecedents get_intent_index weight_dict for i in range len self model estimators_ outcome_list estimator self model estimators_ i try weights estimator coef_ 0 for j in range len weights if weights j 0 outcome_list append self label_column_index facts_vector j 1 weights j outcome_list sort key lambda x abs x 1 reverse True weights abs x 1 for x in outcome_list mean_power math log10 np mean np array weights important_facts x 0 for x in outcome_list if math log10 abs x 1 mean_power additional_facts x 0 for x in outcome_list if math log10 abs x 1 mean_power if self classifier_labels i 0 additional_indemnity_money important_facts append tenant_monthly_payment important_facts append tenant_not_paid_lease_timespan if tenant_not_paid_lease_timespan in additional_facts additional_facts remove tenant_not_paid_lease_timespan if tenant_monthly_payment in additional_facts additional_facts remove tenant_monthly_payment weight_dict self classifier_labels i 0 weight_dict self classifier_labels i 0 important_facts important_facts weight_dict self classifier_labels i 0 additional_facts additional_facts except AttributeError print Problem with prediction format self classifier_labels i 0 return weight_dict\\n',\n",
              " 'def __test self x_test y_test model_metrics Load load_binary model_metrics bin if model_metrics is None model_metrics classifier elif classifier not in model_metrics model_metrics classifier model_metrics data_set size len self data_set index TagPrecedents get_intent_index outcomes_vector Log write Testing Classifier y_predict self model predict x_test Log write Classifier results n for i in range len y_predict 0 yp y_predict i yt y_test i accuracy np sum yp yt 100 0 len yt column_name index self mlb classes_ i 1 precision recall f1 _ precision_recall_fscore_support yt yp Log write Column format column_name Log write Test accuracy format accuracy Log write Precision format precision Log write Recall format recall Log write F1 n format f1 model_metrics classifier column_name prediction_accuracy accuracy Save save_binary model_metrics bin model_metrics\\n',\n",
              " 'def train self x_total y_total self reshape_dataset self mlb MultiLabelBinarizer y_total self mlb fit_transform y_total x_train x_test y_train y_test train_test_split x_total y_total test_size 0 2 random_state 42 Log write Sample size format len x_total Log write Train size format len x_train Log write Test size format len x_test Log write Training Classifier Using Multi Class SVM clf OneVsRestClassifier SVC kernel linear random_state 42 probability True clf fit x_train y_train self model clf self __test x_test y_test\\n',\n",
              " 'def save self linear_labels indices TagPrecedents get_intent_index outcomes_vector for i in range len self mlb classes_ label indices self mlb classes_ i 1 data_type indices self mlb classes_ i 2 linear_labels i label data_type save Save save save_binary classifier_labels bin linear_labels save save_binary multi_class_svm_model bin self model\\n',\n",
              " 'def predict self data if self model is None self model Load load_binary multi_class_svm_model bin data binarize data threshold 0 probabilities self model predict_proba data 0 predictions self model predict data for i in range len probabilities prediction predictions 0 i if prediction 0 probabilities i 1 probabilities i probabilities i format probabilities i 2f return self model predict data probabilities\\n',\n",
              " 'staticmethod def load_classifier_labels return Load load_binary classifier_labels bin\\n',\n",
              " 'def reshape_dataset self x_total np array np reshape precedent facts_vector len precedent facts_vector for precedent in self data_set x_total binarize x_total threshold 0 y_list for precedent in self data_set y_list append self __classify_precedent precedent y_total np array y_list return x_total y_total\\n',\n",
              " 'def __classify_precedent self precedent classified_precedent outcome_vector precedent outcomes_vector for i in range len outcome_vector if outcome_vector i 1 classified_precedent append i return classified_precedent\\n',\n",
              " 'def run command_list command command_list 0 if command CommandEnum PRE_PROCESSING pre_processing_driver run command_list 1 elif command CommandEnum POST_PROCESSING post_processing_driver run command_list 1 else Log write Command not recognized command_list 0 return False return True\\n',\n",
              " 'def run command_list nb_files 1 try nb_files int command_list 0 except BaseException pass regex_tagger run nb_files\\n',\n",
              " 'def regex_finder sentence regex_match_list for fact in RegexLib regex_facts for reg in fact regex_index if re search reg sentence regex_match_list append fact regex_name_index for outcome in RegexLib regex_outcomes for reg in outcome regex_index if re search reg sentence regex_match_list append outcome regex_name_index return regex_match_list\\n',\n",
              " 'def sentence_finder regex_name nb_of_files regexes get_regexes regex_name count 0 regex_dict for i in os listdir Path raw_data_directory if count nb_of_files break count 1 file open Path raw_data_directory i r encoding ISO 8859 1 for line in file regex_index 0 for reg in regexes if reg search line line re sub d line line line replace line line strip if regex_index in regex_dict keys if line in regex_dict regex_index continue regex_dict regex_index append line else regex_dict regex_index regex_index 1 file close return regex_dict\\n',\n",
              " 'staticmethod def predict_outcome input_json facts_vector MlController fact_dict_to_vector input_json facts outcome_vector probabilities MlController classifier_model predict facts_vector outcome_vector outcome_vector 0 outcome_vector MlController regression_model predict facts_vector outcome_vector response MlController outcome_vector_to_dict outcome_vector response probabilities_vector MlController probability_vector_to_dict probabilities similar_dict facts_vector facts_vector outcomes_vector outcome_vector response similar_precedents MlController format_similar_precedents MlController similar_finder get_most_similar similar_dict return response\\n',\n",
              " 'staticmethod def fact_dict_to_vector input_dict output_vector np zeros len MlController indexes facts_vector for index val data_type in MlController indexes facts_vector if val in input_dict output_vector index int input_dict val return output_vector\\n',\n",
              " 'staticmethod def format_similar_precedents similarity_list formatted_precedents for precedent_array in similarity_list precedent precedent MlController precedent_vectors precedent_array 0 file_number distance precedent_array 1 facts MlController fact_vector_to_dict MlController precedent_vectors precedent_array 0 facts_vector facts outcomes MlController outcome_vector_to_dict MlController precedent_vectors precedent_array 0 outcomes_vector outcomes_vector for fact_tuple in MlController indexes facts_vector if fact_tuple 2 bool precedent facts fact_tuple 1 bool float precedent facts fact_tuple 1 for outcome_tuple in MlController classifier_labels values if outcome_tuple 1 bool precedent outcomes outcome_tuple 0 bool float precedent outcomes outcome_tuple 0 formatted_precedents append precedent return formatted_precedents\\n',\n",
              " 'staticmethod def get_weighted_facts return MlController classifier_model get_ordered_weights\\n',\n",
              " 'staticmethod def get_ml_statistics stat_dict Load load_binary model_metrics bin for regressor_type in stat_dict regressor stat_dict regressor regressor_type pop mean_facts_vector None return stat_dict\\n',\n",
              " 'staticmethod def match_any_regex text regex_array regex_type if EntityExtraction regex_bin is None EntityExtraction regex_bin RegexLib model for regex in regex_array regex_result regex search text if regex_result sentence regex_result group 0 lower return EntityExtraction __extract_regex_entity sentence regex_type return False 0\\n',\n",
              " 'staticmethod def __extract_regex_entity sentence regex_type nfkd_form unicodedata normalize NFKD sentence sentence join character for character in nfkd_form if not unicodedata combining character if regex_type BOOLEAN return True 1 elif regex_type MONEY_REGEX return EntityExtraction __regex_money regex_type sentence elif regex_type DATE_REGEX return EntityExtraction get_fact_duration sentence return False 0\\n',\n",
              " 'staticmethod def get_fact_duration sentence non_payment_regex re compile pas paye re IGNORECASE if re findall non_payment_regex sentence __len__ 0 return False 0 start_end_date_regex re compile RegexLib DATE_RANGE_REGEX re IGNORECASE entities re findall start_end_date_regex sentence if entities __len__ 0 entities re findall start_end_date_regex sentence pop 0 try start_day int entities 0 except ValueError as error Log write str error could not convert entities 0 to an int start_day 1 start_month try start_month str EntityExtraction month_dict entities 1 except IndexError as error Log write str error str start_month is not a month or has spelling mistake return False 0 try start_year int entities 2 except ValueError as error Log write str error could not find start year start_year entities 5 try end_day int entities 3 except ValueError as error Log write str error could not convert entities 3 to an int end_day 28 end_month try end_month str EntityExtraction month_dict entities 4 except IndexError as error Log write str error str end_month is not a month or has spelling mistake return False 0 end_year entities 5 start_unix EntityExtraction __date_to_unix str start_day str start_month str start_year end_unix EntityExtraction __date_to_unix str end_day str end_month str end_year return True EntityExtraction __get_time_interval_in_months start_unix end_unix month_regex re compile RegexLib DATE_REGEX re IGNORECASE entities re findall month_regex sentence if entities __len__ 0 return True entities __len__ return False 0\\n',\n",
              " 'staticmethod def __regex_money regex_type sentence generic_regex re compile EntityExtraction regex_bin regex_type entity generic_regex search sentence group 0 entity entity replace entity entity replace entity entity replace if entity 1 entity entity 1 return True entity\\n',\n",
              " 'staticmethod def __date_to_unix date date_string join date try unix_time time mktime datetime datetime strptime date_string d m Y timetuple except ValueError OverflowError as error Log write str error str date_string return None return unix_time\\n',\n",
              " 'staticmethod def __get_time_interval_in_months first_date second_date return math ceil abs first_date second_date EntityExtraction one_month\\n',\n",
              " 'def train self Log write Size of dataset d len self dataset X np array precedent facts_vector self important_facts_index for precedent in self dataset Y np array precedent outcomes_vector self outcome_index for precedent in self dataset self input_dimensions len X 0 regressor KerasRegressor build_fn self _nn_architecture epochs 1000 batch_size 1024 verbose 0 scaler StandardScaler self model AbstractRegressor _create_pipeline scaler regressor self model fit X Y self test\\n',\n",
              " 'def _nn_architecture self model Sequential model add Dense 128 input_dim self input_dimensions kernel_initializer normal activation linear model add Dense 128 kernel_initializer normal activation relu model add Dense 64 kernel_initializer normal activation linear model add Dense 64 kernel_initializer normal activation relu model add Dense 32 kernel_initializer normal activation linear model add Dense 32 kernel_initializer normal activation relu model add Dense 1 kernel_initializer normal activation linear model compile loss mean_squared_error optimizer adam return model\\n',\n",
              " 'def get_conversation conversation_id conversation __get_conversation conversation_id return ConversationSchema jsonify conversation\\n',\n",
              " 'def init_conversation name person_type if person_type upper not in PersonType __members__ return abort make_response jsonify message Invalid person type provided 400 conversation Conversation name name person_type PersonType person_type upper bot_state BotState DETERMINE_CLAIM_CATEGORY db session add conversation db session commit return jsonify conversation_id conversation id\\n',\n",
              " 'def get_fact_entities conversation_id conversation __get_conversation conversation_id return jsonify fact_entities FactEntitySchema dump fact_entity data for fact_entity in conversation fact_entities\\n',\n",
              " 'def delete_fact_entity conversation_id fact_entity_id conversation __get_conversation conversation_id fact_entity next fact_entity for fact_entity in conversation fact_entities if fact_entity id int fact_entity_id None if fact_entity conversation fact_entities remove fact_entity db session commit return jsonify success True else abort make_response jsonify message Fact entity does not exist 404\\n',\n",
              " 'def get_report conversation_id conversation __get_conversation conversation_id if conversation report is not None report json loads conversation report else return abort make_response jsonify message No reports found for this conversation 404 return jsonify report report\\n',\n",
              " 'def receive_message conversation_id message conversation __get_conversation conversation_id response_text None file_request None possible_answers None additional_info None enforce_possible_answer False user_message None conversation_progress 0 if len conversation messages 0 response_text StaticStrings chooseFrom StaticStrings disclaimer format name conversation name possible_answers json dumps Yes enforce_possible_answer True else user_message Message sender_type SenderType USER text message relevant_fact conversation current_fact conversation messages append user_message db session commit response __generate_response conversation user_message text response_text response get response_text conversation_progress response get conversation_progress file_request response get file_request possible_answers response get possible_answers if response_text is not None response Message sender_type SenderType BOT text response_text possible_answers possible_answers enforce_possible_answer enforce_possible_answer relevant_fact conversation current_fact else return abort make_response jsonify message Response text not generated 400 if file_request is not None response file_request file_request conversation messages append response db session commit response_dict conversation_id conversation id progress conversation_progress if response_text is not None response_dict message response_text if file_request is not None response_dict file_request FileRequestSchema dump file_request data if possible_answers is not None response_dict possible_answers possible_answers if enforce_possible_answer response_dict enforce_possible_answer True return jsonify response_dict\\n',\n",
              " 'def store_user_confirmation conversation_id confirmation conversation __get_conversation conversation_id messages conversation messages 1 for message in messages if message sender_type SenderType USER user_message message user_confirmation UserConfirmation fact_id conversation current_fact id message_id user_message id text confirmation db session add user_confirmation db session commit return jsonify message User confirmation stored successfully\\n',\n",
              " 'def get_file_list conversation_id conversation __get_conversation conversation_id return jsonify files FileSchema dump file data for file in conversation files\\n',\n",
              " 'def upload_file conversation_id file conversation __get_conversation conversation_id if file filename abort make_response jsonify message No file selected 400 if file_service is_accepted_format file task_service ocr_extract_text file new_file File name file_service sanitize_name file type file content_type conversation files append new_file db session commit new_file path file_service generate_path conversation id new_file id file_service upload_file file new_file path new_file name db session commit return FileSchema jsonify new_file else abort make_response jsonify message Filetype is not supported Supported filetypes are format file_service get_file_extension file file_service get_accepted_formats_string 400\\n',\n",
              " 'def __get_conversation conversation_id conversation db session query Conversation get conversation_id if conversation return conversation abort make_response jsonify message Conversation does not exist 404\\n',\n",
              " 'def __generate_response conversation message if __has_just_accepted_disclaimer conversation return __ask_initial_question conversation elif conversation bot_state is BotState DETERMINE_CLAIM_CATEGORY nlp_request nlp_service claim_category conversation id message db session refresh conversation return response_text nlp_request message conversation_progress nlp_request conversation_progress else nlp_request nlp_service submit_message conversation id message db session refresh conversation return response_text nlp_request message conversation_progress nlp_request conversation_progress\\n',\n",
              " 'def __ask_initial_question conversation person_type conversation person_type response None if person_type is PersonType TENANT response StaticStrings chooseFrom StaticStrings problem_inquiry_tenant format name conversation name elif person_type is PersonType LANDLORD response StaticStrings chooseFrom StaticStrings problem_inquiry_landlord format name conversation name return response_text response\\n',\n",
              " 'def __dictionary_to_list precedent_vector Load load_binary precedent_vectors bin if precedent_vector is None return Log write Formatting data data_list for precedent_file in precedent_vector data_list append precedent_vector precedent_file return data_list\\n',\n",
              " 'def run command_list if command_list 0 2 if command_list 0 not in CommandEnum command_list Log write command_list 0 not recognized return False precedent_vector __dictionary_to_list if len precedent_vector 0 return False try data_size command_list 1 precedent_vector precedent_vector int data_size except IndexError pass except ValueError pass except TypeError Log write create the precedent vector model first nCommand python main py post return False if CommandEnum ALL in command_list classifier_driver run command_list 1 precedent_vector regression_driver run command_list 1 precedent_vector SimilarFinder train True dataset precedent_vector return True if CommandEnum SVM in command_list classifier_driver run command_list 1 precedent_vector if CommandEnum SVR in command_list regression_driver run command_list 1 precedent_vector if CommandEnum SIMILARITY_FINDER in command_list SimilarFinder train True dataset precedent_vector return True\\n',\n",
              " 'def predict self monthly_rent months n int months 3 1 t months 12 0 nt n t PMT monthly_rent 4 try amount PMT 1 self ANNUAL_INTEREST_RATE n nt except ZeroDivisionError return 0 return amount PMT\\n',\n",
              " 'def run nb_files 1 tag TagPrecedents precedent_vector tag tag_precedents nb_files indexes tag get_intent_index Log write Total precedents parsed format len precedent_vector for i in range len next iter precedent_vector values facts_vector total_fact len 1 for val in precedent_vector values if val facts_vector i 0 Log write Total precedents with 41 format indexes facts_vector i 1 total_fact Log write for i in range len next iter precedent_vector values outcomes_vector total_fact len 1 for val in precedent_vector values if val outcomes_vector i 0 Log write Total precedents with 41 format indexes outcomes_vector i 1 total_fact Log write Log write Saving untagged sentences to text file tag untagged_sentences_to_text\\n',\n",
              " 'def get_intent_index self facts_vector for i in range len self regexes regex_facts name self regexes regex_facts i 0 if self regexes regex_facts i 2 BOOLEAN data_type bool else data_type int facts_vector append i name data_type outcomes_vector for i in range len self regexes regex_outcomes name self regexes regex_outcomes i 0 if self regexes regex_outcomes i 2 BOOLEAN data_type bool else data_type int outcomes_vector append i name data_type return facts_vector facts_vector outcomes_vector outcomes_vector\\n',\n",
              " 'def tag_precedents self nb_files 1 Log write Tagging precedents for file in os listdir self precedents_directory_path if nb_files 1 percent float self nb_text len os listdir self precedents_directory_path 100 else percent float self nb_text nb_files 100 if self nb_text nb_files break stdout write rPrecedents tagged f percent stdout flush self precedent_vector file self __tag_file file self nb_text 1 Log write Precedent coverage str self text_tagged self nb_text Log write Line Coverage str self statements_tagged self nb_lines save Save save save_binary precedent_vectors bin self precedent_vector return self precedent_vector\\n',\n",
              " 'def __tag_file self filename facts_vector numpy zeros len self regexes regex_facts outcomes_vector numpy zeros len self regexes regex_outcomes file open self precedents_directory_path filename r encoding ISO 8859 1 text_tagged False file_contents file read statement_tagged False statement_list re split TagPrecedents fact_match file_contents self nb_lines len statement_list for j in range len statement_list for i _ regex_array regex_type in enumerate self regexes regex_facts match EntityExtraction match_any_regex statement_list j regex_array regex_type if match 0 facts_vector i match 1 statement_tagged True text_tagged True for i _ regex_array regex_type in enumerate self regexes regex_outcomes match EntityExtraction match_any_regex statement_list j regex_array regex_type if match 0 outcomes_vector i match 1 statement_tagged True text_tagged True if statement_tagged self statements_tagged 1 elif j 0 pass elif No dossier in statement_list j pass else self untagged_sent append statement_list j file close if text_tagged self text_tagged 1 file_number_match TagPrecedents file_number_regex search file_contents if file_number_match is None file_number else file_number file_number_match group 2 return name filename file_number file_number facts_vector facts_vector outcomes_vector outcomes_vector\\n',\n",
              " 'def untagged_sentences_to_text self sentence_set set self untagged_sent sentence_list list sentence_set sentence_file open untagged_sent txt w sentence_file truncate for sentence in sentence_list sentence_file writelines sentence sentence_file close\\n',\n",
              " 'def remove_files directory_path files_matching_regexes files_in_english files_parse 0 nb_of_files len os listdir directory_path Log write Filtering precedents for filename in os listdir directory_path percent float files_parse nb_of_files 100 stdout write rINFO Filtering f percent stdout flush files_parse 1 if filename endswith txt precedent_file open directory_path filename r encoding ISO 8859 1 file_removed False for line in precedent_file readlines for reg in regexes if reg search line os remove directory_path filename file_removed True files_matching_regexes append filename break if file_removed break if file_removed precedent_file close continue precedent_file seek 0 file_content precedent_file read if detect file_content en os remove directory_path filename files_in_english append filename precedent_file close print Log write Done filtering precedents Log write Removed file in english format str len files_in_english Log write Removed files without value format str len files_matching_regexes return files_in_english files_matching_regexes\\n',\n",
              " 'def regex_finder sentence regex_match_list for fact in RegexLib regex_facts for reg in fact regex_index if re search reg sentence regex_match_list append fact regex_name_index return regex_match_list\\n',\n",
              " 'def sentence_finder regex_name nb_of_files regexes get_regexes regex_name count 0 sentences_matched for i in os listdir Path raw_data_directory if count nb_of_files break count 1 file open Path raw_data_directory i r encoding ISO 8859 1 for line in file for reg in regexes if reg search line sentences_matched append line file close return sentences_matched\\n',\n",
              " 'def cluster_file_finder regex_name min_match_percentage file_path regexes get_regexes regex_name total_nb_lines_in_file 0 total_lines_matched 0 file open file_path r encoding ISO 8859 1 for line in file if line n break total_nb_lines_in_file 1 line 1 line for reg in regexes if reg search line total_lines_matched 1 file close if total_lines_matched 0 and total_lines_matched total_nb_lines_in_file min_match_percentage return True return False\\n',\n",
              " 'def cluster_regex_mapper folder_name min_match_percentage nb_of_files 1 nb_of_files_proccessed 0 path Path cluster_directory folder_name cluster_regex_dict for file_name in os listdir path if file_name 1 txt continue if nb_of_files 1 and nb_of_files_proccessed nb_of_files break nb_of_files_proccessed 1 for regex in RegexLib regex_facts if cluster_file_finder regex regex_name_index min_match_percentage path file_name if regex regex_name_index in cluster_regex_dict keys cluster_regex_dict regex regex_name_index append file_name cluster_regex_dict regex regex_name_index file_name return cluster_regex_dict\\n',\n",
              " 'def unpack_fact_demand_bin regex_types fact demand for regex_type in regex_types with zipfile ZipFile Path binary_directory regex_type _cluster bin r as zip_ref zip_ref extractall Path cluster_directory for file in os listdir Path cluster_directory regex_type _cluster shutil copy Path cluster_directory regex_type _cluster file Path cluster_directory regex_type shutil rmtree Path cluster_directory regex_type _cluster shutil rmtree Path cluster_directory __MACOSX\\n',\n",
              " 'def create_regex_cluster_bin min_match_percentage unpack_fact_demand_bin rc_fact_dict cluster_regex_mapper fact min_match_percentage rc_demand_dict cluster_regex_mapper demand min_match_percentage cluster_regex_dict fact rc_fact_dict demand rc_demand_dict save Save save save_binary cluster_regex_dict bin cluster_regex_dict\\n',\n",
              " 'def create_regex_bin regexes RegexLib reg_dict reg_dict regex_demands regexes regex_demands reg_dict regex_facts regexes regex_facts reg_dict regex_outcomes regexes regex_outcomes reg_dict MONEY_REGEX regexes MONEY_REGEX save Save save save_binary regexes bin reg_dict\\n',\n",
              " 'def __init__ self dataset None self dataset dataset self monthly_payment_index 1 self months_unpaid_index 1 for tuple in MultiOutputRegression index facts_vector if tuple 1 tenant_not_paid_lease_timespan self months_unpaid_index tuple 0 elif tuple 1 tenant_monthly_payment self monthly_payment_index tuple 0\\n',\n",
              " 'def train self outcomes self dataset 0 outcomes_vector for i in range len outcomes try column_name MultiOutputRegression classifier_labels i 0 except KeyError MultiOutputRegression classifier_labels MultiClassSVM load_classifier_labels column_name MultiOutputRegression classifier_labels i 0 if column_name additional_indemnity_money pass elif column_name tenant_ordered_to_pay_landlord regression TenantPaysLandlord self dataset i regression train regression save elif column_name tenant_ordered_to_pay_landlord_legal_fees pass\\n',\n",
              " 'def predict self facts outcomes for i in range len outcomes if outcomes i 1 column_name MultiOutputRegression classifier_labels i 0 if column_name additional_indemnity_money monthly_payment facts self monthly_payment_index months facts self months_unpaid_index outcomes i AdditionalIndemnity predict monthly_payment months elif column_name tenant_ordered_to_pay_landlord outcomes i TenantPaysLandlord predict facts 0 0 elif column_name tenant_ordered_to_pay_landlord_legal_fees outcomes i 80 return outcomes\\n',\n",
              " 'def get_resolved_fact_keys conversation return fact_entity_row fact name for fact_entity_row in conversation fact_entities\\n',\n",
              " 'def submit_claim_category conversation return fact_id get_next_fact conversation\\n',\n",
              " 'def submit_resolved_fact conversation current_fact entity_value fact_entity FactEntity fact current_fact value entity_value conversation fact_entities append fact_entity return fact_id get_next_fact conversation\\n',\n",
              " 'def get_category_fact_list claim_category category_fact_dict facts additional_facts all_category_outcomes outcome_mapping claim_category lower outcome_facts ml_service get_outcome_facts for outcome in outcome_facts if outcome in all_category_outcomes category_fact_dict facts extend outcome_facts outcome important_facts category_fact_dict additional_facts extend outcome_facts outcome additional_facts anti_facts ml_service get_anti_facts category_fact_dict facts replace_anti_facts category_fact_dict facts anti_facts category_fact_dict additional_facts replace_anti_facts category_fact_dict additional_facts anti_facts category_fact_dict additional_facts fact for fact in category_fact_dict additional_facts if fact not in category_fact_dict facts category_fact_dict facts list OrderedDict fromkeys category_fact_dict facts category_fact_dict additional_facts list OrderedDict fromkeys category_fact_dict additional_facts askable_facts Responses fact_questions keys category_fact_dict facts fact for fact in category_fact_dict facts if fact in askable_facts category_fact_dict additional_facts fact for fact in category_fact_dict additional_facts if fact in askable_facts return category_fact_dict\\n',\n",
              " 'def replace_anti_facts fact_list anti_fact_dict filtered_fact_list askable_facts Responses fact_questions keys for fact in fact_list if fact not in askable_facts if fact in anti_fact_dict keys filtered_fact_list append anti_fact_dict fact elif fact in anti_fact_dict values fact_key_value k for k v in anti_fact_dict items if v fact 0 filtered_fact_list append fact_key_value else filtered_fact_list append fact return filtered_fact_list\\n',\n",
              " 'def get_next_fact conversation all_category_facts get_category_fact_list conversation claim_category value facts_resolved get_resolved_fact_keys conversation facts_unresolved if has_important_facts conversation facts_unresolved fact for fact in all_category_facts facts if fact not in facts_resolved elif has_additional_facts conversation facts_unresolved fact for fact in all_category_facts additional_facts if fact not in facts_resolved if len facts_unresolved 0 return None fact_name facts_unresolved 0 fact Fact query filter_by name fact_name first return fact id\\n',\n",
              " 'def has_important_facts conversation all_category_facts get_category_fact_list conversation claim_category value facts_resolved get_resolved_fact_keys conversation facts_unresolved fact for fact in all_category_facts facts if fact not in facts_resolved if len facts_unresolved 0 return False return True\\n',\n",
              " 'def has_additional_facts conversation all_category_facts get_category_fact_list conversation claim_category value facts_resolved get_resolved_fact_keys conversation facts_unresolved fact for fact in all_category_facts additional_facts if fact not in facts_resolved if len facts_unresolved 0 return False return True\\n',\n",
              " 'def count_important_facts_resolved conversation all_category_facts get_category_fact_list conversation claim_category value facts_resolved get_resolved_fact_keys conversation important_facts_resolved fact for fact in all_category_facts facts if fact in facts_resolved return len important_facts_resolved\\n',\n",
              " 'def count_additional_facts_resolved conversation all_category_facts get_category_fact_list conversation claim_category value facts_resolved get_resolved_fact_keys conversation additional_facts_resolved fact for fact in all_category_facts additional_facts if fact in facts_resolved return len additional_facts_resolved\\n',\n",
              " 'def count_additional_facts_unresolved conversation all_category_facts get_category_fact_list conversation claim_category value facts_resolved get_resolved_fact_keys conversation additional_facts_unresolved fact for fact in all_category_facts additional_facts if fact not in facts_resolved return len additional_facts_unresolved\\n',\n",
              " 'def extract_fact_by_type fact_type intent entities intent_name intent name if fact_type FactType BOOLEAN return intent_name elif fact_type FactType MONEY if intent_name true for entity in entities if entity entity amount of money return entity value elif intent_name false return 0 elif fact_type FactType DURATION_MONTHS if intent_name true for entity in entities if entity entity duration return extract_month_from_duration entity return 0 elif intent_name false return 0\\n',\n",
              " 'def extract_month_from_duration extracted_entity if extracted_entity additional_info month return int math ceil extracted_entity additional_info month time_value extracted_entity additional_info value time_unit extracted_entity additional_info unit time_delta year timedelta weeks time_value 52 week timedelta weeks time_value day timedelta days time_value hour timedelta hours time_value minute timedelta minutes time_value second timedelta seconds time_value time_unit return int math ceil time_delta days 30\\n',\n",
              " 'def is_sufficient self classify_dict if len classify_dict intent_ranking 1 percent_difference self intent_percent_difference classify_dict highest_intent_confidence classify_dict intent confidence if highest_intent_confidence self min_confidence_threshold or percent_difference self min_percent_difference return False return True\\n',\n",
              " 'def intent_percent_difference self intent_dict intent_ranking intent_dict intent_ranking confidence_top intent_ranking 0 confidence confidence_contender intent_ranking 1 confidence percent_difference abs confidence_contender confidence_top 0 5 confidence_contender confidence_top return percent_difference\\n',\n",
              " 'def defaults current dict args AnyMapping dict Override current dict with defaults values param current Current dict param args Sequence with default data dicts for data in args for key value in data items current setdefault key value return current\\n',\n",
              " 'def validate_func_factory validator_class Any ValidateFunc def validate_func schema AnyMapping pure_data AnyMapping AnyMapping Validate schema with given data param schema Schema representation to use param pure_data Pure data to validate return validator_class schema validate pure_data return validate_func\\n',\n",
              " 'def from_env key str default Any None Any return os environ get key default\\n',\n",
              " 'def immutable_settings defaults Settings optionals Any types MappingProxyType Initialize and return immutable Settings dictionary Settings dictionary allows you to setup settings values from multiple sources and make sure that values cannot be changed updated by anyone else after initialization This helps keep things clear and not worry about hidden settings change somewhere around your web application param defaults Read settings values from module or dict like instance param optionals Update base settings with optional values In common additional values shouldn t be passed if settings values already populated from local settings or environment But in case of using application factories this makes sense from import settings def create_app options app app settings immutable_settings settings options return app And yes each additional key overwrite default setting value settings key value for key value in iter_settings defaults for key value in iter_settings optionals settings key value return types MappingProxyType settings\\n',\n",
              " 'def inject_settings mixed Union str Settings context MutableMapping str Any fail_silently bool False None if isinstance mixed str try mixed import_module mixed except Exception if fail_silently return raise for key value in iter_settings mixed context key value\\n',\n",
              " 'def is_setting_key key str bool return key isupper and key 0 _\\n',\n",
              " 'def iter_settings mixed Settings Iterator Tuple str Any if isinstance mixed types ModuleType for attr in dir mixed if not is_setting_key attr continue yield attr getattr mixed attr else yield from filter lambda item is_setting_key item 0 mixed items\\n',\n",
              " 'def setup_locale locale str first_weekday int None str if first_weekday is not None calendar setfirstweekday first_weekday return setlocale LC_ALL locale\\n',\n",
              " 'def setup_timezone timezone str None if timezone and hasattr time tzset tz_root usr share zoneinfo tz_filename os path join tz_root timezone split if os path exists tz_root and not os path exists tz_filename raise ValueError Incorrect timezone value 0 format timezone os environ TZ timezone time tzset\\n',\n",
              " 'def to_bool value Any bool return bool strtobool value if isinstance value str else value\\n',\n",
              " 'def __init__ self module types ModuleType response_factory Callable Any None error_class Any None validator_class Any DefaultValidator validation_error_class Type Exception ValidationError validate_func ValidateFunc None None self _valid_request None self module module self response_factory response_factory self error_class error_class self validator_class validator_class self validate_func validate_func or validate_func_factory validator_class self validation_error_class validation_error_class\\n',\n",
              " 'def make_error self message str error Exception None error_class Any None Exception if error_class is None error_class self error_class if self error_class else Error return error_class message\\n',\n",
              " 'def make_response self data Any None kwargs Any Any Validate response data and wrap it inside response factory param data Response data Could be ommited param kwargs Keyword arguments to be passed to response factory if not self _valid_request logger error Request not validated cannot make response raise self make_error Request not validated before cannot make response if data is None and self response_factory is None logger error Response data omit but no response factory is used raise self make_error Response data could be omitted only when response factory is used response_schema getattr self module response None if response_schema is not None self _validate data response_schema if self response_factory is not None return self response_factory data if data is not None else kwargs return data\\n',\n",
              " 'def validate_request self data Any additional AnyMapping merged_class Type dict dict Any Validate request data against request schema from module param data Request data param additional Additional data dicts to be merged with base request data param merged_class When additional data dicts supplied method by default will return merged dict with all data but you can customize things to use read only dict or any other additional class or callable request_schema getattr self module request None if request_schema is None logger error Request schema should be defined extra schema_module self module schema_module_attrs dir self module raise self make_error Request schema should be defined if isinstance data dict and additional data merged_class self _merge_data data additional try self _validate data request_schema finally self _valid_request False self _valid_request True processor getattr self module request_processor None return processor data if processor else data\\n',\n",
              " 'def _merge_data self data AnyMapping additional AnyMapping dict Merge base data and additional dicts param data Base data param additional Additional data dicts to be merged into base dict return defaults dict data if not isinstance data dict else data dict item for item in additional\\n',\n",
              " 'def _pure_data self data Any Any if not isinstance data dict and not isinstance data list try return dict data except TypeError return data\\n',\n",
              " 'def _validate self data Any schema AnyMapping Any try return self validate_func schema self _pure_data data except self validation_error_class as err logger error Schema validation error exc_info True extra schema schema schema_module self module if self error_class is None raise raise self make_error Validation Error error err from err\\n',\n",
              " 'def default_logging_dict loggers str kwargs Any LoggingDict Prepare logging dict suitable with logging config dictConfig Usage from logging config import dictConfig dictConfig default_logging_dict yourlogger param loggers Enable logging for each logger in sequence param kwargs Setup additional logger params via keyword arguments kwargs setdefault level INFO return version 1 disable_existing_loggers True filters ignore_errors IgnoreErrorsFilter formatters default format asctime s levelname s name s message s naked format message s handlers stdout class logging StreamHandler filters ignore_errors formatter default level DEBUG stream sys stdout stderr class logging StreamHandler formatter default level WARNING stream sys stderr loggers logger dict handlers stdout stderr kwargs for logger in loggers\\n',\n",
              " 'def update_sentry_logging logging_dict LoggingDict sentry_dsn Optional str loggers str kwargs Any None Enable Sentry logging if Sentry DSN passed note Sentry logging requires raven http pypi python org pypi raven _ library to be installed Usage from logging config import dictConfig LOGGING default_logging_dict SENTRY_DSN update_sentry_logging LOGGING SENTRY_DSN dictConfig LOGGING Using AioHttpTransport for SentryHandler This will allow to use aiohttp client for pushing data to Sentry in your aiohttp web app which means elimination of sync calls to Sentry from raven_aiohttp import AioHttpTransport update_sentry_logging LOGGING SENTRY_DSN transport AioHttpTransport param logging_dict Logging dict param sentry_dsn Sentry DSN value If None do not update logging dict at all param loggers Use Sentry logging for each logger in the sequence If the sequence is empty use Sentry logging to each available logger param kwargs Additional kwargs to be passed to SentryHandler if not sentry_dsn return kwargs class raven handlers logging SentryHandler kwargs dsn sentry_dsn logging_dict handlers sentry dict level WARNING kwargs loggers tuple logging_dict loggers if not loggers else loggers for logger in loggers logger_dict logging_dict loggers get logger if not logger_dict continue if logger_dict pop ignore_sentry False continue handlers list logger_dict setdefault handlers handlers append sentry logger_dict handlers tuple handlers\\n',\n",
              " 'def filter self record logging LogRecord bool return record levelname in DEBUG INFO\\n',\n",
              " 'contextmanager def add_resource_context router web AbstractRouter url_prefix str None name_prefix str None Iterator Any def add_resource url str get View None name str None kwargs Any web Resource Inner function to create resource and add necessary routes to it Support adding routes of all methods supported by aiohttp as GET POST PUT PATCH DELETE HEAD OPTIONS e g with add_resource_context app router as add_resource add_resource get views get post views post add_resource wildcard views wildcard param url Resource URL If url_prefix setup in context it will be prepended to URL with param get GET handler Only handler to be setup without explicit call param name Resource name type name str rtype aiohttp web Resource kwargs get get if url_prefix url join url_prefix rstrip url lstrip if not name and get name get __name__ if name_prefix and name name join name_prefix rstrip name lstrip resource router add_resource url name name for method handler in kwargs items if handler is None continue resource add_route method upper handler return resource yield add_resource\\n',\n",
              " 'def is_xhr_request request web Request bool return request headers get X Requested With XMLHttpRequest\\n',\n",
              " 'def parse_aioredis_url url str Dict str Any parts urlparse url db parts path 1 or None if db db int db return address parts hostname parts port or 6379 db db password parts password\\n',\n",
              " 'def extend_with_default validator_class Any Any validate_properties validator_class VALIDATORS properties def set_defaults validator Any properties dict instance dict schema dict Iterator ValidationError for prop subschema in properties items if default in subschema instance setdefault prop subschema default for error in validate_properties validator properties instance schema yield error return extend validator_class properties set_defaults\\n',\n",
              " 'def setup_platform hass config add_devices discovery_info None data BF1StatsData hass name config get CONF_NAME try data update except RunTimeError _LOGGER error Unable to connect fetch BF1Stats data s return False add_devices BF1StatsSensor data name\\n',\n",
              " 'def __init__ self data name self data data self _name name\\n',\n",
              " 'property def state self if self data data return int self data data pc count int self data data xone count int self data data ps4 count else return STATE_UNKNOWN\\n',\n",
              " 'property def device_state_attributes self attr attr PC int self data data pc count attr XBOX int self data data xone count attr PS4 int self data data ps4 count attr ATTR_ATTRIBUTION CONF_ATTRIBUTION return attr\\n',\n",
              " 'property def unit_of_measurement self return UNIT\\n',\n",
              " 'property def icon self return ICON\\n',\n",
              " 'def update self self data update\\n',\n",
              " 'def __init__ self hass self _hass hass self data None\\n',\n",
              " 'Throttle MIN_TIME_BETWEEN_UPDATES def update self try self data requests get _RESOURCE timeout 10 json _LOGGER debug Data s self data except ValueError as err _LOGGER error Check BF1Stats s err args self data None raise\\n',\n",
              " 'def setup_platform hass config add_devices discovery_info None try stick Stick config get CONF_PORT _LOGGER info Connected to Plugwise stick except TimeoutException SerialException as reason _LOGGER error Error s reason return for name mac in config CONF_CIRCLES items data PlugwiseSwitchData stick mac add_devices PlugwiseSwitch hass data name True _LOGGER info Created Plugwise switch for Circle s name\\n',\n",
              " 'def __init__ self hass data name self data data self _name name\\n',\n",
              " 'property def name self return self _name\\n',\n",
              " 'property def device_state_attributes self attrs if self data current_consumption STATE_UNKNOWN attrs ATTR_CURRENT_CONSUMPTION 1f format self data current_consumption attrs ATTR_CURRENT_CONSUMPTION_UNIT format ATTR_CURRENT_CONSUMPTION_UNIT_VALUE attrs ATTR_FW_VERSION self data fwversion attrs ATTR_DATE_TIME self data datetime return attrs\\n',\n",
              " 'property def current_power_watt self try return float self data current_consumption except ValueError return None\\n',\n",
              " 'property def is_on self return self data state\\n',\n",
              " 'def turn_on self kwargs self data switch_on\\n',\n",
              " 'def turn_off self self data switch_off\\n',\n",
              " 'def update self self data update\\n',\n",
              " 'def __init__ self stick mac self mac mac self stick stick self getinfo None self fwversion None self datetime None self state STATE_UNKNOWN self current_consumption STATE_UNKNOWN\\n',\n",
              " 'def update self self current_consumption Circle self mac self stick get_power_usage _LOGGER debug Current Consumption s self current_consumption self getinfo Circle self mac self stick get_info self state self getinfo relay_state self fwversion self getinfo fw_ver self datetime self getinfo datetime _LOGGER debug Relay State s self state return\\n',\n",
              " 'def switch_on self Circle self mac self stick switch_on\\n',\n",
              " 'def switch_off self Circle self mac self stick switch_off\\n',\n",
              " 'def setup_platform hass config add_entities discovery_info None host config get CONF_HOST port config get CONF_PORT username config get CONF_USERNAME password config get CONF_PASSWORD try data SolarPortalData host port username password except RunTimeError _LOGGER error Unable to connect fetch data from Solar Portal s s host port return False entities for resource in config CONF_RESOURCES sensor_type resource lower if sensor_type not in SENSOR_TYPES SENSOR_TYPES sensor_type sensor_type title mdi flash entities append SolarPortalSensor data sensor_type add_entities entities\\n',\n",
              " 'def __init__ self host port username password self _host host self _port port self _username username self _password password self data None mdhash hashlib md5 mdhash update self _password encode utf 8 pwhash mdhash hexdigest requesturl BASE_URL format self _host self _port serverapi method Login username self _username password pwhash key apitest client iPhone root ET parse urlopen requesturl getroot self token root find token text stationlisturl BASE_URL format self _host self _port serverapi method Powerstationslist username self _username token self token key apitest stationroot ET parse urlopen stationlisturl getroot for elem in stationroot findall power self stationid elem find stationID text\\n',\n",
              " 'Throttle MIN_TIME_BETWEEN_UPDATES def update self dataurl BASE_URL format self _host self _port serverapi method Data username self _username stationid str self stationid token self token key apitest self data ET parse urlopen dataurl getroot _LOGGER debug Data s self data\\n',\n",
              " 'def __init__ self data sensor_type self data data self type sensor_type self _name SENSOR_PREFIX SENSOR_TYPES self type 0 self _unit_of_measurement SENSOR_TYPES self type 1 self _icon SENSOR_TYPES self type 2 self _state None self update\\n',\n",
              " 'def update self self data update income self data data find income if self type actualpower self _state income find ActualPower text elif self type energytoday self _state income find etoday text elif self type energytotal self _state income find etotal text elif self type incometoday self _state income find TodayIncome text elif self type incometotal self _state income find TotalIncome text\\n',\n",
              " 'def setup hass config regios config get DOMAIN CONF_REGIOS disciplines config get DOMAIN CONF_DISCIPLINES distance config get DOMAIN CONF_DISTANCE interval config get DOMAIN CONF_INTERVAL if None in hass config latitude hass config longitude _LOGGER error Lat and or longitude not set in Home Assistant config return False latitude util convert hass config latitude float longitude util convert hass config longitude float url _RESOURCE format regios disciplines P2000Manager url distance latitude longitude interval hass return True\\n',\n",
              " 'def __init__ self url distance latitude longitude interval hass self _url url self _maxdist distance self _feed None self _lastmsg_time None self _restart True self _hass hass self _lat latitude self _lon longitude track_utc_time_change hass lambda now self _update second range 1 59 interval\\n',\n",
              " 'def _update self _LOGGER debug Fetching data from feed s self _url self _feed feedparser parse self _url etag None if not self _feed else self _feed get etag modified None if not self _feed else self _feed get modified if not self _feed _LOGGER debug Error fetching feed data from s self _url elif self _feed bozo 0 _LOGGER debug Error parsing feed s self _url elif len self _feed entries 0 _LOGGER debug s entries available in feed s len self _feed entries self _url self _publish_new_entries else _LOGGER debug No new entries found in feed s self _url _LOGGER debug Fetch from feed s completed self _url\\n',\n",
              " 'def _publish_new_entries self if self _restart pubdate self _feed entries 0 published self _lastmsg_time self _convert_time pubdate self _restart False _LOGGER info Restarted last datestamp s self _lastmsg_time return for item in reversed self _feed entries msgtext lat_event 0 0 lon_event 0 0 dist 0 if published in item pubdate item published lastmsg_time self _convert_time pubdate if lastmsg_time self _lastmsg_time _LOGGER debug Message is older s than last sent s skipping lastmsg_time self _lastmsg_time continue self _lastmsg_time lastmsg_time if geo_lat in item lat_event float item geo_lat else continue if geo_long in item lon_event float item geo_long else continue if lat_event and lon_event p1 self _lat self _lon p2 lat_event lon_event dist vincenty p1 p2 meters msgtext item title replace n pubdate n _LOGGER debug msgtext _LOGGER debug Calculated distance is d meters max range is d meters dist self _maxdist if dist self _maxdist msgtext continue if msgtext self _hass bus fire EVENT_P2000 ATTR_TEXT msgtext\\n',\n",
              " 'def setup_platform hass config add_devices discovery_info None add_devices ThermostatDevice config get CONF_NAME config get CONF_HOST config get CONF_PORT\\n',\n",
              " 'def __init__ self name host port self _data None self _name name self _host host self _port port self _current_temp None self _current_setpoint None self _current_state 1 self _current_operation self _set_state None self _operation_list Comfort Home Sleep Away Holiday _LOGGER debug Init called self update\\n',\n",
              " 'staticmethod def do_api_request url req requests get url timeout DEFAULT_TIMEOUT if req status_code requests codes ok _LOGGER exception Error doing API request else _LOGGER debug API request ok d req status_code Fixes invalid JSON output by TOON reqinvalid req text reqvalid reqinvalid replace return json loads req text\\n',\n",
              " 'property def should_poll self _LOGGER debug Should_Poll called return True\\n',\n",
              " 'def update self self _data self do_api_request BASE_URL format self _host self _port happ_thermstat action getThermostatInfo self _current_setpoint int self _data currentSetpoint 100 self _current_temp int self _data currentTemp 100 self _current_state int self _data activeState _LOGGER debug Update called\\n',\n",
              " 'property def device_state_attributes self return ATTR_MODE self _current_state\\n',\n",
              " 'property def target_temperature self return self _current_setpoint\\n',\n",
              " 'property def current_operation self state self _current_state if state in 0 1 2 3 4 return self _operation_list state elif state 1 return STATE_MANUAL else return STATE_UNKNOWN\\n',\n",
              " 'def set_operation_mode self operation_mode if operation_mode Comfort mode 0 elif operation_mode Home mode 1 elif operation_mode Sleep mode 2 elif operation_mode Away mode 3 elif operation_mode Holiday mode 4 self _data self do_api_request BASE_URL format self _host self _port happ_thermstat action changeSchemeState state 2 temperatureState str mode _LOGGER debug Set operation mode s s str operation_mode str mode\\n',\n",
              " 'def set_temperature self kwargs temperature kwargs get ATTR_TEMPERATURE 100 if temperature is None return else self _data self do_api_request BASE_URL format self _host self _port happ_thermstat action setSetpoint Setpoint str temperature _LOGGER debug Set temperature s str temperature\\n',\n",
              " 'def setup hass config remarksfile config get DOMAIN CONF_FILE outside_temp_sensor config get DOMAIN CONF_OUTSIDE_T_SENSOR cold_threshold config get DOMAIN CONF_COLD_THRESHOLD freeze_threshold config get DOMAIN CONF_FREEZE_THRESHOLD hour config get DOMAIN CONF_HOUR minute config get DOMAIN CONF_MINUTE temp_hour config get DOMAIN CONF_TEMP_HOUR temp_minute config get DOMAIN CONF_TEMP_MINUTE RemarksManager remarksfile outside_temp_sensor cold_threshold freeze_threshold hour minute temp_hour temp_minute hass return True\\n',\n",
              " 'def __init__ self remarksfile temp_sensor cold_threshold freeze_threshold hour minute temp_hour temp_minute hass self _hass hass self _cfgdir self _hass config config_dir self _remarksfile self _cfgdir remarks remarksfile self _temp_sensor temp_sensor self _cold_threshold cold_threshold self _cold_temp_file self _cfgdir remarks list_temp_below_20 txt self _freeze_threshold freeze_threshold self _freeze_temp_file self _cfgdir remarks list_temp_below_0 txt self _remark None self _hour hour self _minute minute self _temp_hour temp_hour self _temp_minute temp_minute track_time_change hass lambda now self _get_remark hour self _hour minute self _minute second 0 track_time_change hass lambda now self _get_temp_remark hour self _temp_hour minute self _temp_minute second 0\\n',\n",
              " 'def _get_remark self _LOGGER debug Fetching remark from file s self _remarksfile lines open str self _remarksfile read splitlines self _remark random choice lines _LOGGER debug Fire event for new remark self _hass bus fire EVENT_REMARKS ATTR_TEXT self _remark\\n',\n",
              " 'def _get_temp_remark self sensor self _hass states get self _temp_sensor unit sensor attributes get unit_of_measurement if float sensor state float self _freeze_threshold _LOGGER debug Fetching remark from s self _freeze_temp_file lines open str self _freeze_temp_file read splitlines self _remark It is currently sensor state unit outside random choice lines _LOGGER debug Event for freezetemp remark temp is s s sensor state unit self _hass bus fire EVENT_REMARKS ATTR_TEXT self _remark elif float sensor state float self _cold_threshold _LOGGER debug Fetching remark from s self _cold_temp_file lines open str self _cold_temp_file read splitlines self _remark It is currently sensor state unit outside random choice lines _LOGGER debug Event for coldtemp remark temp is s s sensor state unit self _hass bus fire EVENT_REMARKS ATTR_TEXT self _remark\\n',\n",
              " 'def setup_platform hass config add_entities discovery_info None scan_interval config get CONF_SCAN_INTERVAL host config get CONF_HOST port config get CONF_PORT try data ToonData host port except requests exceptions HTTPError as error _LOGGER error error return False entities for resource in config CONF_RESOURCES sensor_type resource lower if sensor_type not in SENSOR_TYPES SENSOR_TYPES sensor_type sensor_type title mdi flash entities append ToonSmartMeterSensor data sensor_type add_entities entities\\n',\n",
              " 'def __init__ self host port self _host host self _port port self data None\\n',\n",
              " 'Throttle MIN_TIME_BETWEEN_UPDATES def update self try self data requests get BASE_URL format self _host self _port hdrv_zwave action getDevices json timeout 5 json _LOGGER debug Data s self data except requests exceptions RequestException _LOGGER error Error occurred while fetching data self data None return False\\n',\n",
              " 'def __init__ self data sensor_type self data data self type sensor_type self _name SENSOR_PREFIX SENSOR_TYPES self type 0 self _unit SENSOR_TYPES self type 1 self _icon SENSOR_TYPES self type 2 self _state None\\n',\n",
              " 'property def state self return self _state\\n',\n",
              " 'def update self self data update energy self data data Go to http toon ip port hdrv_zwave action getDevices json and search for dev_ if self type gasused if dev_3 1 in energy self _state float energy dev_3 1 CurrentGasFlow 100 elif dev_2 1 in energy self _state float energy dev_2 1 CurrentGasFlow 100 elif self type gasusedcnt if dev_3 1 in energy self _state float energy dev_3 1 CurrentGasQuantity 1000 elif dev_2 1 in energy self _state float energy dev_2 1 CurrentGasQuantity 1000 elif self type elecusageflowpulse if dev_3 2 in energy self _state energy dev_3 2 CurrentElectricityFlow elif dev_2 2 in energy self _state energy dev_2 2 CurrentElectricityFlow elif self type elecusagecntpulse if dev_3 2 in energy self _state float energy dev_3 2 CurrentElectricityQuantity 1000 elif dev_2 2 in energy self _state float energy dev_2 2 CurrentElectricityQuantity 1000 elif self type elecusageflowlow if dev_3 5 in energy self _state energy dev_3 5 CurrentElectricityFlow elif dev_2 6 in energy self _state energy dev_2 6 CurrentElectricityFlow elif self type elecusagecntlow if dev_3 5 in energy self _state float energy dev_3 5 CurrentElectricityQuantity 1000 elif dev_2 6 in energy self _state float energy dev_2 6 CurrentElectricityQuantity 1000 elif self type elecusageflowhigh if dev_3 3 in energy self _state energy dev_3 3 CurrentElectricityFlow elif dev_2 4 in energy self _state energy dev_2 4 CurrentElectricityFlow elif self type elecusagecnthigh if dev_3 3 in energy self _state float energy dev_3 3 CurrentElectricityQuantity 1000 elif dev_2 4 in energy self _state float energy dev_2 4 CurrentElectricityQuantity 1000 elif self type elecprodflowlow if dev_3 6 in energy self _state energy dev_3 6 CurrentElectricityFlow elif dev_2 7 in energy self _state energy dev_2 7 CurrentElectricityFlow elif self type elecprodcntlow if dev_3 6 in energy self _state float energy dev_3 6 CurrentElectricityQuantity 1000 elif dev_2 7 in energy self _state float energy dev_2 7 CurrentElectricityQuantity 1000 elif self type elecprodflowhigh if dev_3 4 in energy self _state energy dev_3 4 CurrentElectricityFlow elif dev_2 5 in energy self _state energy dev_2 5 CurrentElectricityFlow elif self type elecprodcnthigh if dev_3 4 in energy self _state float energy dev_3 4 CurrentElectricityQuantity 1000 elif dev_2 5 in energy self _state float energy dev_2 5 CurrentElectricityQuantity 1000 elif self type elecsolar if dev_2 3 in energy self _state energy dev_2 3 CurrentElectricityFlow elif self type elecsolarcnt if dev_2 3 in energy self _state float energy dev_2 3 CurrentElectricityQuantity 1000 elif self type heat if dev_2 8 in energy self _state float energy dev_2 8 CurrentHeatQuantity 1000\\n',\n",
              " 'def setup_platform hass config add_entities discovery_info None scan_interval config get CONF_SCAN_INTERVAL host config get CONF_HOST port config get CONF_PORT try data ToonData host port except requests exceptions HTTPError as error _LOGGER error error return False entities for resource in config CONF_RESOURCES sensor_type resource lower if sensor_type not in SENSOR_TYPES SENSOR_TYPES sensor_type sensor_type title mdi flash entities append ToonBoilerStatusSensor data sensor_type add_entities entities\\n',\n",
              " 'Throttle MIN_TIME_BETWEEN_UPDATES def update self try self data requests get BASE_URL format self _host self _port boilerstatus boilervalues txt timeout 5 json _LOGGER debug Data s self data except requests exceptions RequestException _LOGGER error Error occurred while fetching data self data None return False\\n',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "t9ALbbpmY9rm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c5ee74e3-f88b-4660-dcba-354d09fa5659"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  random_image_cpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\n",
        "  net_cpu = tf.reduce_sum(net_cpu)\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n",
        "  net_gpu = tf.reduce_sum(net_gpu)\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "# Test execution once to detect errors early.\n",
        "try:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise\n",
        "\n",
        "def cpu():\n",
        "  sess.run(net_cpu)\n",
        "  \n",
        "def gpu():\n",
        "  sess.run(net_gpu)\n",
        "  \n",
        "# Runs the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "8.350230318000058\n",
            "GPU (s):\n",
            "0.1842791589999706\n",
            "GPU speedup over CPU: 45x\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}